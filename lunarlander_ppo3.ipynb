{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f2794acc",
      "metadata": {
        "id": "f2794acc"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e967ed",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a66PnqFsWjux",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a66PnqFsWjux",
        "outputId": "8cd6acd7-44aa-44e3-8c96-bbdfa9ac68c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,565 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2351189 sha256=9d912b2fc827f66fe63606e5990fd39fa91aaefa4cf6e8c454f84918691944b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-py-2.3.8\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d box2d-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "YHcWrparWm6I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHcWrparWm6I",
        "outputId": "361b1050-ea31-4d0a-9157-233ccf32bfaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "44162f9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44162f9a",
        "outputId": "f37fccca-b93e-4118-fafa-17be44edc726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "observation space observation_space (8,)\n",
            "action space 4\n"
          ]
        }
      ],
      "source": [
        "# this is our environment that will train our ppo agent on\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\",render_mode=\"human\")\n",
        "observation, info = env.reset()\n",
        "\n",
        "episode_over = False\n",
        "while not episode_over:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "\n",
        "    episode_over = terminated or truncated\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print('observation space observation_space',env.observation_space.shape)\n",
        "print('action space',env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88723e19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88723e19",
        "outputId": "67c40057-1547-44fe-f3dd-c587447db66c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# check for cuda availability\n",
        "device  =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bea39c6b",
      "metadata": {
        "id": "bea39c6b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# this class define the storage buffer of the environment\n",
        "\n",
        "class StorageBuffer:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.episode_rewards = []  # Store rewards for each episode\n",
        "        self.episode_lengths = []  # Store lengths for each episode\n",
        "\n",
        "    def reset(self):\n",
        "        # Current episode storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.next_state = None\n",
        "        self.current_reward = 0\n",
        "\n",
        "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "        self.current_reward += reward\n",
        "\n",
        "        if done:\n",
        "            self.next_state = next_state\n",
        "            self.episode_rewards.append(self.current_reward)\n",
        "            self.episode_lengths.append(len(self.rewards))\n",
        "\n",
        "    def get_episode_data(self):\n",
        "        return {\n",
        "            'states': torch.FloatTensor(np.array(self.states)),\n",
        "            'actions': torch.tensor(self.actions),\n",
        "            'rewards': torch.tensor(self.rewards),\n",
        "            'log_probs': torch.tensor(self.log_probs),\n",
        "            'dones': torch.tensor(self.dones),\n",
        "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
        "        }\n",
        "\n",
        "    def get_statistics(self):\n",
        "\n",
        "        if not self.episode_rewards:\n",
        "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
        "\n",
        "        return {\n",
        "            \"mean_reward\": np.mean(self.episode_rewards),\n",
        "            \"max_reward\": np.max(self.episode_rewards),\n",
        "            \"min_reward\": np.min(self.episode_rewards),\n",
        "            \"mean_length\": np.mean(self.episode_lengths),\n",
        "            \"current_reward\": self.current_reward,\n",
        "            \"current_length\": len(self.rewards)\n",
        "        }\n",
        "\n",
        "#the policy and value networks\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-YcrCil4XWuI",
      "metadata": {
        "id": "-YcrCil4XWuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a6296e7d",
      "metadata": {
        "id": "a6296e7d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from gymnasium.vector import SyncVectorEnv\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim,device=\"cpu\", lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=10, entropy_coef=0.01):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.old_policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "        self.value_function = ValueNetwork(state_dim).to(device)\n",
        "\n",
        "        self.policy_optimizer = optim.AdamW(self.policy.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.AdamW(self.value_function.parameters(), lr=lr)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.k_epochs = k_epochs\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
        "        with torch.no_grad():\n",
        "            action_prob = self.old_policy(state)\n",
        "        dist = Categorical(action_prob)\n",
        "        action = dist.sample()\n",
        "        return action, dist.log_prob(action), dist.entropy()\n",
        "\n",
        "    def compute_advantages(self, rewards, values, next_value, dones, lambda_gae=0.95):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
        "            gae = delta + self.gamma * lambda_gae * (1 - float(dones[t])) * gae\n",
        "            advantages.insert(0, gae)\n",
        "            next_value = values[t]\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
        "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    def update(self, buffer, batch_size=120):\n",
        "    # Collect episode data\n",
        "        data = buffer.get_episode_data()\n",
        "        states = data['states'].clone().detach().float().to(device)\n",
        "        actions = data['actions'].clone().detach().long().to(device)\n",
        "        log_probs_old = data['log_probs'].clone().detach().float().to(device)\n",
        "        rewards = data['rewards']\n",
        "        dones = data['dones']\n",
        "\n",
        "        values = self.value_function(states).squeeze().detach()\n",
        "        next_value = self.value_function(torch.tensor(data['next_state'], dtype=torch.float32).to(device)).item() if data['next_state'] else 0.0\n",
        "        advantages = self.compute_advantages(rewards, values, next_value, dones)\n",
        "        targets = advantages + values\n",
        "\n",
        "        # Create dataset and dataloader\n",
        "        dataset = TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        total_loss_actor = 0\n",
        "        total_loss_critic = 0\n",
        "        total_entropy = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for _ in range(self.k_epochs):\n",
        "            for batch_states, batch_actions, batch_log_probs_old, batch_advantages, batch_targets in dataloader:\n",
        "                action_probs = self.policy(batch_states)\n",
        "                dist = Categorical(action_probs)\n",
        "                log_probs = dist.log_prob(batch_actions)\n",
        "\n",
        "                # we add this entropy to encourage exploration\n",
        "                entropy = dist.entropy().mean()\n",
        "\n",
        "                ratios = torch.exp(log_probs - batch_log_probs_old)\n",
        "                surr1 = ratios * batch_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
        "                loss_actor = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy\n",
        "\n",
        "                values_pred = self.value_function(batch_states).squeeze()\n",
        "                loss_critic = self.mse_loss(values_pred, batch_targets)\n",
        "\n",
        "                self.policy_optimizer.zero_grad()\n",
        "                loss_actor.backward()\n",
        "                self.policy_optimizer.step()\n",
        "\n",
        "                self.value_optimizer.zero_grad()\n",
        "                loss_critic.backward()\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "                total_loss_actor += loss_actor.item()\n",
        "                total_loss_critic += loss_critic.item()\n",
        "                total_entropy += entropy.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        return total_loss_actor / num_batches, total_loss_critic / num_batches, total_entropy / num_batches\n",
        "\n",
        "\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'value_state_dict': self.value_function.state_dict(),\n",
        "            'policy_optimizer': self.policy_optimizer.state_dict(),\n",
        "            'value_optimizer': self.value_optimizer.state_dict()\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.old_policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.value_function.load_state_dict(checkpoint['value_state_dict'])\n",
        "\n",
        "        if 'policy_optimizer' in checkpoint:\n",
        "            self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
        "        if 'value_optimizer' in checkpoint:\n",
        "            self.value_optimizer.load_state_dict(checkpoint['value_optimizer'])\n",
        "\n",
        "\n",
        "def train_ppo(num_episodes=5000, max_time_steps=500,model_name = None):\n",
        "\n",
        "    # runing multiple environments at once so that our model can learn faster and from multiple environments\n",
        "\n",
        "    num_envs = 5\n",
        "\n",
        "    env = SyncVectorEnv([lambda: gym.make(\"LunarLander-v3\") for _ in range(num_envs)])\n",
        "    state_dim = env.single_observation_space.shape[0]\n",
        "    action_dim = env.single_action_space.n\n",
        "\n",
        "    agent = PPOAgent(state_dim, action_dim,device)\n",
        "    if model_name is not None:\n",
        "        agent.load_model(model_name)\n",
        "\n",
        "    buffer = StorageBuffer()\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        states, _ = env.reset()\n",
        "        buffer.reset()\n",
        "        episode_rewards = np.zeros(num_envs)\n",
        "\n",
        "        for t in range(max_time_steps):\n",
        "            actions, log_probs, entropys = agent.select_action(states)\n",
        "            next_states, rewards, terminateds, truncateds, _ = env.step(actions.tolist())\n",
        "            dones = terminateds | truncateds\n",
        "            for i in range(num_envs):\n",
        "                episode_rewards[i] += rewards[i]\n",
        "                buffer.add_step(states[i], actions[i], rewards[i], log_probs[i], dones[i], next_states[i] if not dones[i] else None)\n",
        "            states = next_states\n",
        "            if all(dones):\n",
        "                break\n",
        "\n",
        "        loss_actor, loss_critic, entropy_val = agent.update(buffer,500)\n",
        "\n",
        "        if episode_rewards.mean() > best_reward:\n",
        "            best_reward = episode_rewards.mean()\n",
        "            agent.save_model(\"ppo_lunarlander3.pth\")\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode + 1},AVG Reward: {float(episode_rewards.mean()):.2f}, Best: {float(best_reward):.2f}, Loss Actor: {float(loss_actor):.4f}, Loss Critic: {float(loss_critic):.4f}, Entropy: {float(entropy_val):.4f}\")\n",
        "\n",
        "\n",
        "    env.close()\n",
        "    # wandb.finish()\n",
        "    return agent, buffer\n",
        "\n",
        "\n",
        "def test_model(model_path, num_episodes=10):\n",
        "    env = gym.make(\"LunarLander-v3\" ,render_mode=\"human\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = PPOAgent(state_dim, action_dim,device)\n",
        "    agent.load_model(model_path)\n",
        "    buffer = StorageBuffer()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        buffer.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _, _ = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action.tolist())\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Test Episode {episode + 1}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d6b375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a8d6b375",
        "outputId": "dc4559b5-bb77-450b-dbaf-4d141aeaf572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 10000 episodes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-6-1364379946.py:38: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
            "  'dones': torch.tensor(self.dones),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10,AVG Reward: -40.01, Best: 33.69, Loss Actor: -0.0077, Loss Critic: 0.8539, Entropy: 0.5286\n",
            "Episode 20,AVG Reward: -83.77, Best: 33.69, Loss Actor: -0.0089, Loss Critic: 0.8380, Entropy: 0.6228\n",
            "Episode 30,AVG Reward: -68.32, Best: 33.69, Loss Actor: -0.0091, Loss Critic: 0.9375, Entropy: 0.5776\n",
            "Episode 40,AVG Reward: -44.27, Best: 33.69, Loss Actor: -0.0134, Loss Critic: 0.8813, Entropy: 0.6671\n",
            "Episode 50,AVG Reward: 80.01, Best: 80.01, Loss Actor: -0.0097, Loss Critic: 0.9102, Entropy: 0.4941\n",
            "Episode 60,AVG Reward: 70.59, Best: 106.55, Loss Actor: -0.0095, Loss Critic: 0.8736, Entropy: 0.4214\n",
            "Episode 70,AVG Reward: 39.38, Best: 106.55, Loss Actor: -0.0103, Loss Critic: 0.8915, Entropy: 0.5117\n",
            "Episode 80,AVG Reward: -42.75, Best: 106.55, Loss Actor: -0.0091, Loss Critic: 0.8761, Entropy: 0.5553\n",
            "Episode 90,AVG Reward: -76.25, Best: 106.55, Loss Actor: -0.0093, Loss Critic: 0.8665, Entropy: 0.5180\n",
            "Episode 100,AVG Reward: -47.08, Best: 106.55, Loss Actor: -0.0088, Loss Critic: 0.8556, Entropy: 0.5269\n",
            "Episode 110,AVG Reward: -95.14, Best: 125.66, Loss Actor: -0.0098, Loss Critic: 0.8778, Entropy: 0.5637\n",
            "Episode 120,AVG Reward: -12.40, Best: 125.66, Loss Actor: -0.0093, Loss Critic: 0.8015, Entropy: 0.4519\n",
            "Episode 130,AVG Reward: 2.16, Best: 125.66, Loss Actor: -0.0084, Loss Critic: 0.8337, Entropy: 0.4229\n",
            "Episode 140,AVG Reward: -100.98, Best: 125.66, Loss Actor: -0.0096, Loss Critic: 0.8505, Entropy: 0.4261\n",
            "Episode 150,AVG Reward: -75.89, Best: 125.66, Loss Actor: -0.0073, Loss Critic: 0.8570, Entropy: 0.4418\n",
            "Episode 160,AVG Reward: -48.40, Best: 125.66, Loss Actor: -0.0103, Loss Critic: 0.8446, Entropy: 0.4559\n",
            "Episode 170,AVG Reward: -49.43, Best: 125.66, Loss Actor: -0.0081, Loss Critic: 0.8942, Entropy: 0.4731\n",
            "Episode 180,AVG Reward: -42.71, Best: 125.66, Loss Actor: -0.0076, Loss Critic: 0.8373, Entropy: 0.4297\n",
            "Episode 190,AVG Reward: 19.26, Best: 125.66, Loss Actor: -0.0078, Loss Critic: 0.7940, Entropy: 0.4494\n",
            "Episode 200,AVG Reward: -38.19, Best: 125.66, Loss Actor: -0.0086, Loss Critic: 0.7309, Entropy: 0.4218\n",
            "Episode 210,AVG Reward: -15.53, Best: 125.66, Loss Actor: -0.0085, Loss Critic: 0.7675, Entropy: 0.4528\n",
            "Episode 220,AVG Reward: -109.01, Best: 125.66, Loss Actor: -0.0089, Loss Critic: 0.8542, Entropy: 0.4850\n",
            "Episode 230,AVG Reward: -221.02, Best: 125.66, Loss Actor: -0.0079, Loss Critic: 0.8122, Entropy: 0.4260\n",
            "Episode 240,AVG Reward: -60.54, Best: 125.66, Loss Actor: -0.0102, Loss Critic: 0.8636, Entropy: 0.4222\n",
            "Episode 250,AVG Reward: -95.88, Best: 125.66, Loss Actor: -0.0105, Loss Critic: 0.8756, Entropy: 0.4724\n",
            "Episode 260,AVG Reward: -18.46, Best: 125.66, Loss Actor: -0.0084, Loss Critic: 0.8677, Entropy: 0.4236\n",
            "Episode 270,AVG Reward: -55.99, Best: 125.66, Loss Actor: -0.0105, Loss Critic: 0.8139, Entropy: 0.4525\n",
            "Episode 280,AVG Reward: -12.17, Best: 125.66, Loss Actor: -0.0105, Loss Critic: 0.8132, Entropy: 0.4463\n",
            "Episode 290,AVG Reward: -45.05, Best: 125.66, Loss Actor: -0.0060, Loss Critic: 0.8445, Entropy: 0.3804\n",
            "Episode 300,AVG Reward: 103.18, Best: 125.66, Loss Actor: -0.0101, Loss Critic: 0.8320, Entropy: 0.4748\n",
            "Episode 310,AVG Reward: -119.70, Best: 125.66, Loss Actor: -0.0076, Loss Critic: 0.8844, Entropy: 0.4047\n",
            "Episode 320,AVG Reward: -73.13, Best: 125.66, Loss Actor: -0.0085, Loss Critic: 0.8942, Entropy: 0.4363\n",
            "Episode 330,AVG Reward: 36.55, Best: 127.69, Loss Actor: -0.0093, Loss Critic: 0.8648, Entropy: 0.4382\n",
            "Episode 340,AVG Reward: -42.71, Best: 127.69, Loss Actor: -0.0085, Loss Critic: 0.8556, Entropy: 0.4307\n",
            "Episode 350,AVG Reward: 21.06, Best: 127.69, Loss Actor: -0.0091, Loss Critic: 0.8444, Entropy: 0.4338\n",
            "Episode 360,AVG Reward: 43.73, Best: 127.69, Loss Actor: -0.0079, Loss Critic: 0.7408, Entropy: 0.4564\n",
            "Episode 370,AVG Reward: 48.08, Best: 127.69, Loss Actor: -0.0100, Loss Critic: 0.7670, Entropy: 0.4993\n",
            "Episode 380,AVG Reward: 100.86, Best: 127.69, Loss Actor: -0.0096, Loss Critic: 0.8275, Entropy: 0.5212\n",
            "Episode 390,AVG Reward: 59.92, Best: 127.69, Loss Actor: -0.0123, Loss Critic: 0.7450, Entropy: 0.5382\n",
            "Episode 400,AVG Reward: 73.59, Best: 127.69, Loss Actor: -0.0089, Loss Critic: 0.8749, Entropy: 0.4502\n",
            "Episode 410,AVG Reward: 34.02, Best: 127.69, Loss Actor: -0.0085, Loss Critic: 0.8754, Entropy: 0.4801\n",
            "Episode 420,AVG Reward: 6.83, Best: 127.69, Loss Actor: -0.0095, Loss Critic: 0.8888, Entropy: 0.4445\n",
            "Episode 430,AVG Reward: -43.43, Best: 127.69, Loss Actor: -0.0093, Loss Critic: 0.7364, Entropy: 0.4124\n",
            "Episode 440,AVG Reward: 5.38, Best: 127.69, Loss Actor: -0.0084, Loss Critic: 0.8603, Entropy: 0.4782\n",
            "Episode 450,AVG Reward: -74.71, Best: 127.69, Loss Actor: -0.0087, Loss Critic: 0.8782, Entropy: 0.4887\n",
            "Episode 460,AVG Reward: -100.09, Best: 127.69, Loss Actor: -0.0090, Loss Critic: 0.7871, Entropy: 0.3896\n",
            "Episode 470,AVG Reward: 4.00, Best: 127.69, Loss Actor: -0.0097, Loss Critic: 0.7493, Entropy: 0.5560\n",
            "Episode 480,AVG Reward: -55.08, Best: 127.69, Loss Actor: -0.0099, Loss Critic: 0.8590, Entropy: 0.4281\n",
            "Episode 490,AVG Reward: -34.60, Best: 127.69, Loss Actor: -0.0091, Loss Critic: 0.7724, Entropy: 0.4572\n",
            "Episode 500,AVG Reward: -109.00, Best: 127.69, Loss Actor: -0.0077, Loss Critic: 0.7985, Entropy: 0.4179\n",
            "Episode 510,AVG Reward: -61.80, Best: 127.69, Loss Actor: -0.0089, Loss Critic: 0.7823, Entropy: 0.4634\n",
            "Episode 520,AVG Reward: 31.18, Best: 127.69, Loss Actor: -0.0120, Loss Critic: 0.7611, Entropy: 0.5503\n",
            "Episode 530,AVG Reward: -62.89, Best: 127.69, Loss Actor: -0.0089, Loss Critic: 0.8405, Entropy: 0.4266\n",
            "Episode 540,AVG Reward: -54.51, Best: 127.69, Loss Actor: -0.0096, Loss Critic: 0.8563, Entropy: 0.4345\n",
            "Episode 550,AVG Reward: -84.98, Best: 127.69, Loss Actor: -0.0075, Loss Critic: 0.8115, Entropy: 0.3319\n",
            "Episode 560,AVG Reward: 13.19, Best: 127.69, Loss Actor: -0.0093, Loss Critic: 0.7921, Entropy: 0.4660\n",
            "Episode 570,AVG Reward: -71.85, Best: 127.69, Loss Actor: -0.0123, Loss Critic: 0.8460, Entropy: 0.4664\n",
            "Episode 580,AVG Reward: -25.49, Best: 133.51, Loss Actor: -0.0095, Loss Critic: 0.8678, Entropy: 0.4368\n",
            "Episode 590,AVG Reward: -102.35, Best: 133.51, Loss Actor: -0.0081, Loss Critic: 0.8235, Entropy: 0.4300\n",
            "Episode 600,AVG Reward: -157.07, Best: 133.51, Loss Actor: -0.0078, Loss Critic: 0.8055, Entropy: 0.3993\n",
            "Episode 610,AVG Reward: 123.88, Best: 133.51, Loss Actor: -0.0081, Loss Critic: 0.8318, Entropy: 0.4770\n",
            "Episode 620,AVG Reward: -68.74, Best: 133.51, Loss Actor: -0.0083, Loss Critic: 0.8117, Entropy: 0.3488\n",
            "Episode 630,AVG Reward: 52.47, Best: 133.51, Loss Actor: -0.0089, Loss Critic: 0.7843, Entropy: 0.4160\n",
            "Episode 640,AVG Reward: 39.66, Best: 133.51, Loss Actor: -0.0085, Loss Critic: 0.8334, Entropy: 0.4721\n",
            "Episode 650,AVG Reward: 161.82, Best: 161.82, Loss Actor: -0.0088, Loss Critic: 0.8031, Entropy: 0.4438\n",
            "Episode 660,AVG Reward: -53.80, Best: 161.82, Loss Actor: -0.0097, Loss Critic: 0.9141, Entropy: 0.4681\n",
            "Episode 670,AVG Reward: -55.85, Best: 161.82, Loss Actor: -0.0098, Loss Critic: 0.8078, Entropy: 0.4257\n",
            "Episode 680,AVG Reward: 0.64, Best: 161.82, Loss Actor: -0.0095, Loss Critic: 0.7525, Entropy: 0.3673\n",
            "Episode 690,AVG Reward: 121.91, Best: 161.82, Loss Actor: -0.0091, Loss Critic: 0.8663, Entropy: 0.3828\n",
            "Episode 700,AVG Reward: -143.49, Best: 161.82, Loss Actor: -0.0091, Loss Critic: 0.6774, Entropy: 0.3746\n",
            "Episode 710,AVG Reward: 67.27, Best: 161.82, Loss Actor: -0.0077, Loss Critic: 0.8361, Entropy: 0.3502\n",
            "Episode 720,AVG Reward: -21.27, Best: 163.73, Loss Actor: -0.0087, Loss Critic: 0.7755, Entropy: 0.3598\n",
            "Episode 730,AVG Reward: 30.24, Best: 163.73, Loss Actor: -0.0075, Loss Critic: 0.7866, Entropy: 0.3652\n",
            "Episode 740,AVG Reward: 10.96, Best: 163.73, Loss Actor: -0.0081, Loss Critic: 0.8732, Entropy: 0.3944\n",
            "Episode 750,AVG Reward: 106.80, Best: 163.73, Loss Actor: -0.0081, Loss Critic: 0.8706, Entropy: 0.3190\n",
            "Episode 760,AVG Reward: 109.77, Best: 163.73, Loss Actor: -0.0091, Loss Critic: 0.8453, Entropy: 0.3880\n",
            "Episode 770,AVG Reward: 185.91, Best: 185.91, Loss Actor: -0.0090, Loss Critic: 0.8197, Entropy: 0.3836\n",
            "Episode 780,AVG Reward: 154.33, Best: 193.39, Loss Actor: -0.0078, Loss Critic: 0.8315, Entropy: 0.3847\n",
            "Episode 790,AVG Reward: -85.62, Best: 193.39, Loss Actor: -0.0072, Loss Critic: 0.8079, Entropy: 0.4090\n",
            "Episode 800,AVG Reward: -112.11, Best: 193.39, Loss Actor: -0.0091, Loss Critic: 0.8591, Entropy: 0.3876\n",
            "Episode 810,AVG Reward: 63.48, Best: 199.52, Loss Actor: -0.0076, Loss Critic: 0.7883, Entropy: 0.4025\n",
            "Episode 820,AVG Reward: -38.17, Best: 199.52, Loss Actor: -0.0102, Loss Critic: 0.7592, Entropy: 0.4657\n",
            "Episode 830,AVG Reward: -62.11, Best: 199.52, Loss Actor: -0.0091, Loss Critic: 0.7455, Entropy: 0.4074\n",
            "Episode 840,AVG Reward: 179.92, Best: 212.77, Loss Actor: -0.0086, Loss Critic: 0.8158, Entropy: 0.3592\n",
            "Episode 850,AVG Reward: -27.07, Best: 212.77, Loss Actor: -0.0074, Loss Critic: 0.8565, Entropy: 0.3834\n",
            "Episode 860,AVG Reward: 48.18, Best: 237.83, Loss Actor: -0.0083, Loss Critic: 0.7896, Entropy: 0.4100\n",
            "Episode 870,AVG Reward: 1.11, Best: 237.83, Loss Actor: -0.0080, Loss Critic: 0.8378, Entropy: 0.3534\n",
            "Episode 880,AVG Reward: 62.39, Best: 237.83, Loss Actor: -0.0090, Loss Critic: 0.8348, Entropy: 0.3245\n",
            "Episode 890,AVG Reward: -18.10, Best: 237.83, Loss Actor: -0.0077, Loss Critic: 0.8427, Entropy: 0.4163\n",
            "Episode 900,AVG Reward: 104.15, Best: 237.83, Loss Actor: -0.0118, Loss Critic: 0.8354, Entropy: 0.4557\n",
            "Episode 910,AVG Reward: 3.44, Best: 237.83, Loss Actor: -0.0095, Loss Critic: 0.7440, Entropy: 0.3361\n",
            "Episode 920,AVG Reward: 47.21, Best: 282.65, Loss Actor: -0.0074, Loss Critic: 0.7898, Entropy: 0.3561\n",
            "Episode 930,AVG Reward: 117.53, Best: 282.65, Loss Actor: -0.0084, Loss Critic: 0.8353, Entropy: 0.3750\n",
            "Episode 940,AVG Reward: 68.21, Best: 282.65, Loss Actor: -0.0066, Loss Critic: 0.8047, Entropy: 0.3284\n",
            "Episode 950,AVG Reward: 222.67, Best: 282.65, Loss Actor: -0.0083, Loss Critic: 0.7970, Entropy: 0.3281\n",
            "Episode 960,AVG Reward: 71.69, Best: 282.65, Loss Actor: -0.0095, Loss Critic: 0.7724, Entropy: 0.4022\n",
            "Episode 970,AVG Reward: 49.03, Best: 282.65, Loss Actor: -0.0068, Loss Critic: 0.8321, Entropy: 0.3295\n",
            "Episode 980,AVG Reward: 15.16, Best: 282.65, Loss Actor: -0.0064, Loss Critic: 0.7379, Entropy: 0.2954\n",
            "Episode 990,AVG Reward: 53.73, Best: 282.65, Loss Actor: -0.0087, Loss Critic: 0.7223, Entropy: 0.3748\n",
            "Episode 1000,AVG Reward: -124.18, Best: 282.65, Loss Actor: -0.0089, Loss Critic: 0.8280, Entropy: 0.3631\n",
            "Episode 1010,AVG Reward: 44.40, Best: 282.65, Loss Actor: -0.0101, Loss Critic: 0.7340, Entropy: 0.4540\n",
            "Episode 1020,AVG Reward: -16.28, Best: 282.65, Loss Actor: -0.0091, Loss Critic: 0.8058, Entropy: 0.4112\n",
            "Episode 1030,AVG Reward: 48.83, Best: 282.65, Loss Actor: -0.0075, Loss Critic: 0.7684, Entropy: 0.3407\n",
            "Episode 1040,AVG Reward: 149.09, Best: 282.65, Loss Actor: -0.0068, Loss Critic: 0.7888, Entropy: 0.2962\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-694124005.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# model_name = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining completed! Training statistics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-11-2365934343.py\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(num_episodes, max_time_steps, model_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mloss_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_reward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-11-2365934343.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, buffer, batch_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_log_probs_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_advantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_targets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     raise ValueError(\n\u001b[1;32m     72\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # this is for training and testing the model\n",
        "    # note that this environment is bit complex for that it will take a lot more training if you have gpu access that can speed up the training\n",
        "    num_episodes = 10000  # You can change this number\n",
        "    print(f\"Starting training for {num_episodes} episodes...\")\n",
        "\n",
        "    model_name=\"ppo_lunarlander3.pth\"\n",
        "    # model_name = None\n",
        "\n",
        "    agent, train_storage = train_ppo(num_episodes=num_episodes,model_name=model_name)\n",
        "\n",
        "    print(\"\\nTraining completed! Training statistics:\")\n",
        "\n",
        "    train_stats = train_storage.get_statistics()\n",
        "    print(f\"Mean reward: {train_stats['mean_reward']:.2f}\")\n",
        "    print(f\"Max reward: {train_stats['max_reward']:.2f}\")\n",
        "    print(f\"Mean episode length: {train_stats['mean_length']:.2f}\")\n",
        "\n",
        "    model_path = \"ppo_lunarlander3.pth\"\n",
        "\n",
        "    print(\"\\nStarting model testing...\")\n",
        "    test_storage = test_model(model_path, num_episodes=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "01e43b1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01e43b1a",
        "outputId": "eee1fe46-2a2a-4dc7-c1b6-9879bb308020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting model testing...\n",
            "Test Episode 1, Total Reward: 256.98\n",
            "Test Episode 2, Total Reward: 205.33\n",
            "Test Episode 3, Total Reward: 42.90\n",
            "Test Episode 4, Total Reward: 238.27\n",
            "Test Episode 5, Total Reward: -34.29\n"
          ]
        }
      ],
      "source": [
        "# testing the model with the saved params\n",
        "\n",
        "model_path = \"ppo_lunarlander3.pth\"\n",
        "\n",
        "print(\"\\nStarting model testing...\")\n",
        "test_storage = test_model(model_path, num_episodes=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7323c0",
      "metadata": {
        "id": "1f7323c0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba74e8dd",
      "metadata": {
        "id": "ba74e8dd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "quant",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
