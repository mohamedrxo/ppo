{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e27cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# this class define the storage buffer of the environment  \n",
    "\n",
    "class StorageBuffer:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.episode_rewards = []  # Store rewards for each episode\n",
    "        self.episode_lengths = []  # Store lengths for each episode\n",
    "        \n",
    "    def reset(self):\n",
    "        # Current episode storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.next_state = None\n",
    "        self.current_reward = 0\n",
    "        \n",
    "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "        self.current_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            self.next_state = next_state\n",
    "            self.episode_rewards.append(self.current_reward)\n",
    "            self.episode_lengths.append(len(self.rewards))\n",
    "            \n",
    "    def get_episode_data(self):\n",
    "        return {\n",
    "            'states': torch.FloatTensor(np.array(self.states)),\n",
    "            'actions': torch.tensor(self.actions),\n",
    "            'rewards': torch.tensor(self.rewards),\n",
    "            'log_probs': torch.tensor(self.log_probs),\n",
    "            'dones': torch.tensor(self.dones,dtype=torch.bool),\n",
    "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
    "        }\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        if not self.episode_rewards:\n",
    "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.episode_rewards),\n",
    "            \"max_reward\": np.max(self.episode_rewards),\n",
    "            \"min_reward\": np.min(self.episode_rewards),\n",
    "            \"mean_length\": np.mean(self.episode_lengths),\n",
    "            \"current_reward\": self.current_reward,\n",
    "            \"current_length\": len(self.rewards)\n",
    "        }\n",
    "\n",
    "#the policy and value networks\n",
    "# i used small model since it simple game but you can make it bigger if you want\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e6450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space observation_space 4\n",
      "action space 2\n"
     ]
    }
   ],
   "source": [
    "# this is our environment that will train our ppo agent on \n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "print('observation space observation_space',env.observation_space.shape[0])\n",
    "print('action space',env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1180de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "898ac482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "class PPOAgent():\n",
    "    def __init__(self, state_dim, action_dim, lr=5e-4, gamma=0.99, eps_clip=0.2, k_epochs=10, gae_lambda=0.95, entropy_coef=0.01, value_coef=0.5, device=None):\n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.old_policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        self.value_function = ValueNetwork(state_dim).to(self.device)\n",
    "        self.policy_optimizer = optim.AdamW(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.AdamW(self.value_function.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.k_epochs = k_epochs \n",
    "        self.eps_clip = eps_clip\n",
    "        self.entropy_coef=entropy_coef\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # this function used to select action by our old policy and returning the log_prob of it \n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action_prob = self.old_policy(state)\n",
    "        \n",
    "        disc = Categorical(action_prob)\n",
    "        action = disc.sample()\n",
    "        \n",
    "        return action.cpu(), disc.log_prob(action).cpu()\n",
    "\n",
    "    def compute_advantages(self, rewards, values, next_value, dones):\n",
    "        # this function calculate the advantage function\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - float(dones[t])) * gae\n",
    "            advantages.insert(0, gae)\n",
    "            next_value = values[t]\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'value_state_dict': self.value_function.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.old_policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.value_function.load_state_dict(checkpoint['value_state_dict'])\n",
    "    \n",
    "    def update(self, buffer,batch_size=32):\n",
    "        data = buffer.get_episode_data()\n",
    "        states = data['states'].to(self.device)\n",
    "        actions = data['actions'].to(self.device)\n",
    "        rewards = data['rewards']\n",
    "        dones = data['dones']\n",
    "        log_probs_old = data['log_probs'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.value_function(states).squeeze()\n",
    "            if data['next_state'] is not None:\n",
    "                next_state_tensor = data['next_state'].to(self.device)\n",
    "                next_values = self.value_function(next_state_tensor).item()\n",
    "            else:\n",
    "                next_values = 0.0\n",
    "        \n",
    "        advantages = self.compute_advantages(rewards, values.cpu(), next_values, dones)\n",
    "        advantages = advantages.to(self.device)\n",
    "        targets = advantages + values\n",
    "        \n",
    "        # Normalize advantages across the entire batch        \n",
    "\n",
    "        dataset = TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        entropy_losses = []\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            for batch in dataloader:\n",
    "                batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_targets = batch\n",
    "\n",
    "                # Policy loss with entropy bonus\n",
    "                action_probs = self.policy(batch_states)\n",
    "                dist = Categorical(action_probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs - batch_old_log_probs)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "\n",
    "                loss_actor = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy\n",
    "                \n",
    "                # Value loss\n",
    "                values_pred = self.value_function(batch_states).squeeze()\n",
    "                loss_critic = self.mse_loss(values_pred, batch_targets)\n",
    "\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.value_function.parameters(), 0.5)\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                policy_losses.append(loss_actor.item())\n",
    "                value_losses.append(loss_critic.item())\n",
    "                entropy_losses.append(entropy.item())\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        return {\n",
    "            'policy_loss': np.mean(policy_losses),\n",
    "            'value_loss': np.mean(value_losses),\n",
    "            'entropy': np.mean(entropy_losses),\n",
    "            'average_value': values.mean().item()\n",
    "        }\n",
    "\n",
    "def train_ppo(num_episodes=5000, max_time_steps=500, num_env=5, model_path=None, device=None):\n",
    "    # this function uses the interact with the environment and passes data to the update function to apply ppo on the model\n",
    "    import numpy as np\n",
    "    env = SyncVectorEnv([lambda: gym.make(\"CartPole-v1\") for _ in range(num_env)])\n",
    "    state_dim = env.single_observation_space.shape[0]\n",
    "    action_dim = env.single_action_space.n\n",
    "    best_reward = -float('inf')\n",
    "    \n",
    "    agent = PPOAgent(state_dim, action_dim, device=device)\n",
    "    # if model_path is not None:\n",
    "    #   agent.load_model(model_path)\n",
    "    reward_progress = []\n",
    "    \n",
    "    print(f\"Training on device: {agent.device}\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        storages = [StorageBuffer() for _ in range(num_env)]\n",
    "        states, _ = env.reset()\n",
    "        states = np.array(states)\n",
    "        episode_active = [True] * num_env\n",
    "        episode_rewards = np.zeros(num_env)\n",
    "        \n",
    "        for t in range(max_time_steps):\n",
    "            actions, old_log_probs = agent.select_action(states)\n",
    "\n",
    "            next_states, rewards, terminateds, truncateds, _ = env.step(actions.tolist())\n",
    "\n",
    "            dones = terminateds | truncateds\n",
    "            for i in range(num_env):\n",
    "                if episode_active[i]:\n",
    "                  \n",
    "                    episode_rewards[i]+=rewards[i]\n",
    "\n",
    "                    storages[i].add_step(\n",
    "                        state=states[i],\n",
    "                        action=actions[i],\n",
    "                        reward=rewards[i],\n",
    "                        log_prob=old_log_probs[i],\n",
    "                        done=dones[i],\n",
    "                        next_state=next_states[i] if not dones[i] else None\n",
    "                    )\n",
    "                    \n",
    "                    if dones[i]:\n",
    "                        episode_active[i] = False\n",
    "\n",
    "            states = next_states\n",
    "            if all(dones):\n",
    "                break\n",
    "        \n",
    "        for storage in storages:\n",
    "            if len(storage.rewards) > 0:\n",
    "                agent.update(storage,batch_size=20)\n",
    "                \n",
    "        avg_reward = episode_rewards.mean()\n",
    "        reward_progress.append(avg_reward)\n",
    "\n",
    "        if avg_reward >= best_reward:\n",
    "            agent.save_model(model_path)\n",
    "\n",
    "        if episode_rewards.max().tolist() >= best_reward:\n",
    "              best_reward = episode_rewards.max().tolist()\n",
    "        \n",
    "        if (episode + 1) % 10 == 0:\n",
    "          print(f\"Episode {episode + 1}, AVG Reward: {avg_reward:.3f}, Best Reward: {best_reward:.3f}\")\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    return agent, storages, reward_progress\n",
    "\n",
    "\n",
    "def test_model(model_path, num_episodes=10, device=None):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = PPOAgent(state_dim, action_dim, device=device)\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    print(f\"Testing on device: {agent.device}\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        storage = StorageBuffer()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(torch.tensor(state))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            storage.add_step(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                log_prob,\n",
    "                done,\n",
    "                next_state\n",
    "            )\n",
    "            state = next_state\n",
    "        \n",
    "        stats = storage.get_statistics()\n",
    "        print(f\"Test Episode {episode + 1}, Total Reward: {stats['current_reward']}\")\n",
    "    \n",
    "    env.close()\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2b615cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 200 episodes...\n",
      "Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_21636\\1681120133.py:46: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'dones': torch.tensor(self.dones,dtype=torch.bool),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, AVG Reward: 47.400, Best Reward: 98.000\n",
      "Episode 20, AVG Reward: 192.800, Best Reward: 395.000\n",
      "Episode 30, AVG Reward: 500.000, Best Reward: 500.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mcartpole_ppo2.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m episodes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m agent, train_storage,reward_progress = \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_env\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# print(\"\\nTraining completed! Training statistics:\")\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# train_stats = train_storage.get_statistics()\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# print(f\"Mean reward: {train_stats['mean_reward']:.2f}\")\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# print(f\"Max reward: {train_stats['max_reward']:.2f}\")\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# print(f\"Mean episode length: {train_stats['mean_length']:.2f}\")\u001b[39;00m\n\u001b[32m     16\u001b[39m plt.plot(reward_progress, label=\u001b[33m'\u001b[39m\u001b[33mEpisode Reward\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(num_episodes, max_time_steps, num_env, model_path, device)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m storage \u001b[38;5;129;01min\u001b[39;00m storages:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(storage.rewards) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m avg_reward = episode_rewards.mean()\n\u001b[32m    190\u001b[39m reward_progress.append(avg_reward)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mPPOAgent.update\u001b[39m\u001b[34m(self, buffer, batch_size)\u001b[39m\n\u001b[32m     91\u001b[39m entropy_losses = []\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.k_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_old_log_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_advantages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Policy loss with entropy bonus\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if __name__ == \"__main__\":\n",
    "    # this is for training and testing the model\n",
    "    # since this environment only gives reward at the end of the episode we need to give it more training\n",
    "    num_episodes = 200  # You can change this number\n",
    "    model_path = \"cartpole_ppo2.pth\"\n",
    "    print(f\"Starting training for {num_episodes} episodes...\")\n",
    "    agent, train_storage,reward_progress = train_ppo(num_episodes=num_episodes,num_env=5,model_path=model_path)\n",
    "    \n",
    "    # print(\"\\nTraining completed! Training statistics:\")\n",
    "    # train_stats = train_storage.get_statistics()\n",
    "    # print(f\"Mean reward: {train_stats['mean_reward']:.2f}\")\n",
    "    # print(f\"Max reward: {train_stats['max_reward']:.2f}\")\n",
    "    # print(f\"Mean episode length: {train_stats['mean_length']:.2f}\")\n",
    "    \n",
    "    plt.plot(reward_progress, label='Episode Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward Progress Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nStarting model testing...\")\n",
    "    test_storage = test_model(model_path, num_episodes=5)\n",
    "    \n",
    "    print(\"\\nTesting completed! Test statistics:\")\n",
    "    test_stats = test_storage.get_statistics()\n",
    "    print(f\"Mean test reward: {test_stats['mean_reward']:.2f}\")\n",
    "    print(f\"Max test reward: {test_stats['max_reward']:.2f}\")\n",
    "    print(f\"Min test reward: {test_stats['min_reward']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250bbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_21636\\2942341419.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1, Total Reward: 500.0\n",
      "Test Episode 2, Total Reward: 485.0\n",
      "Test Episode 3, Total Reward: 500.0\n",
      "Test Episode 4, Total Reward: 500.0\n",
      "Test Episode 5, Total Reward: 477.0\n"
     ]
    }
   ],
   "source": [
    "test_storage = test_model(model_path, num_episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
