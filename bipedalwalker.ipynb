{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JDy7dXK6KdH",
        "outputId": "8c6644b7-56ee-46e5-f11d-bd31866da5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 2s (495 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2351177 sha256=5bd93d71f77741fdf546a020fc3ba4dbdcafa482ede5cf0ca8950ade30402f96\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-py-2.3.8\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d box2d-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H78N2vBd6Lo8",
        "outputId": "88c3e30b-6264-4fe6-a400-022f2dc289ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351224 sha256=5ba771b790dd6bff517706571c2891d77975b07ecea079145a5a0adda014e019\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "  Attempting uninstall: box2d-py\n",
            "    Found existing installation: box2d-py 2.3.8\n",
            "    Uninstalling box2d-py-2.3.8:\n",
            "      Successfully uninstalled box2d-py-2.3.8\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K63eots4Y9EA",
        "outputId": "a1f20f7e-39e3-435a-97a9-898cb83cd3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Heu3h2k46aHo"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# this class define the storage buffer of the environment\n",
        "\n",
        "class StorageBuffer:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.episode_rewards = []  # Store rewards for each episode\n",
        "        self.episode_lengths = []  # Store lengths for each episode\n",
        "\n",
        "    def reset(self):\n",
        "        # Current episode storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.next_state = None\n",
        "        self.current_reward = 0\n",
        "\n",
        "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "        self.current_reward += reward\n",
        "\n",
        "        if done:\n",
        "            self.next_state = next_state\n",
        "            self.episode_rewards.append(self.current_reward)\n",
        "            self.episode_lengths.append(len(self.rewards))\n",
        "\n",
        "    def get_episode_data(self):\n",
        "        return {\n",
        "            'states': torch.FloatTensor(np.array(self.states)),\n",
        "            'actions': torch.tensor(self.actions),\n",
        "            'rewards': torch.tensor(self.rewards),\n",
        "            'log_probs': torch.tensor(self.log_probs),\n",
        "            'dones': torch.tensor(self.dones),\n",
        "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
        "        }\n",
        "\n",
        "    def get_statistics(self):\n",
        "\n",
        "        if not self.episode_rewards:\n",
        "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
        "\n",
        "        return {\n",
        "            \"mean_reward\": np.mean(self.episode_rewards),\n",
        "            \"max_reward\": np.max(self.episode_rewards),\n",
        "            \"min_reward\": np.min(self.episode_rewards),\n",
        "            \"mean_length\": np.mean(self.episode_lengths),\n",
        "            \"current_reward\": self.current_reward,\n",
        "            \"current_length\": len(self.rewards)\n",
        "        }\n",
        "\n",
        "#the policy and value networks\n",
        "# i used small model since it simple game but you can make it bigger if you want\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, action_dim),\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean =   self.fc(x)\n",
        "        std = self.log_std.exp()\n",
        "        return mean,std\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zHTcUCwj6zkL"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Normal\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=10):\n",
        "\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.old_policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "        self.value_net = ValueNetwork(state_dim)\n",
        "\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.k_epochs = k_epochs\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            mean, std = self.old_policy(state)\n",
        "\n",
        "        dist = Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        action_log_prob = dist.log_prob(action).sum(dim=-1)  # Sum for multi-dimensional actions\n",
        "        return action.numpy()[0], action_log_prob.numpy()[0]\n",
        "\n",
        "    def compute_advantages(self, rewards, values, next_value, dones, lambda_gae=0.95):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
        "            gae = delta + self.gamma * lambda_gae * (1 - float(dones[t])) * gae\n",
        "            advantages.insert(0, gae)\n",
        "            next_value = values[t]\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float32)\n",
        "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'old_policy_state_dict': self.old_policy.state_dict(),\n",
        "            'value_state_dict': self.value_net.state_dict(),\n",
        "            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),\n",
        "            'value_optimizer_state_dict': self.value_optimizer.state_dict()\n",
        "        }, filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.old_policy.load_state_dict(checkpoint['old_policy_state_dict'])\n",
        "        self.value_net.load_state_dict(checkpoint['value_state_dict'])\n",
        "        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
        "        self.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])\n",
        "\n",
        "        # Ensure all models are in eval mode for testing\n",
        "        self.policy.eval()\n",
        "        self.old_policy.eval()\n",
        "        self.value_net.eval()\n",
        "\n",
        "    def update(self, buffer, batch_size=64):\n",
        "        data = buffer.get_episode_data()\n",
        "        states = torch.tensor(data['states'], dtype=torch.float32)\n",
        "        actions = torch.tensor(data['actions'], dtype=torch.float32)\n",
        "        log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n",
        "        rewards = data['rewards']\n",
        "        dones = data['dones']\n",
        "\n",
        "        values = self.value_net(states).squeeze().detach()\n",
        "        next_value = self.value_net(torch.tensor(data['next_state'], dtype=torch.float32)).item() if data['next_state'] else 0.0\n",
        "        advantages = self.compute_advantages(rewards, values, next_value, dones)\n",
        "        targets = advantages + values\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for _ in range(self.k_epochs):\n",
        "            for batch_states, batch_actions, batch_log_probs_old, batch_advantages, batch_targets in dataloader:\n",
        "\n",
        "                mean, std = self.policy(batch_states)\n",
        "                values_pred  =  self.value_net(batch_states)\n",
        "                dist = Normal(mean, std)\n",
        "                log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
        "\n",
        "                ratios = torch.exp(log_probs - batch_log_probs_old)\n",
        "                surr1 = ratios * batch_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
        "                loss_actor = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                loss_critic = self.mse_loss(values_pred.squeeze(), batch_targets)\n",
        "\n",
        "                self.policy_optimizer.zero_grad()\n",
        "                loss_actor.backward(retain_graph=True)\n",
        "                self.policy_optimizer.step()\n",
        "\n",
        "                self.value_optimizer.zero_grad()\n",
        "                loss_critic.backward()\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xTkbnhH073ZF"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "def train_ppo(env_name=\"BipedalWalker-v3\", num_episodes=2000, max_timesteps=2000):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    buffer = StorageBuffer()\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    mean_raward = []\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        buffer.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(max_timesteps):\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "\n",
        "            buffer.add_step(state, action, reward, log_prob, done, next_state if not done else None)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        mean_raward.append(episode_reward)\n",
        "        # wandb.log({\"episode_reward\":episode_reward})\n",
        "\n",
        "        agent.update(buffer)\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            agent.save_model(\"bipedalwalker.pth\")\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}, mean raward: {statistics.mean(mean_raward)}, Best: {best_reward:.2f}\")\n",
        "            mean_raward = []\n",
        "\n",
        "    env.close()\n",
        "    return agent\n",
        "\n",
        "\n",
        "def test_model(model_path, env_name=\"BipedalWalker-v3\", num_episodes=10):\n",
        "    env = gym.make(env_name, render_mode=\"human\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    agent.load_model(model_path)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Test Episode {episode + 1}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOwk5RyM8FJb",
        "outputId": "f6a44c89-ca1c-40ca-d349-1817c7dc060e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-46d81eb0ba57>:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  'actions': torch.tensor(self.actions),\n",
            "<ipython-input-4-117b7c798710>:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(data['states'], dtype=torch.float32)\n",
            "<ipython-input-4-117b7c798710>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actions = torch.tensor(data['actions'], dtype=torch.float32)\n",
            "<ipython-input-4-117b7c798710>:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10, mean raward: -99.93580009136424, Best: -72.34\n",
            "Episode 20, mean raward: -112.6236916339224, Best: -72.34\n",
            "Episode 30, mean raward: -116.88318935659956, Best: -70.27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 40, mean raward: -115.28107283723963, Best: -70.27\n",
            "Episode 50, mean raward: -116.9066574259234, Best: -70.27\n",
            "Episode 60, mean raward: -113.79672936876932, Best: -70.27\n",
            "Episode 70, mean raward: -116.17572300497957, Best: -70.27\n",
            "Episode 80, mean raward: -110.02570149639239, Best: -70.27\n",
            "Episode 90, mean raward: -91.84736049857655, Best: -68.72\n",
            "Episode 100, mean raward: -116.15181642928626, Best: -68.72\n",
            "Episode 110, mean raward: -100.82962420187887, Best: -68.72\n",
            "Episode 120, mean raward: -101.63209073847503, Best: -68.72\n",
            "Episode 130, mean raward: -106.48455070313408, Best: -68.72\n",
            "Episode 140, mean raward: -108.48783668053439, Best: -68.72\n",
            "Episode 150, mean raward: -105.99701019759694, Best: -68.72\n",
            "Episode 160, mean raward: -104.65312283607486, Best: -68.72\n",
            "Episode 170, mean raward: -104.33744079761257, Best: -68.72\n",
            "Episode 180, mean raward: -105.23578079898947, Best: -68.72\n",
            "Episode 190, mean raward: -112.47844194968144, Best: -68.72\n",
            "Episode 200, mean raward: -114.34771116834094, Best: -68.72\n",
            "Episode 210, mean raward: -114.60988629704829, Best: -68.72\n",
            "Episode 220, mean raward: -117.19093961702768, Best: -68.72\n",
            "Episode 230, mean raward: -110.1263799056666, Best: -68.72\n",
            "Episode 240, mean raward: -112.59604296247176, Best: -68.72\n",
            "Episode 250, mean raward: -103.53216426124182, Best: -68.72\n",
            "Episode 260, mean raward: -114.41546387623409, Best: -68.72\n",
            "Episode 270, mean raward: -107.02523221787945, Best: -68.72\n",
            "Episode 280, mean raward: -112.87405797236961, Best: -68.72\n",
            "Episode 290, mean raward: -115.47321461474874, Best: -68.72\n",
            "Episode 300, mean raward: -109.33648202064849, Best: -68.72\n",
            "Episode 310, mean raward: -127.44930572536083, Best: -68.72\n",
            "Episode 320, mean raward: -120.67622575092426, Best: -68.72\n",
            "Episode 330, mean raward: -112.10968184035563, Best: -68.72\n",
            "Episode 340, mean raward: -116.6376422986014, Best: -68.72\n",
            "Episode 350, mean raward: -126.41176336192584, Best: -68.72\n",
            "Episode 360, mean raward: -149.1200569007518, Best: -68.72\n",
            "Episode 370, mean raward: -117.3171814352971, Best: -68.72\n",
            "Episode 380, mean raward: -111.88274800618511, Best: -68.72\n",
            "Episode 390, mean raward: -105.941894632293, Best: -68.72\n",
            "Episode 400, mean raward: -75.5374925072437, Best: -61.66\n",
            "Episode 410, mean raward: -55.3604061025612, Best: -29.36\n",
            "Episode 420, mean raward: -99.43055343384792, Best: -29.36\n",
            "Episode 430, mean raward: -120.82137873655208, Best: -29.36\n",
            "Episode 440, mean raward: -120.16555840892725, Best: -29.36\n",
            "Episode 450, mean raward: -104.09113306407089, Best: -29.36\n",
            "Episode 460, mean raward: -103.31496102790466, Best: -29.36\n",
            "Episode 470, mean raward: -54.79676613058515, Best: -29.36\n",
            "Episode 480, mean raward: -54.96899523303981, Best: -15.16\n",
            "Episode 490, mean raward: -60.2244796170662, Best: -15.16\n",
            "Episode 500, mean raward: -41.45980655661727, Best: -15.16\n",
            "Episode 510, mean raward: -51.675553410174345, Best: -15.16\n",
            "Episode 520, mean raward: -56.75620007077041, Best: -15.16\n",
            "Episode 530, mean raward: -65.89302804290888, Best: 7.31\n",
            "Episode 540, mean raward: -29.262285207508047, Best: 7.31\n",
            "Episode 550, mean raward: -27.396441464644603, Best: 7.31\n",
            "Episode 560, mean raward: -36.96384882509298, Best: 7.31\n",
            "Episode 570, mean raward: -17.237260639395807, Best: 23.92\n",
            "Episode 580, mean raward: -45.99000660813209, Best: 23.92\n",
            "Episode 590, mean raward: -16.08718452346861, Best: 43.16\n",
            "Episode 600, mean raward: 17.874225475989704, Best: 58.56\n",
            "Episode 610, mean raward: -13.753691096289483, Best: 58.56\n",
            "Episode 620, mean raward: -8.83127907025939, Best: 58.56\n",
            "Episode 630, mean raward: -75.96989116751868, Best: 58.56\n",
            "Episode 640, mean raward: -91.56472537224325, Best: 58.56\n",
            "Episode 650, mean raward: -32.20265186266675, Best: 58.56\n",
            "Episode 660, mean raward: -64.48078734350463, Best: 58.56\n",
            "Episode 670, mean raward: -57.9325151121111, Best: 58.56\n",
            "Episode 680, mean raward: -28.539824288397973, Best: 58.56\n",
            "Episode 690, mean raward: -7.014062711345195, Best: 58.56\n",
            "Episode 700, mean raward: -91.62841659386457, Best: 58.56\n",
            "Episode 710, mean raward: -13.602353788708543, Best: 58.56\n",
            "Episode 720, mean raward: -8.779635519337003, Best: 58.56\n",
            "Episode 730, mean raward: -0.3569676612904189, Best: 67.21\n",
            "Episode 740, mean raward: 1.986485528399027, Best: 67.21\n",
            "Episode 750, mean raward: -7.435480784179958, Best: 67.21\n",
            "Episode 760, mean raward: 1.0268122359884087, Best: 67.21\n",
            "Episode 770, mean raward: -45.32813675373111, Best: 67.21\n",
            "Episode 780, mean raward: -4.9686196495609884, Best: 77.88\n",
            "Episode 790, mean raward: -2.655992980652236, Best: 77.88\n",
            "Episode 800, mean raward: -60.96391962706851, Best: 83.64\n",
            "Episode 810, mean raward: -5.668947559132897, Best: 85.21\n",
            "Episode 820, mean raward: -24.81665658800995, Best: 85.21\n",
            "Episode 830, mean raward: -38.03008327435538, Best: 85.21\n",
            "Episode 840, mean raward: -29.382251071878933, Best: 88.47\n",
            "Episode 850, mean raward: 10.996699310127568, Best: 88.47\n",
            "Episode 860, mean raward: 15.476879051612345, Best: 88.47\n",
            "Episode 870, mean raward: -2.2739525579951603, Best: 88.47\n",
            "Episode 880, mean raward: -2.5991092033004626, Best: 88.47\n",
            "Episode 890, mean raward: 8.347511139978193, Best: 88.47\n",
            "Episode 900, mean raward: 32.11630888244662, Best: 88.47\n",
            "Episode 910, mean raward: 17.085356629547597, Best: 88.47\n",
            "Episode 920, mean raward: -3.98518915758087, Best: 88.47\n",
            "Episode 930, mean raward: 16.803333819769957, Best: 88.47\n",
            "Episode 940, mean raward: 3.3506500043133842, Best: 90.94\n",
            "Episode 950, mean raward: 27.89612003815936, Best: 94.41\n",
            "Episode 960, mean raward: -23.67855562361332, Best: 94.41\n",
            "Episode 970, mean raward: -16.398316393538106, Best: 94.41\n",
            "Episode 980, mean raward: -34.825788522889845, Best: 94.41\n",
            "Episode 990, mean raward: 32.30882170595304, Best: 94.41\n",
            "Episode 1000, mean raward: 24.674922537367287, Best: 94.41\n",
            "Episode 1010, mean raward: -21.80067606585077, Best: 94.41\n",
            "Episode 1020, mean raward: 21.123031202367986, Best: 94.41\n",
            "Episode 1030, mean raward: 0.08194468307185394, Best: 94.41\n",
            "Episode 1040, mean raward: 14.721129470644657, Best: 94.41\n",
            "Episode 1050, mean raward: -35.46514657556479, Best: 94.41\n",
            "Episode 1060, mean raward: -7.452697600161306, Best: 94.41\n",
            "Episode 1070, mean raward: 37.91463932604714, Best: 94.41\n",
            "Episode 1080, mean raward: 16.08413699069254, Best: 94.41\n",
            "Episode 1090, mean raward: -12.6472635692248, Best: 94.41\n",
            "Episode 1100, mean raward: -28.967461663076143, Best: 94.41\n",
            "Episode 1110, mean raward: 3.995105105937572, Best: 94.41\n",
            "Episode 1120, mean raward: -25.77450543937918, Best: 94.41\n",
            "Episode 1130, mean raward: -25.92428491207593, Best: 94.41\n",
            "Episode 1140, mean raward: 18.508418722207317, Best: 94.41\n",
            "Episode 1150, mean raward: 1.7151984658254988, Best: 102.39\n",
            "Episode 1160, mean raward: -24.70312561069782, Best: 102.39\n",
            "Episode 1170, mean raward: -19.51574849223522, Best: 102.39\n",
            "Episode 1180, mean raward: -72.72151520927001, Best: 102.39\n",
            "Episode 1190, mean raward: -3.2682722428374573, Best: 102.39\n",
            "Episode 1200, mean raward: 23.99010599630087, Best: 102.39\n",
            "Episode 1210, mean raward: 22.20176041037604, Best: 102.39\n",
            "Episode 1220, mean raward: -6.92998725326536, Best: 102.39\n",
            "Episode 1230, mean raward: 7.024563033206499, Best: 102.39\n",
            "Episode 1240, mean raward: -22.905966373546523, Best: 102.39\n",
            "Episode 1250, mean raward: 21.255364010909915, Best: 102.39\n",
            "Episode 1260, mean raward: 0.03280657287650328, Best: 102.39\n",
            "Episode 1270, mean raward: 13.9542998170276, Best: 110.63\n",
            "Episode 1280, mean raward: 10.20880897279452, Best: 117.36\n",
            "Episode 1290, mean raward: -62.621487322256854, Best: 117.36\n",
            "Episode 1300, mean raward: 17.496759138784284, Best: 125.76\n",
            "Episode 1310, mean raward: -27.085843144877565, Best: 125.76\n",
            "Episode 1320, mean raward: -44.858725347255984, Best: 125.76\n",
            "Episode 1330, mean raward: -22.582434215700793, Best: 125.76\n",
            "Episode 1340, mean raward: -57.76002486813762, Best: 125.76\n",
            "Episode 1350, mean raward: -25.867924500268057, Best: 125.76\n",
            "Episode 1360, mean raward: -10.241445025677274, Best: 125.76\n",
            "Episode 1370, mean raward: -34.395901013479175, Best: 125.76\n",
            "Episode 1380, mean raward: -106.96791745684102, Best: 125.76\n",
            "Episode 1390, mean raward: -110.5558597256427, Best: 125.76\n",
            "Episode 1400, mean raward: -54.32796452447618, Best: 125.76\n",
            "Episode 1410, mean raward: -79.51435768432745, Best: 125.76\n",
            "Episode 1420, mean raward: -26.328128551319857, Best: 125.76\n",
            "Episode 1430, mean raward: -51.18755684149806, Best: 125.76\n",
            "Episode 1440, mean raward: -68.04051867997062, Best: 125.76\n",
            "Episode 1450, mean raward: -25.35986474327604, Best: 125.76\n",
            "Episode 1460, mean raward: -83.00740476575047, Best: 125.76\n",
            "Episode 1470, mean raward: -124.3009397549094, Best: 125.76\n",
            "Episode 1480, mean raward: -120.20496975577878, Best: 125.76\n",
            "Episode 1490, mean raward: -101.73320302876046, Best: 125.76\n",
            "Episode 1500, mean raward: -105.09581732825255, Best: 125.76\n",
            "Episode 1510, mean raward: -78.59722392265097, Best: 125.76\n",
            "Episode 1520, mean raward: -58.19689038365141, Best: 125.76\n",
            "Episode 1530, mean raward: -118.15599701081558, Best: 125.76\n",
            "Episode 1540, mean raward: -122.08950984554441, Best: 125.76\n",
            "Episode 1550, mean raward: -99.69831767005483, Best: 125.76\n",
            "Episode 1560, mean raward: -84.7964991886926, Best: 125.76\n",
            "Episode 1570, mean raward: -82.85102773942775, Best: 125.76\n",
            "Episode 1580, mean raward: -75.89442219085004, Best: 125.76\n",
            "Episode 1590, mean raward: -91.74783172843537, Best: 125.76\n",
            "Episode 1600, mean raward: -46.30341515785632, Best: 125.76\n",
            "Episode 1610, mean raward: -70.5659436827521, Best: 125.76\n",
            "Episode 1620, mean raward: -64.02581143778076, Best: 125.76\n",
            "Episode 1630, mean raward: -88.05562990323082, Best: 125.76\n",
            "Episode 1640, mean raward: -15.430888007068193, Best: 125.76\n",
            "Episode 1650, mean raward: -53.954464463713975, Best: 125.76\n",
            "Episode 1660, mean raward: -57.82857912510747, Best: 125.76\n",
            "Episode 1670, mean raward: -75.52774614516001, Best: 125.76\n",
            "Episode 1680, mean raward: -12.510904600669047, Best: 125.76\n",
            "Episode 1690, mean raward: -26.09094962690882, Best: 125.76\n",
            "Episode 1700, mean raward: 4.512114161675959, Best: 125.76\n",
            "Episode 1710, mean raward: -34.04197457488998, Best: 125.76\n",
            "Episode 1720, mean raward: -31.405143329857573, Best: 125.76\n",
            "Episode 1730, mean raward: -69.2596939263016, Best: 125.76\n",
            "Episode 1740, mean raward: -61.907260545578374, Best: 125.76\n",
            "Episode 1750, mean raward: -97.12904603182733, Best: 125.76\n",
            "Episode 1760, mean raward: -53.65400975471345, Best: 125.76\n",
            "Episode 1770, mean raward: -42.751885691789255, Best: 125.76\n",
            "Episode 1780, mean raward: -8.06373511003196, Best: 125.76\n",
            "Episode 1790, mean raward: 11.013138567857567, Best: 125.76\n",
            "Episode 1800, mean raward: 18.738652068102667, Best: 125.76\n",
            "Episode 1810, mean raward: 57.455653290237066, Best: 136.41\n",
            "Episode 1820, mean raward: -1.2871066774712916, Best: 136.41\n",
            "Episode 1830, mean raward: -11.155696154160264, Best: 136.41\n",
            "Episode 1840, mean raward: 20.56617950726109, Best: 136.41\n",
            "Episode 1850, mean raward: 17.260586562233435, Best: 136.41\n",
            "Episode 1860, mean raward: -24.205362090639692, Best: 136.41\n",
            "Episode 1870, mean raward: -93.34552537293193, Best: 136.41\n",
            "Episode 1880, mean raward: -60.24115101294785, Best: 136.41\n",
            "Episode 1890, mean raward: -28.55459030279275, Best: 136.41\n",
            "Episode 1900, mean raward: -17.55704439495153, Best: 136.41\n",
            "Episode 1910, mean raward: 21.776043135814056, Best: 136.41\n",
            "Episode 1920, mean raward: -12.308346035051635, Best: 136.41\n",
            "Episode 1930, mean raward: -28.988922637701037, Best: 136.41\n",
            "Episode 1940, mean raward: -15.593428106948103, Best: 136.41\n",
            "Episode 1950, mean raward: -83.96699121354179, Best: 136.41\n",
            "Episode 1960, mean raward: -63.97284944407794, Best: 136.41\n",
            "Episode 1970, mean raward: -55.451488307071564, Best: 136.41\n",
            "Episode 1980, mean raward: -44.89794818626583, Best: 136.41\n",
            "Episode 1990, mean raward: -67.25825734255517, Best: 136.41\n",
            "Episode 2000, mean raward: -22.451889811453277, Best: 136.41\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.PPOAgent at 0x7fef145e1110>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ppo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0PwtdbL8v0o",
        "outputId": "0883b2c6-8bde-44ae-d132-6540d62df00a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9224\\4034181294.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Episode 1, Total Reward: 110.19\n",
            "Test Episode 2, Total Reward: 116.27\n",
            "Test Episode 3, Total Reward: 110.63\n",
            "Test Episode 4, Total Reward: 114.90\n",
            "Test Episode 5, Total Reward: -17.01\n",
            "Test Episode 6, Total Reward: 109.05\n",
            "Test Episode 7, Total Reward: 83.73\n",
            "Test Episode 8, Total Reward: 106.99\n",
            "Test Episode 9, Total Reward: 96.32\n",
            "Test Episode 10, Total Reward: 103.72\n"
          ]
        }
      ],
      "source": [
        "model_path  = \"bipedalwalker.pth\"\n",
        "test_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H_JuquMWwHd",
        "outputId": "68e1d4fb-2791-4f4c-87c1-9086693ff85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
