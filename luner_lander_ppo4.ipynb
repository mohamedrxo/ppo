{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a61dae0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a61dae0",
        "outputId": "625844b7-d00f-4638-adc4-3362e9c6d06d"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# this class define the storage buffer of the environment\n",
        "\n",
        "class StorageBuffer:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.episode_rewards = []  # Store rewards for each episode\n",
        "        self.episode_lengths = []  # Store lengths for each episode\n",
        "\n",
        "    def reset(self):\n",
        "        # Current episode storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.next_state = None\n",
        "        self.current_reward = 0\n",
        "\n",
        "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "        self.current_reward += reward\n",
        "\n",
        "\n",
        "        if done:\n",
        "            self.next_state = next_state\n",
        "            self.episode_rewards.append(self.current_reward)\n",
        "            self.episode_lengths.append(len(self.rewards))\n",
        "\n",
        "    def get_episode_data(self):\n",
        "        return {\n",
        "            'states': torch.FloatTensor(np.array(self.states)),\n",
        "            'actions': torch.tensor(self.actions),\n",
        "            'rewards': torch.tensor(self.rewards),\n",
        "            'log_probs': torch.tensor(self.log_probs),\n",
        "            'dones': torch.tensor(self.dones,dtype=torch.bool),\n",
        "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
        "        }\n",
        "\n",
        "    def get_statistics(self):\n",
        "        if not self.episode_rewards:\n",
        "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
        "\n",
        "        return {\n",
        "            \"mean_reward\": np.mean(self.episode_rewards),\n",
        "            \"max_reward\": np.max(self.episode_rewards),\n",
        "            \"min_reward\": np.min(self.episode_rewards),\n",
        "            \"mean_length\": np.mean(self.episode_lengths),\n",
        "            \"current_reward\": self.current_reward,\n",
        "            \"current_length\": len(self.rewards)\n",
        "        }\n",
        "\n",
        "#the policy and value networks\n",
        "# i used small model since it simple game but you can make it bigger if you want\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ypApTfTA9Fzd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypApTfTA9Fzd",
        "outputId": "cdae2e76-bc4f-49ae-c4c8-b12a7498794c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1.post0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1.post0\n"
          ]
        }
      ],
      "source": [
        "pip install swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9Qs5HZKj9OrD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qs5HZKj9OrD",
        "outputId": "a6fc9038-d8fe-49f8-e1e7-8fe1079c2190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.3.1.post0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2409496 sha256=50489730bf7fc8f014c5b19c0ef371e99a5a8d84bf29778ee00da847f4a163d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "pip install \"gymnasium[box2d]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "69514e0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69514e0d",
        "outputId": "bbfb55be-93a3-4846-805f-59f2e8502c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "observation space observation_space 8\n",
            "action space 4\n"
          ]
        }
      ],
      "source": [
        "# this is our environment that will train our ppo agent on\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
        "observation, info = env.reset()\n",
        "\n",
        "episode_over = False\n",
        "while not episode_over:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    episode_over = terminated or truncated\n",
        "\n",
        "env.close()\n",
        "print('observation space observation_space',env.observation_space.shape[0])\n",
        "print('action space',env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "hmc-rYZM9sMU",
      "metadata": {
        "id": "hmc-rYZM9sMU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "679743d8",
      "metadata": {
        "id": "679743d8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from gymnasium.vector import SyncVectorEnv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PPOAgent():\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, eps_clip=0.2, k_epochs=10, gae_lambda=0.95, entropy_coef=0.01, value_coef=0.5, device=None):\n",
        "        # Set device\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.old_policy = PolicyNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "        self.value_function = ValueNetwork(state_dim).to(self.device)\n",
        "        self.policy_optimizer = optim.AdamW(self.policy.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.AdamW(self.value_function.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.k_epochs = k_epochs\n",
        "        self.eps_clip = eps_clip\n",
        "        self.entropy_coef=entropy_coef\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # this function used to select action by our old policy and returning the log_prob of it\n",
        "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action_prob = self.old_policy(state)\n",
        "\n",
        "        disc = Categorical(action_prob)\n",
        "        action = disc.sample()\n",
        "\n",
        "        return action.cpu(), disc.log_prob(action).cpu()\n",
        "\n",
        "    def compute_advantages(self, rewards, values, next_value, dones):\n",
        "        # this function calculate the advantage function\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
        "            gae = delta + self.gamma * self.gae_lambda * (1 - float(dones[t])) * gae\n",
        "            advantages.insert(0, gae)\n",
        "            next_value = values[t]\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'value_state_dict': self.value_function.state_dict(),\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.old_policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.value_function.load_state_dict(checkpoint['value_state_dict'])\n",
        "\n",
        "    def update(self, buffer,batch_size=32):\n",
        "        data = buffer.get_episode_data()\n",
        "        states = data['states'].to(self.device)\n",
        "        actions = data['actions'].to(self.device)\n",
        "        rewards = data['rewards']\n",
        "        dones = data['dones']\n",
        "        log_probs_old = data['log_probs'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            values = self.value_function(states).squeeze()\n",
        "            if data['next_state'] is not None:\n",
        "                next_state_tensor = data['next_state'].to(self.device)\n",
        "                next_values = self.value_function(next_state_tensor).item()\n",
        "            else:\n",
        "                next_values = 0.0\n",
        "\n",
        "        advantages = self.compute_advantages(rewards, values.cpu(), next_values, dones)\n",
        "        advantages = advantages.to(self.device)\n",
        "        targets = advantages + values\n",
        "\n",
        "        # Normalize advantages across the entire batch\n",
        "\n",
        "        dataset = TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        policy_losses = []\n",
        "        value_losses = []\n",
        "        entropy_losses = []\n",
        "\n",
        "        for _ in range(self.k_epochs):\n",
        "            for batch in dataloader:\n",
        "                batch_states, batch_actions, batch_old_log_probs, batch_advantages, batch_targets = batch\n",
        "\n",
        "                # Policy loss with entropy bonus\n",
        "                action_probs = self.policy(batch_states)\n",
        "                dist = Categorical(action_probs)\n",
        "                log_probs = dist.log_prob(batch_actions)\n",
        "                entropy = dist.entropy().mean()\n",
        "\n",
        "                ratios = torch.exp(log_probs - batch_old_log_probs)\n",
        "                surr1 = ratios * batch_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
        "\n",
        "                loss_actor = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy\n",
        "\n",
        "                # Value loss\n",
        "                values_pred = self.value_function(batch_states).squeeze()\n",
        "                loss_critic = self.mse_loss(values_pred, batch_targets)\n",
        "\n",
        "                self.policy_optimizer.zero_grad()\n",
        "                loss_actor.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
        "                self.policy_optimizer.step()\n",
        "\n",
        "                self.value_optimizer.zero_grad()\n",
        "                loss_critic.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.value_function.parameters(), 0.5)\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "                policy_losses.append(loss_actor.item())\n",
        "                value_losses.append(loss_critic.item())\n",
        "                entropy_losses.append(entropy.item())\n",
        "\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        return {\n",
        "            'policy_loss': np.mean(policy_losses),\n",
        "            'value_loss': np.mean(value_losses),\n",
        "            'entropy': np.mean(entropy_losses),\n",
        "            'average_value': values.mean().item()\n",
        "        }\n",
        "\n",
        "def train_ppo(num_episodes=5000, max_time_steps=200, num_env=5, model_path=None, device=None):\n",
        "    # this function uses the interact with the environment and passes data to the update function to apply ppo on the model\n",
        "    import numpy as np\n",
        "    env = SyncVectorEnv([lambda: gym.make(\"LunarLander-v3\") for _ in range(num_env)])\n",
        "    state_dim = env.single_observation_space.shape[0]\n",
        "    action_dim = env.single_action_space.n\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    agent = PPOAgent(state_dim, action_dim, device=device)\n",
        "    if model_path is not None:\n",
        "      agent.load_model(model_path)\n",
        "    reward_progress = []\n",
        "\n",
        "    print(f\"Training on device: {agent.device}\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        storages = [StorageBuffer() for _ in range(num_env)]\n",
        "        states, _ = env.reset()\n",
        "        states = np.array(states)\n",
        "        episode_active = [True] * num_env\n",
        "        episode_rewards = np.zeros(num_env)\n",
        "\n",
        "        for t in range(max_time_steps):\n",
        "            actions, old_log_probs = agent.select_action(states)\n",
        "\n",
        "            next_states, rewards, terminateds, truncateds, _ = env.step(actions.tolist())\n",
        "\n",
        "            dones = terminateds | truncateds\n",
        "            for i in range(num_env):\n",
        "                if episode_active[i]:\n",
        "\n",
        "                    episode_rewards[i]+=rewards[i]\n",
        "\n",
        "                    storages[i].add_step(\n",
        "                        state=states[i],\n",
        "                        action=actions[i],\n",
        "                        reward=rewards[i],\n",
        "                        log_prob=old_log_probs[i],\n",
        "                        done=dones[i],\n",
        "                        next_state=next_states[i] if not dones[i] else None\n",
        "                    )\n",
        "\n",
        "                    if dones[i]:\n",
        "                        episode_active[i] = False\n",
        "\n",
        "            states = next_states\n",
        "            if all(dones):\n",
        "                break\n",
        "\n",
        "        for storage in storages:\n",
        "            if len(storage.rewards) > 0:\n",
        "                agent.update(storage,batch_size=300)\n",
        "\n",
        "        avg_reward = episode_rewards.mean()\n",
        "        reward_progress.append(avg_reward)\n",
        "\n",
        "        if avg_reward >= best_reward:\n",
        "            agent.save_model(model_path)\n",
        "\n",
        "        if episode_rewards.max().tolist() >= best_reward:\n",
        "              best_reward = episode_rewards.max().tolist()\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "          print(f\"Episode {episode + 1}, AVG Reward: {avg_reward:.3f}, Best Reward: {best_reward:.3f}\")\n",
        "\n",
        "\n",
        "    env.close()\n",
        "    return agent, storages, reward_progress\n",
        "\n",
        "\n",
        "def test_model(model_path, num_episodes=10, device=None):\n",
        "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = PPOAgent(state_dim, action_dim, device=device)\n",
        "    agent.load_model(model_path)\n",
        "\n",
        "    print(f\"Testing on device: {agent.device}\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        storage = StorageBuffer()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = agent.select_action(torch.tensor(state))\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            storage.add_step(\n",
        "                state,\n",
        "                action,\n",
        "                reward,\n",
        "                log_prob,\n",
        "                done,\n",
        "                next_state\n",
        "            )\n",
        "            state = next_state\n",
        "\n",
        "        stats = storage.get_statistics()\n",
        "        print(f\"Test Episode {episode + 1}, Total Reward: {stats['current_reward']}\")\n",
        "\n",
        "    env.close()\n",
        "    return storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "87ef04a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "87ef04a6",
        "outputId": "d511c6d8-b7e9-494d-a425-2baca8224042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 1000 episodes...\n",
            "Training on device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1681120133.py:46: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
            "  'dones': torch.tensor(self.dones,dtype=torch.bool),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10, AVG Reward: -73.797, Best Reward: 5.042\n",
            "Episode 20, AVG Reward: -19.064, Best Reward: 130.020\n",
            "Episode 30, AVG Reward: 0.134, Best Reward: 144.317\n",
            "Episode 40, AVG Reward: 28.630, Best Reward: 144.317\n",
            "Episode 50, AVG Reward: -57.312, Best Reward: 144.317\n",
            "Episode 60, AVG Reward: 9.908, Best Reward: 207.247\n",
            "Episode 70, AVG Reward: 50.604, Best Reward: 207.247\n",
            "Episode 80, AVG Reward: 38.036, Best Reward: 207.247\n",
            "Episode 90, AVG Reward: -12.603, Best Reward: 245.182\n",
            "Episode 100, AVG Reward: 46.934, Best Reward: 245.182\n",
            "Episode 110, AVG Reward: -10.207, Best Reward: 245.182\n",
            "Episode 120, AVG Reward: 66.138, Best Reward: 245.182\n",
            "Episode 130, AVG Reward: 65.649, Best Reward: 245.182\n",
            "Episode 140, AVG Reward: -47.814, Best Reward: 245.182\n",
            "Episode 150, AVG Reward: -3.030, Best Reward: 245.182\n",
            "Episode 160, AVG Reward: 22.760, Best Reward: 245.182\n",
            "Episode 170, AVG Reward: 3.099, Best Reward: 245.182\n",
            "Episode 180, AVG Reward: -25.166, Best Reward: 245.182\n",
            "Episode 190, AVG Reward: 107.725, Best Reward: 245.182\n",
            "Episode 200, AVG Reward: 48.586, Best Reward: 245.182\n",
            "Episode 210, AVG Reward: 51.037, Best Reward: 245.182\n",
            "Episode 220, AVG Reward: -18.117, Best Reward: 245.182\n",
            "Episode 230, AVG Reward: 74.597, Best Reward: 245.182\n",
            "Episode 240, AVG Reward: 94.873, Best Reward: 245.182\n",
            "Episode 250, AVG Reward: 21.592, Best Reward: 245.182\n",
            "Episode 260, AVG Reward: 17.944, Best Reward: 245.182\n",
            "Episode 270, AVG Reward: 91.481, Best Reward: 245.182\n",
            "Episode 280, AVG Reward: 56.764, Best Reward: 245.182\n",
            "Episode 290, AVG Reward: 48.976, Best Reward: 245.182\n",
            "Episode 300, AVG Reward: -69.961, Best Reward: 262.788\n",
            "Episode 310, AVG Reward: 106.263, Best Reward: 262.788\n",
            "Episode 320, AVG Reward: 62.531, Best Reward: 262.788\n",
            "Episode 330, AVG Reward: 101.758, Best Reward: 262.788\n",
            "Episode 340, AVG Reward: 70.589, Best Reward: 262.788\n",
            "Episode 350, AVG Reward: 98.993, Best Reward: 262.788\n",
            "Episode 360, AVG Reward: -16.108, Best Reward: 262.788\n",
            "Episode 370, AVG Reward: -16.784, Best Reward: 262.788\n",
            "Episode 380, AVG Reward: 22.100, Best Reward: 262.788\n",
            "Episode 390, AVG Reward: -86.730, Best Reward: 262.788\n",
            "Episode 400, AVG Reward: 19.654, Best Reward: 262.788\n",
            "Episode 410, AVG Reward: 90.143, Best Reward: 262.788\n",
            "Episode 420, AVG Reward: 55.361, Best Reward: 262.788\n",
            "Episode 430, AVG Reward: 78.130, Best Reward: 262.788\n",
            "Episode 440, AVG Reward: -54.900, Best Reward: 262.788\n",
            "Episode 450, AVG Reward: 11.466, Best Reward: 262.788\n",
            "Episode 460, AVG Reward: -35.572, Best Reward: 262.788\n",
            "Episode 470, AVG Reward: 52.524, Best Reward: 262.788\n",
            "Episode 480, AVG Reward: -57.014, Best Reward: 262.788\n",
            "Episode 490, AVG Reward: 103.202, Best Reward: 262.788\n",
            "Episode 500, AVG Reward: 42.147, Best Reward: 262.788\n",
            "Episode 510, AVG Reward: 34.680, Best Reward: 262.788\n",
            "Episode 520, AVG Reward: -35.210, Best Reward: 262.788\n",
            "Episode 530, AVG Reward: 38.647, Best Reward: 262.788\n",
            "Episode 540, AVG Reward: -19.708, Best Reward: 262.788\n",
            "Episode 550, AVG Reward: 15.805, Best Reward: 262.788\n",
            "Episode 560, AVG Reward: -1.765, Best Reward: 262.788\n",
            "Episode 570, AVG Reward: -61.106, Best Reward: 262.788\n",
            "Episode 580, AVG Reward: -9.704, Best Reward: 262.788\n",
            "Episode 590, AVG Reward: -8.849, Best Reward: 262.788\n",
            "Episode 600, AVG Reward: 52.096, Best Reward: 262.788\n",
            "Episode 610, AVG Reward: -77.761, Best Reward: 262.788\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-515145541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lunar_lander4.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting training for {num_episodes} episodes...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# print(\"\\nTraining completed! Training statistics:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2277937617.py\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(num_episodes, max_time_steps, num_env, model_path, device)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstorage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstorages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mavg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2277937617.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, buffer, batch_size)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m# Value loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mvalues_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mloss_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3887\u001b[0m             )\n\u001b[1;32m   3888\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m         return torch._C._nn.mse_loss(\n\u001b[0m\u001b[1;32m   3890\u001b[0m             \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "if __name__ == \"__main__\":\n",
        "    # this is for training and testing the model\n",
        "    # since this environment only gives reward at the end of the episode we need to give it more training\n",
        "    num_episodes = 1000  # You can change this number\n",
        "    model_path = \"lunar_lander4.pth\"\n",
        "    print(f\"Starting training for {num_episodes} episodes...\")\n",
        "    agent, train_storage,reward_progress = train_ppo(num_episodes=num_episodes,num_env=5,model_path=model_path)\n",
        "\n",
        "    # print(\"\\nTraining completed! Training statistics:\")\n",
        "    # train_stats = train_storage.get_statistics()\n",
        "    # print(f\"Mean reward: {train_stats['mean_reward']:.2f}\")\n",
        "    # print(f\"Max reward: {train_stats['max_reward']:.2f}\")\n",
        "    # print(f\"Mean episode length: {train_stats['mean_length']:.2f}\")\n",
        "\n",
        "    plt.plot(reward_progress, label='Episode Reward')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Reward Progress Over Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rCKfUKX-Vmz6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCKfUKX-Vmz6",
        "outputId": "a3e9cfa5-9d4c-42d2-919b-588132012f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting model testing...\n",
            "Testing on device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_2828\\1536016902.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(state, dtype=torch.float32).to(self.device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Episode 1, Total Reward: 118.8628495310367\n",
            "Test Episode 2, Total Reward: -23.99068718786036\n",
            "Test Episode 3, Total Reward: -52.94602981307331\n",
            "Test Episode 4, Total Reward: 0.3140470418869654\n",
            "Test Episode 5, Total Reward: 157.2568481800966\n",
            "\n",
            "Testing completed! Test statistics:\n",
            "Mean test reward: 157.26\n",
            "Max test reward: 157.26\n",
            "Min test reward: 157.26\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStarting model testing...\")\n",
        "model_path = \"lunar_lander4.pth\"\n",
        "test_storage = test_model(model_path, num_episodes=5)\n",
        "\n",
        "print(\"\\nTesting completed! Test statistics:\")\n",
        "test_stats = test_storage.get_statistics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JeOWfPSmghRz",
      "metadata": {
        "id": "JeOWfPSmghRz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
