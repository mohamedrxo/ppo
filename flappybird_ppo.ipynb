{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e577d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test test\n"
     ]
    }
   ],
   "source": [
    "print(\"test test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44162f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs [ 0.98611111  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.45898438 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.97222222  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.44335938 -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.95833333  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.4296875  -0.7         0.43333333] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.94444444  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.41210938 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.93055556  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.39648438 -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.91666667  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.3828125  -0.7         0.43333333] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.90277778  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.37109375 -0.6         0.4       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.88888889  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.35351562 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.875      0.2734375  0.46875    1.         0.         1.\n",
      "  1.         0.         1.         0.3359375 -0.9        0.5      ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.86111111  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.3203125  -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.84722222  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.30664062 -0.7         0.43333333] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.83333333  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.29492188 -0.6         0.4       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.81944444  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.27734375 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.80555556  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.26171875 -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.79166667  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.24414062 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.77777778  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.22851562 -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.76388889  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.2109375  -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.75        0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.1953125  -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.73611111  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.18164062 -0.7         0.43333333] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.72222222  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.1640625  -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.70833333  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.14648438 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.69444444  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.12890625 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.68055556  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.11132812 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.66666667  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.09570312 -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.65277778  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.08203125 -0.7         0.43333333] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.63888889  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.06445312 -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.625       0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.04882812 -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.61111111  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.03125    -0.9         0.5       ] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.59722222  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.015625   -0.8         0.46666667] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.58333333  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.          0.00195312 -0.7         0.43333333] reward 0.1 terminated False  trancated False\n",
      "obs [ 0.56944444  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.         -0.00976562 -0.6         0.4       ] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.55555556  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.         -0.01953125 -0.5         0.36666667] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.54166667  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.         -0.02734375 -0.4         0.33333333] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.52777778  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.         -0.04492188 -0.9         0.5       ] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.51388889  0.2734375   0.46875     1.          0.          1.\n",
      "  1.          0.          1.         -0.06054688 -0.8         0.46666667] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.5        0.2734375  0.46875    1.         0.3125     0.5078125\n",
      "  1.         0.         1.        -0.078125  -0.9        0.5      ] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.48611111  0.2734375   0.46875     0.98611111  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.09375    -0.8         0.46666667] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.47222222  0.2734375   0.46875     0.97222222  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.10742188 -0.7         0.43333333] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.45833333  0.2734375   0.46875     0.95833333  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.11914062 -0.6         0.4       ] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.44444444  0.2734375   0.46875     0.94444444  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.12890625 -0.5         0.36666667] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.43055556  0.2734375   0.46875     0.93055556  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.13671875 -0.4         0.33333333] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.41666667  0.2734375   0.46875     0.91666667  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.14257812 -0.3         0.3       ] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.40277778  0.2734375   0.46875     0.90277778  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.14648438 -0.2         0.26666667] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.38888889  0.2734375   0.46875     0.88888889  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.1484375  -0.1         0.23333333] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.375      0.2734375  0.46875    0.875      0.3125     0.5078125\n",
      "  1.         0.         1.        -0.1484375  0.         0.2      ] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.36111111  0.2734375   0.46875     0.86111111  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.14648438  0.1         0.16666667] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.34722222  0.2734375   0.46875     0.84722222  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.14257812  0.2         0.13333333] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.33333333  0.2734375   0.46875     0.83333333  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.13671875  0.3         0.1       ] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.31944444  0.2734375   0.46875     0.81944444  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.12890625  0.4         0.06666667] reward -0.5 terminated False  trancated False\n",
      "obs [ 0.30555556  0.2734375   0.46875     0.80555556  0.3125      0.5078125\n",
      "  1.          0.          1.         -0.11914062  0.5         0.03333333] reward -1 terminated True  trancated False\n"
     ]
    }
   ],
   "source": [
    "import flappy_bird_gymnasium\n",
    "import gymnasium\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "while True:\n",
    "    # Next action:\n",
    "    # (feed the observation to your agent here)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Processing:\n",
    "    obs, reward, terminated, trancated, info = env.step(action)\n",
    "    \n",
    "    print(\"obs\",obs,\"reward\", reward, \"terminated\",terminated,\" trancated\", trancated)\n",
    "    # Checking if the player is still alive\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88723e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device  =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea39c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# this class define the storage buffer of the environment\n",
    "\n",
    "class StorageBuffer:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.episode_rewards = []  # Store rewards for each episode\n",
    "        self.episode_lengths = []  # Store lengths for each episode\n",
    "\n",
    "    def reset(self):\n",
    "        # Current episode storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.next_state = None\n",
    "        self.current_reward = 0\n",
    "\n",
    "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "        self.current_reward += reward\n",
    "\n",
    "        if done:\n",
    "            self.next_state = next_state\n",
    "            self.episode_rewards.append(self.current_reward)\n",
    "            self.episode_lengths.append(len(self.rewards))\n",
    "\n",
    "    def get_episode_data(self):\n",
    "        return {\n",
    "            'states': torch.FloatTensor(np.array(self.states)),\n",
    "            'actions': torch.tensor(self.actions),\n",
    "            'rewards': torch.tensor(self.rewards),\n",
    "            'log_probs': torch.tensor(self.log_probs),\n",
    "            'dones': torch.tensor(self.dones),\n",
    "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
    "        }\n",
    "\n",
    "    def get_statistics(self):\n",
    "\n",
    "        if not self.episode_rewards:\n",
    "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
    "\n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.episode_rewards),\n",
    "            \"max_reward\": np.max(self.episode_rewards),\n",
    "            \"min_reward\": np.min(self.episode_rewards),\n",
    "            \"mean_length\": np.mean(self.episode_lengths),\n",
    "            \"current_reward\": self.current_reward,\n",
    "            \"current_length\": len(self.rewards)\n",
    "        }\n",
    "\n",
    "#the policy and value networks\n",
    "# i used small model since it simple game but you can make it bigger if you want\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6296e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gymnasium.vector import SyncVectorEnv\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim,device=\"cpu\", lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=10, entropy_coef=0.01):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.old_policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        self.value_function = ValueNetwork(state_dim).to(device)\n",
    "\n",
    "        self.policy_optimizer = optim.AdamW(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.AdamW(self.value_function.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_prob = self.old_policy(state)\n",
    "        dist = Categorical(action_prob)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), dist.entropy()\n",
    "\n",
    "    def compute_advantages(self, rewards, values, next_value, dones, lambda_gae=0.95):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
    "            gae = delta + self.gamma * lambda_gae * (1 - float(dones[t])) * gae\n",
    "            advantages.insert(0, gae)\n",
    "            next_value = values[t]\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    def update(self, buffer, batch_size=120):\n",
    "    # Collect episode data\n",
    "        data = buffer.get_episode_data()\n",
    "        # states = torch.tensor(data['states'], dtype=torch.float32).to(device)\n",
    "        # actions = torch.tensor(data['actions'], dtype=torch.int64).to(device)\n",
    "        # log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32).to(device)\n",
    "        states = data['states'].clone().detach().float().to(device)\n",
    "        actions = data['actions'].clone().detach().long().to(device)\n",
    "        log_probs_old = data['log_probs'].clone().detach().float().to(device)\n",
    "        rewards = data['rewards']\n",
    "        dones = data['dones']\n",
    "\n",
    "        values = self.value_function(states).squeeze().detach()\n",
    "        next_value = self.value_function(torch.tensor(data['next_state'], dtype=torch.float32).to(device)).item() if data['next_state'] else 0.0\n",
    "        advantages = self.compute_advantages(rewards, values, next_value, dones)\n",
    "        targets = advantages + values\n",
    "\n",
    "\n",
    "        dataset = TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        total_loss_actor = 0\n",
    "        total_loss_critic = 0\n",
    "        total_entropy = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            for batch_states, batch_actions, batch_log_probs_old, batch_advantages, batch_targets in dataloader:\n",
    "                action_probs = self.policy(batch_states)\n",
    "                dist = Categorical(action_probs)\n",
    "                log_probs = dist.log_prob(batch_actions)\n",
    "\n",
    "                # we add this entropy to encourage exploration\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs - batch_log_probs_old)\n",
    "                surr1 = ratios * batch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                loss_actor = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy\n",
    "\n",
    "                values_pred = self.value_function(batch_states).squeeze()\n",
    "                loss_critic = self.mse_loss(values_pred, batch_targets)\n",
    "\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                total_loss_actor += loss_actor.item()\n",
    "                total_loss_critic += loss_critic.item()\n",
    "                total_entropy += entropy.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        return total_loss_actor / num_batches, total_loss_critic / num_batches, total_entropy / num_batches\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'value_state_dict': self.value_function.state_dict(),\n",
    "            'policy_optimizer': self.policy_optimizer.state_dict(),\n",
    "            'value_optimizer': self.value_optimizer.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "            \n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.old_policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.value_function.load_state_dict(checkpoint['value_state_dict'])\n",
    "        \n",
    "        if 'policy_optimizer' in checkpoint:\n",
    "            self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "        if 'value_optimizer' in checkpoint:\n",
    "            self.value_optimizer.load_state_dict(checkpoint['value_optimizer'])\n",
    "\n",
    "\n",
    "def train_ppo(num_episodes=5000, max_time_steps=200,model_name = None):\n",
    "\n",
    "    # env = gymnasium.make(\"FlappyBird-v0\",  use_lidar=False)\n",
    "    # state_dim = env.observation_space.shape[0]\n",
    "    # action_dim = env.action_space.n\n",
    "\n",
    "    num_envs = 5\n",
    "    \n",
    "    env = SyncVectorEnv([lambda: gymnasium.make(\"FlappyBird-v0\",  use_lidar=False) for _ in range(num_envs)]) \n",
    "    state_dim = env.single_observation_space.shape[0]\n",
    "    action_dim = env.single_action_space.n\n",
    "\n",
    "    agent = PPOAgent(state_dim, action_dim,device)\n",
    "    if model_name is not None:\n",
    "        agent.load_model(model_name)\n",
    "\n",
    "    buffer = StorageBuffer()\n",
    "    best_reward = -float('inf')\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        states, _ = env.reset()\n",
    "        buffer.reset()\n",
    "        episode_rewards = np.zeros(num_envs)\n",
    "\n",
    "        for t in range(max_time_steps):\n",
    "            actions, log_probs, entropys = agent.select_action(states)\n",
    "            next_states, rewards, terminateds, truncateds, _ = env.step(actions)\n",
    "            dones = terminateds | truncateds\n",
    "            for i in range(num_envs):\n",
    "                episode_rewards[i] += rewards[i]\n",
    "                buffer.add_step(states[i], actions[i], rewards[i], log_probs[i], dones[i], next_states[i] if not dones[i] else None)\n",
    "            states = next_states\n",
    "            if all(dones):\n",
    "                break\n",
    "\n",
    "        loss_actor, loss_critic, entropy_val = agent.update(buffer,250)\n",
    "\n",
    "        if episode_rewards.mean() > best_reward:\n",
    "            best_reward = episode_rewards.mean()\n",
    "            agent.save_model(\"ppo_flappy_bird.pth\")\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}, Reward: {float(episode_rewards.mean()):.2f}, Best: {float(best_reward):.2f}, Loss Actor: {float(loss_actor):.4f}, Loss Critic: {float(loss_critic):.4f}, Entropy: {float(entropy_val):.4f}\")\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    # wandb.finish()\n",
    "    return agent, buffer\n",
    "\n",
    "\n",
    "def test_model(model_path, num_episodes=10):\n",
    "    env = gymnasium.make(\"FlappyBird-v0\" ,render_mode=\"human\",use_lidar=False)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = PPOAgent(state_dim, action_dim,device)\n",
    "    agent.load_model(model_path)\n",
    "    buffer = StorageBuffer()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        buffer.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _, _ = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"Test Episode {episode + 1}, Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8d6b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 500 episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_3552\\3923554124.py:46: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  'dones': torch.tensor(self.dones),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Reward: 68.20, Best: 68.20, Loss Actor: -0.0067, Loss Critic: 1.0076, Entropy: 0.4243\n",
      "Episode 20, Reward: 15.30, Best: 68.20, Loss Actor: -0.0045, Loss Critic: 0.8866, Entropy: 0.3673\n",
      "Episode 30, Reward: 82.00, Best: 82.90, Loss Actor: -0.0030, Loss Critic: 0.9992, Entropy: 0.2664\n",
      "Episode 40, Reward: 84.70, Best: 85.90, Loss Actor: -0.0044, Loss Critic: 0.9605, Entropy: 0.1922\n",
      "Episode 50, Reward: 87.70, Best: 88.60, Loss Actor: -0.0038, Loss Critic: 0.9040, Entropy: 0.1680\n",
      "Episode 60, Reward: 19.50, Best: 88.60, Loss Actor: -0.0042, Loss Critic: 0.9837, Entropy: 0.2290\n",
      "Episode 70, Reward: 18.90, Best: 88.60, Loss Actor: -0.0028, Loss Critic: 0.9894, Entropy: 0.2131\n",
      "Episode 80, Reward: 19.50, Best: 88.60, Loss Actor: -0.0054, Loss Critic: 0.7441, Entropy: 0.2101\n",
      "Episode 90, Reward: 18.90, Best: 88.60, Loss Actor: -0.0053, Loss Critic: 0.7042, Entropy: 0.2678\n",
      "Episode 100, Reward: 11.70, Best: 88.60, Loss Actor: -0.0087, Loss Critic: 1.0033, Entropy: 0.2888\n",
      "Episode 110, Reward: -3.30, Best: 88.60, Loss Actor: -0.0035, Loss Critic: 0.8965, Entropy: 0.2492\n",
      "Episode 120, Reward: 4.50, Best: 88.60, Loss Actor: -0.0032, Loss Critic: 0.9828, Entropy: 0.2675\n",
      "Episode 130, Reward: 8.70, Best: 88.60, Loss Actor: -0.0046, Loss Critic: 1.0648, Entropy: 0.2322\n",
      "Episode 140, Reward: 19.50, Best: 88.60, Loss Actor: -0.0048, Loss Critic: 0.9696, Entropy: 0.1495\n",
      "Episode 150, Reward: 83.80, Best: 88.60, Loss Actor: -0.0023, Loss Critic: 0.9806, Entropy: 0.2070\n",
      "Episode 160, Reward: 89.80, Best: 89.80, Loss Actor: -0.0049, Loss Critic: 0.9523, Entropy: 0.2184\n",
      "Episode 170, Reward: 82.90, Best: 89.80, Loss Actor: -0.0041, Loss Critic: 0.9515, Entropy: 0.2234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m model_name=\u001b[33m\"\u001b[39m\u001b[33mppo_flappy_bird.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m model_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m agent, train_storage = \u001b[43mtrain_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed! Training statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m train_stats = train_storage.get_statistics()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mtrain_ppo\u001b[39m\u001b[34m(num_episodes, max_time_steps, model_name)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(dones):\n\u001b[32m    160\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m loss_actor, loss_critic, entropy_val = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m250\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode_rewards.mean() > best_reward:\n\u001b[32m    165\u001b[39m     best_reward = episode_rewards.mean()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mPPOAgent.update\u001b[39m\u001b[34m(self, buffer, batch_size)\u001b[39m\n\u001b[32m     69\u001b[39m num_batches = \u001b[32m0\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.k_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_log_probs_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_advantages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdist\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:207\u001b[39m, in \u001b[36mTensorDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:207\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tensors)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # this is for training and testing the model\n",
    "    # note that this environment is bit complex for that it will take a lot more training if you have gpu access that can speed up the training\n",
    "    num_episodes = 500  # You can change this number\n",
    "    print(f\"Starting training for {num_episodes} episodes...\")\n",
    "\n",
    "    model_name=\"ppo_flappy_bird.pth\"\n",
    "    # model_name = None\n",
    "\n",
    "    agent, train_storage = train_ppo(num_episodes=num_episodes,model_name=model_name)\n",
    "\n",
    "    print(\"\\nTraining completed! Training statistics:\")\n",
    "\n",
    "    train_stats = train_storage.get_statistics()\n",
    "    print(f\"Mean reward: {train_stats['mean_reward']:.2f}\")\n",
    "    print(f\"Max reward: {train_stats['max_reward']:.2f}\")\n",
    "    print(f\"Mean episode length: {train_stats['mean_length']:.2f}\")\n",
    "\n",
    "    model_path = \"ppo_flappy_bird.pth\"\n",
    "\n",
    "    print(\"\\nStarting model testing...\")\n",
    "    test_storage = test_model(model_path, num_episodes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01e43b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model testing...\n",
      "Test Episode 1, Total Reward: 12.90\n",
      "Test Episode 2, Total Reward: 8.40\n",
      "Test Episode 3, Total Reward: 50.40\n",
      "Test Episode 4, Total Reward: 12.90\n",
      "Test Episode 5, Total Reward: 12.90\n"
     ]
    }
   ],
   "source": [
    "# testing the model with the saved params\n",
    "\n",
    "model_path = \"ppo_flappy_bird.pth\"\n",
    "\n",
    "print(\"\\nStarting model testing...\")\n",
    "test_storage = test_model(model_path, num_episodes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7323c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
