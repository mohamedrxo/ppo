{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILJr8jkqslKb",
        "outputId": "f3bc79e7-02e8-4580-8a78-d150b62f87bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": [
        "print('hello world')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS8w194ltZ7e",
        "outputId": "d7b969ea-b81f-4656-c1af-78c3dd821537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,552 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2351231 sha256=9b9e37d2c3e28f7557e8ab805f6075c830244e49e0b90ea1ad382aeff8654673\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-py-2.3.8\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d box2d-py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIih9DD-uV1e",
        "outputId": "423710ea-7623-4766-8ee4-c7e155becf18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3_X1pPlslKf",
        "outputId": "cd667666-cb41-4eab-b247-8c61c23ffc9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "observation space observation_space (8,)\n",
            "action space 4\n"
          ]
        }
      ],
      "source": [
        "# this is our environment that will train our ppo agent on\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\",render_mode=\"human\")\n",
        "observation, info = env.reset()\n",
        "\n",
        "episode_over = False\n",
        "while not episode_over:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "\n",
        "    episode_over = terminated or truncated\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print('observation space observation_space',env.observation_space.shape)\n",
        "print('action space',env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ir0tpPLjslKg"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# this class define the storage buffer of the environment\n",
        "\n",
        "class StorageBuffer:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.episode_rewards = []  # Store rewards for each episode\n",
        "        self.episode_lengths = []  # Store lengths for each episode\n",
        "\n",
        "    def reset(self):\n",
        "        # Current episode storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.next_state = None\n",
        "        self.current_reward = 0\n",
        "\n",
        "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "        self.current_reward += reward\n",
        "\n",
        "        if done:\n",
        "            self.next_state = next_state\n",
        "            self.episode_rewards.append(self.current_reward)\n",
        "            self.episode_lengths.append(len(self.rewards))\n",
        "\n",
        "    def get_episode_data(self):\n",
        "        return {\n",
        "            'states': torch.FloatTensor(np.array(self.states)),\n",
        "            'actions': torch.tensor(self.actions),\n",
        "            'rewards': torch.tensor(self.rewards),\n",
        "            'log_probs': torch.tensor(self.log_probs),\n",
        "            'dones': torch.tensor(self.dones),\n",
        "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
        "        }\n",
        "\n",
        "    def get_statistics(self):\n",
        "\n",
        "        if not self.episode_rewards:\n",
        "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
        "\n",
        "        return {\n",
        "            \"mean_reward\": np.mean(self.episode_rewards),\n",
        "            \"max_reward\": np.max(self.episode_rewards),\n",
        "            \"min_reward\": np.min(self.episode_rewards),\n",
        "            \"mean_length\": np.mean(self.episode_lengths),\n",
        "            \"current_reward\": self.current_reward,\n",
        "            \"current_length\": len(self.rewards)\n",
        "        }\n",
        "\n",
        "#the policy and value networks\n",
        "# i used small model since it simple game but you can make it bigger if you want\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "i used  weight and biases for visualizing the reward and losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jZ6DCRGEn6M",
        "outputId": "cd73ab76-0def-4223-ced6-ef73ab378269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fcb54iZExhk"
      },
      "outputs": [],
      "source": [
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QPpaO32mA2yC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=10, entropy_coef=0.01):\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.old_policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "        self.value_function = ValueNetwork(state_dim)\n",
        "\n",
        "        self.policy_optimizer = optim.AdamW(self.policy.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.AdamW(self.value_function.parameters(), lr=lr)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.k_epochs = k_epochs\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            action_prob = self.old_policy(state)\n",
        "        dist = Categorical(action_prob)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action), dist.entropy()\n",
        "\n",
        "    def compute_advantages(self, rewards, values, next_value, dones, lambda_gae=0.95):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
        "            gae = delta + self.gamma * lambda_gae * (1 - float(dones[t])) * gae\n",
        "            advantages.insert(0, gae)\n",
        "            next_value = values[t]\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float32)\n",
        "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    def update(self, buffer, batch_size=64):\n",
        "    # Collect episode data\n",
        "        data = buffer.get_episode_data()\n",
        "        states = torch.tensor(data['states'], dtype=torch.float32)\n",
        "        actions = torch.tensor(data['actions'], dtype=torch.int64)\n",
        "        log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n",
        "        rewards = data['rewards']\n",
        "        dones = data['dones']\n",
        "\n",
        "        values = self.value_function(states).squeeze().detach()\n",
        "        next_value = self.value_function(torch.tensor(data['next_state'], dtype=torch.float32)).item() if data['next_state'] else 0.0\n",
        "        advantages = self.compute_advantages(rewards, values, next_value, dones)\n",
        "        targets = advantages + values\n",
        "\n",
        "        dataset = TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        total_loss_actor = 0\n",
        "        total_loss_critic = 0\n",
        "        total_entropy = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for _ in range(self.k_epochs):\n",
        "            for batch_states, batch_actions, batch_log_probs_old, batch_advantages, batch_targets in dataloader:\n",
        "                action_probs = self.policy(batch_states)\n",
        "                dist = Categorical(action_probs)\n",
        "                log_probs = dist.log_prob(batch_actions)\n",
        "                \n",
        "                # we add this entropy to encourage exploration\n",
        "                entropy = dist.entropy().mean()\n",
        "\n",
        "                ratios = torch.exp(log_probs - batch_log_probs_old)\n",
        "                surr1 = ratios * batch_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
        "                loss_actor = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy\n",
        "\n",
        "                values_pred = self.value_function(batch_states).squeeze()\n",
        "                loss_critic = self.mse_loss(values_pred, batch_targets)\n",
        "\n",
        "                self.policy_optimizer.zero_grad()\n",
        "                loss_actor.backward()\n",
        "                self.policy_optimizer.step()\n",
        "\n",
        "                self.value_optimizer.zero_grad()\n",
        "                loss_critic.backward()\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "                total_loss_actor += loss_actor.item()\n",
        "                total_loss_critic += loss_critic.item()\n",
        "                total_entropy += entropy.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        return total_loss_actor / num_batches, total_loss_critic / num_batches, total_entropy / num_batches\n",
        "\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'value_state_dict': self.value_function.state_dict(),\n",
        "        }, path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        checkpoint = torch.load(path)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.old_policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.value_function.load_state_dict(checkpoint['value_state_dict'])\n",
        "\n",
        "\n",
        "def train_ppo(num_episodes=5000, max_time_steps=200):\n",
        "    # init a new wandb parameters\n",
        "    wandb.init(project=\"PPO-LunarLander\", config={\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"gamma\": 0.99,\n",
        "        \"eps_clip\": 0.2,\n",
        "        \"entropy_coef\": 0.01,\n",
        "        \"batch_size\": 64,\n",
        "        \"k_epochs\": 10\n",
        "    })\n",
        "\n",
        "    env = gym.make(\"LunarLander-v2\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    buffer = StorageBuffer()\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        buffer.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(max_time_steps):\n",
        "            action, log_prob, entropy = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            buffer.add_step(state, action, reward, log_prob, done, next_state if not done else None)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        loss_actor, loss_critic, entropy_val = agent.update(buffer)\n",
        "        #sending data and visualizing it on  weight and biases official website\n",
        "        wandb.log({\n",
        "            \"episode\": episode,\n",
        "            \"reward\": episode_reward,\n",
        "            \"actor_loss\": loss_actor,\n",
        "            \"critic_loss\": loss_critic,\n",
        "            \"entropy\": entropy_val\n",
        "        })\n",
        "\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            agent.save_model(\"ppo_lunar_lander.pth\")\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}, Reward: {episode_reward:.2f}, Best: {best_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    wandb.finish()\n",
        "    return agent, buffer\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model_path, num_episodes=10):\n",
        "    env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    agent.load_model(model_path)\n",
        "    buffer = StorageBuffer()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        buffer.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _, _ = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Test Episode {episode + 1}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9gQpbYCFslKm",
        "outputId": "3257c63a-c8ff-4c50-df09-2224f0c35bab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 10000 episodes...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250216_111230-yqycu6qy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander/runs/yqycu6qy' target=\"_blank\">zany-water-3</a></strong> to <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander/runs/yqycu6qy' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander/runs/yqycu6qy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-40e75fc282cc>:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(data['states'], dtype=torch.float32)\n",
            "<ipython-input-11-40e75fc282cc>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actions = torch.tensor(data['actions'], dtype=torch.int64)\n",
            "<ipython-input-11-40e75fc282cc>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10, Reward: -260.68, Best: -28.68\n",
            "Episode 20, Reward: -165.22, Best: -28.68\n",
            "Episode 30, Reward: -124.42, Best: -28.68\n",
            "Episode 40, Reward: -149.49, Best: -28.68\n",
            "Episode 50, Reward: -158.68, Best: -28.68\n",
            "Episode 60, Reward: -216.27, Best: -28.68\n",
            "Episode 70, Reward: -128.80, Best: -26.71\n",
            "Episode 80, Reward: 102.56, Best: 102.56\n",
            "Episode 90, Reward: -276.89, Best: 102.56\n",
            "Episode 100, Reward: -60.11, Best: 107.19\n",
            "Episode 110, Reward: -8.88, Best: 107.19\n",
            "Episode 120, Reward: -49.17, Best: 107.19\n",
            "Episode 130, Reward: 37.54, Best: 107.19\n",
            "Episode 140, Reward: -37.25, Best: 107.19\n",
            "Episode 150, Reward: 27.99, Best: 107.19\n",
            "Episode 160, Reward: 26.72, Best: 107.19\n",
            "Episode 170, Reward: 34.51, Best: 107.19\n",
            "Episode 180, Reward: 46.21, Best: 107.19\n",
            "Episode 190, Reward: 29.58, Best: 107.19\n",
            "Episode 200, Reward: 53.18, Best: 107.19\n",
            "Episode 210, Reward: 21.02, Best: 107.19\n",
            "Episode 220, Reward: 69.26, Best: 107.19\n",
            "Episode 230, Reward: 20.28, Best: 107.19\n",
            "Episode 240, Reward: 54.54, Best: 107.19\n",
            "Episode 250, Reward: 48.78, Best: 107.19\n",
            "Episode 260, Reward: 52.07, Best: 107.19\n",
            "Episode 270, Reward: 25.72, Best: 107.19\n",
            "Episode 280, Reward: 15.30, Best: 107.19\n",
            "Episode 290, Reward: 18.17, Best: 107.19\n",
            "Episode 300, Reward: 18.43, Best: 107.19\n",
            "Episode 310, Reward: 3.83, Best: 107.19\n",
            "Episode 320, Reward: 6.90, Best: 107.19\n",
            "Episode 330, Reward: 6.12, Best: 107.19\n",
            "Episode 340, Reward: 42.44, Best: 107.19\n",
            "Episode 350, Reward: 5.41, Best: 107.19\n",
            "Episode 360, Reward: -7.10, Best: 107.19\n",
            "Episode 370, Reward: 33.20, Best: 107.19\n",
            "Episode 380, Reward: -18.59, Best: 107.19\n",
            "Episode 390, Reward: 10.35, Best: 107.19\n",
            "Episode 400, Reward: 26.34, Best: 107.19\n",
            "Episode 410, Reward: -13.20, Best: 107.19\n",
            "Episode 420, Reward: -14.93, Best: 107.19\n",
            "Episode 430, Reward: -0.07, Best: 107.19\n",
            "Episode 440, Reward: 20.66, Best: 107.19\n",
            "Episode 450, Reward: -15.24, Best: 107.19\n",
            "Episode 460, Reward: 20.61, Best: 107.19\n",
            "Episode 470, Reward: 40.70, Best: 107.19\n",
            "Episode 480, Reward: -21.47, Best: 107.19\n",
            "Episode 490, Reward: -6.65, Best: 107.19\n",
            "Episode 500, Reward: 10.06, Best: 107.19\n",
            "Episode 510, Reward: 12.38, Best: 107.19\n",
            "Episode 520, Reward: -4.74, Best: 107.19\n",
            "Episode 530, Reward: -4.44, Best: 107.19\n",
            "Episode 540, Reward: 13.16, Best: 107.19\n",
            "Episode 550, Reward: 9.40, Best: 107.19\n",
            "Episode 560, Reward: 17.57, Best: 107.19\n",
            "Episode 570, Reward: 44.32, Best: 107.19\n",
            "Episode 580, Reward: 14.67, Best: 107.19\n",
            "Episode 590, Reward: 20.82, Best: 107.19\n",
            "Episode 600, Reward: 47.27, Best: 107.19\n",
            "Episode 610, Reward: 8.19, Best: 107.19\n",
            "Episode 620, Reward: 7.95, Best: 107.19\n",
            "Episode 630, Reward: 19.77, Best: 107.19\n",
            "Episode 640, Reward: 18.50, Best: 107.19\n",
            "Episode 650, Reward: 25.51, Best: 107.19\n",
            "Episode 660, Reward: 35.89, Best: 107.19\n",
            "Episode 670, Reward: 21.91, Best: 107.19\n",
            "Episode 680, Reward: 62.75, Best: 107.19\n",
            "Episode 690, Reward: 36.29, Best: 107.19\n",
            "Episode 700, Reward: 33.97, Best: 107.19\n",
            "Episode 710, Reward: 26.51, Best: 107.19\n",
            "Episode 720, Reward: 16.07, Best: 107.19\n",
            "Episode 730, Reward: 24.70, Best: 107.19\n",
            "Episode 740, Reward: 24.67, Best: 107.19\n",
            "Episode 750, Reward: 2.95, Best: 107.19\n",
            "Episode 760, Reward: 61.47, Best: 107.19\n",
            "Episode 770, Reward: 15.65, Best: 107.19\n",
            "Episode 780, Reward: 53.63, Best: 107.19\n",
            "Episode 790, Reward: 61.86, Best: 107.19\n",
            "Episode 800, Reward: 16.07, Best: 107.19\n",
            "Episode 810, Reward: -16.07, Best: 107.19\n",
            "Episode 820, Reward: -14.95, Best: 107.19\n",
            "Episode 830, Reward: 8.67, Best: 107.19\n",
            "Episode 840, Reward: 10.32, Best: 107.19\n",
            "Episode 850, Reward: 17.82, Best: 107.19\n",
            "Episode 860, Reward: 20.37, Best: 107.19\n",
            "Episode 870, Reward: -19.91, Best: 107.19\n",
            "Episode 880, Reward: 27.99, Best: 107.19\n",
            "Episode 890, Reward: 35.34, Best: 107.19\n",
            "Episode 900, Reward: 46.26, Best: 107.19\n",
            "Episode 910, Reward: -6.52, Best: 107.19\n",
            "Episode 920, Reward: 23.44, Best: 107.19\n",
            "Episode 930, Reward: -12.91, Best: 107.19\n",
            "Episode 940, Reward: -1.28, Best: 107.19\n",
            "Episode 950, Reward: -11.13, Best: 107.19\n",
            "Episode 960, Reward: -1.45, Best: 107.19\n",
            "Episode 970, Reward: 4.79, Best: 107.19\n",
            "Episode 980, Reward: 57.89, Best: 107.19\n",
            "Episode 990, Reward: -0.32, Best: 107.19\n",
            "Episode 1000, Reward: -4.79, Best: 107.19\n",
            "Episode 1010, Reward: 20.84, Best: 107.19\n",
            "Episode 1020, Reward: 66.98, Best: 107.19\n",
            "Episode 1030, Reward: 43.83, Best: 107.19\n",
            "Episode 1040, Reward: 35.80, Best: 107.19\n",
            "Episode 1050, Reward: 23.73, Best: 107.19\n",
            "Episode 1060, Reward: 46.14, Best: 107.19\n",
            "Episode 1070, Reward: 17.08, Best: 107.19\n",
            "Episode 1080, Reward: -1.70, Best: 107.19\n",
            "Episode 1090, Reward: 31.59, Best: 107.19\n",
            "Episode 1100, Reward: 45.07, Best: 107.19\n",
            "Episode 1110, Reward: 53.41, Best: 107.19\n",
            "Episode 1120, Reward: 21.67, Best: 107.19\n",
            "Episode 1130, Reward: -24.60, Best: 107.19\n",
            "Episode 1140, Reward: 24.42, Best: 107.19\n",
            "Episode 1150, Reward: 6.41, Best: 107.19\n",
            "Episode 1160, Reward: 30.09, Best: 107.19\n",
            "Episode 1170, Reward: 17.01, Best: 107.19\n",
            "Episode 1180, Reward: -15.34, Best: 107.19\n",
            "Episode 1190, Reward: 43.69, Best: 107.19\n",
            "Episode 1200, Reward: 39.39, Best: 107.19\n",
            "Episode 1210, Reward: 13.04, Best: 107.19\n",
            "Episode 1220, Reward: 2.19, Best: 107.19\n",
            "Episode 1230, Reward: 2.73, Best: 107.19\n",
            "Episode 1240, Reward: 30.17, Best: 107.19\n",
            "Episode 1250, Reward: 12.59, Best: 107.19\n",
            "Episode 1260, Reward: 25.77, Best: 107.19\n",
            "Episode 1270, Reward: 44.46, Best: 107.19\n",
            "Episode 1280, Reward: 23.28, Best: 107.19\n",
            "Episode 1290, Reward: 17.81, Best: 107.19\n",
            "Episode 1300, Reward: 26.88, Best: 107.19\n",
            "Episode 1310, Reward: 7.39, Best: 107.19\n",
            "Episode 1320, Reward: 31.43, Best: 107.19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1330, Reward: -50.58, Best: 107.19\n",
            "Episode 1340, Reward: 22.77, Best: 107.19\n",
            "Episode 1350, Reward: -17.77, Best: 107.19\n",
            "Episode 1360, Reward: -0.55, Best: 107.19\n",
            "Episode 1370, Reward: 7.50, Best: 107.19\n",
            "Episode 1380, Reward: 18.84, Best: 107.19\n",
            "Episode 1390, Reward: 8.82, Best: 107.19\n",
            "Episode 1400, Reward: -5.44, Best: 107.19\n",
            "Episode 1410, Reward: 16.71, Best: 107.19\n",
            "Episode 1420, Reward: 23.72, Best: 107.19\n",
            "Episode 1430, Reward: 33.02, Best: 107.19\n",
            "Episode 1440, Reward: 58.27, Best: 107.19\n",
            "Episode 1450, Reward: 57.60, Best: 107.19\n",
            "Episode 1460, Reward: 15.33, Best: 107.19\n",
            "Episode 1470, Reward: 50.69, Best: 107.19\n",
            "Episode 1480, Reward: 56.11, Best: 107.19\n",
            "Episode 1490, Reward: 31.17, Best: 107.19\n",
            "Episode 1500, Reward: 6.16, Best: 107.19\n",
            "Episode 1510, Reward: 19.87, Best: 107.19\n",
            "Episode 1520, Reward: -11.48, Best: 107.19\n",
            "Episode 1530, Reward: 55.14, Best: 107.19\n",
            "Episode 1540, Reward: 5.86, Best: 107.19\n",
            "Episode 1550, Reward: 21.33, Best: 107.19\n",
            "Episode 1560, Reward: -5.49, Best: 107.19\n",
            "Episode 1570, Reward: -104.80, Best: 107.19\n",
            "Episode 1580, Reward: -2.45, Best: 107.19\n",
            "Episode 1590, Reward: 3.08, Best: 107.19\n",
            "Episode 1600, Reward: 57.49, Best: 107.19\n",
            "Episode 1610, Reward: -119.26, Best: 107.19\n",
            "Episode 1620, Reward: 58.29, Best: 107.19\n",
            "Episode 1630, Reward: 47.64, Best: 107.19\n",
            "Episode 1640, Reward: -144.01, Best: 107.19\n",
            "Episode 1650, Reward: 30.92, Best: 107.19\n",
            "Episode 1660, Reward: -14.84, Best: 107.19\n",
            "Episode 1670, Reward: 37.51, Best: 107.19\n",
            "Episode 1680, Reward: 48.52, Best: 107.19\n",
            "Episode 1690, Reward: 58.54, Best: 107.19\n",
            "Episode 1700, Reward: 62.36, Best: 107.19\n",
            "Episode 1710, Reward: -6.24, Best: 107.19\n",
            "Episode 1720, Reward: 27.00, Best: 107.19\n",
            "Episode 1730, Reward: 23.65, Best: 107.19\n",
            "Episode 1740, Reward: 95.64, Best: 107.19\n",
            "Episode 1750, Reward: 22.89, Best: 107.19\n",
            "Episode 1760, Reward: 81.94, Best: 107.19\n",
            "Episode 1770, Reward: 91.75, Best: 107.19\n",
            "Episode 1780, Reward: 17.59, Best: 107.19\n",
            "Episode 1790, Reward: 26.43, Best: 107.19\n",
            "Episode 1800, Reward: 87.24, Best: 107.19\n",
            "Episode 1810, Reward: 79.93, Best: 107.19\n",
            "Episode 1820, Reward: 30.51, Best: 107.19\n",
            "Episode 1830, Reward: 95.69, Best: 107.19\n",
            "Episode 1840, Reward: 84.34, Best: 108.31\n",
            "Episode 1850, Reward: 102.73, Best: 113.21\n",
            "Episode 1860, Reward: 32.49, Best: 113.21\n",
            "Episode 1870, Reward: 80.46, Best: 113.21\n",
            "Episode 1880, Reward: 41.26, Best: 113.21\n",
            "Episode 1890, Reward: 23.49, Best: 113.21\n",
            "Episode 1900, Reward: 96.54, Best: 113.23\n",
            "Episode 1910, Reward: 73.26, Best: 121.37\n",
            "Episode 1920, Reward: 94.75, Best: 121.37\n",
            "Episode 1930, Reward: 43.29, Best: 122.00\n",
            "Episode 1940, Reward: 108.83, Best: 122.83\n",
            "Episode 1950, Reward: 55.27, Best: 122.83\n",
            "Episode 1960, Reward: 102.26, Best: 122.83\n",
            "Episode 1970, Reward: 75.95, Best: 122.83\n",
            "Episode 1980, Reward: 76.40, Best: 122.83\n",
            "Episode 1990, Reward: 75.20, Best: 122.83\n",
            "Episode 2000, Reward: 96.07, Best: 129.77\n",
            "Episode 2010, Reward: 67.77, Best: 129.77\n",
            "Episode 2020, Reward: 69.28, Best: 129.77\n",
            "Episode 2030, Reward: 97.41, Best: 129.77\n",
            "Episode 2040, Reward: 31.79, Best: 129.77\n",
            "Episode 2050, Reward: -73.84, Best: 129.77\n",
            "Episode 2060, Reward: -112.53, Best: 129.77\n",
            "Episode 2070, Reward: 94.86, Best: 129.77\n",
            "Episode 2080, Reward: -30.88, Best: 129.77\n",
            "Episode 2090, Reward: -70.64, Best: 178.59\n",
            "Episode 2100, Reward: 71.63, Best: 178.59\n",
            "Episode 2110, Reward: 53.96, Best: 178.59\n",
            "Episode 2120, Reward: -39.77, Best: 178.59\n",
            "Episode 2130, Reward: 68.85, Best: 178.59\n",
            "Episode 2140, Reward: -4.00, Best: 178.59\n",
            "Episode 2150, Reward: 90.95, Best: 178.59\n",
            "Episode 2160, Reward: 79.74, Best: 178.59\n",
            "Episode 2170, Reward: -39.39, Best: 178.59\n",
            "Episode 2180, Reward: -8.80, Best: 178.59\n",
            "Episode 2190, Reward: 115.31, Best: 178.59\n",
            "Episode 2200, Reward: 102.94, Best: 178.59\n",
            "Episode 2210, Reward: 61.85, Best: 178.59\n",
            "Episode 2220, Reward: -69.86, Best: 178.59\n",
            "Episode 2230, Reward: -28.39, Best: 178.59\n",
            "Episode 2240, Reward: -105.40, Best: 178.59\n",
            "Episode 2250, Reward: 19.63, Best: 178.59\n",
            "Episode 2260, Reward: 54.92, Best: 178.59\n",
            "Episode 2270, Reward: -67.87, Best: 178.59\n",
            "Episode 2280, Reward: -93.29, Best: 178.59\n",
            "Episode 2290, Reward: 237.46, Best: 237.46\n",
            "Episode 2300, Reward: 75.74, Best: 237.46\n",
            "Episode 2310, Reward: 54.58, Best: 237.46\n",
            "Episode 2320, Reward: 40.67, Best: 237.46\n",
            "Episode 2330, Reward: 79.13, Best: 237.46\n",
            "Episode 2340, Reward: -78.06, Best: 237.46\n",
            "Episode 2350, Reward: -18.22, Best: 237.46\n",
            "Episode 2360, Reward: -75.89, Best: 237.46\n",
            "Episode 2370, Reward: 110.60, Best: 237.46\n",
            "Episode 2380, Reward: -86.62, Best: 237.46\n",
            "Episode 2390, Reward: -78.26, Best: 237.46\n",
            "Episode 2400, Reward: 39.85, Best: 237.46\n",
            "Episode 2410, Reward: 54.05, Best: 237.46\n",
            "Episode 2420, Reward: -55.69, Best: 237.46\n",
            "Episode 2430, Reward: -95.27, Best: 237.46\n",
            "Episode 2440, Reward: -116.14, Best: 237.46\n",
            "Episode 2450, Reward: 45.02, Best: 237.46\n",
            "Episode 2460, Reward: 103.30, Best: 237.46\n",
            "Episode 2470, Reward: 68.13, Best: 237.46\n",
            "Episode 2480, Reward: 84.48, Best: 237.46\n",
            "Episode 2490, Reward: 5.06, Best: 237.46\n",
            "Episode 2500, Reward: 3.69, Best: 237.46\n",
            "Episode 2510, Reward: -28.62, Best: 237.46\n",
            "Episode 2520, Reward: -75.25, Best: 237.46\n",
            "Episode 2530, Reward: 34.24, Best: 237.46\n",
            "Episode 2540, Reward: -28.04, Best: 237.46\n",
            "Episode 2550, Reward: 78.16, Best: 237.46\n",
            "Episode 2560, Reward: 63.20, Best: 237.46\n",
            "Episode 2570, Reward: -91.62, Best: 237.46\n",
            "Episode 2580, Reward: -36.19, Best: 237.46\n",
            "Episode 2590, Reward: -47.45, Best: 237.46\n",
            "Episode 2600, Reward: 48.82, Best: 237.46\n",
            "Episode 2610, Reward: 112.06, Best: 237.46\n",
            "Episode 2620, Reward: 31.12, Best: 237.46\n",
            "Episode 2630, Reward: 105.99, Best: 237.46\n",
            "Episode 2640, Reward: 92.13, Best: 237.46\n",
            "Episode 2650, Reward: 93.44, Best: 237.46\n",
            "Episode 2660, Reward: 56.31, Best: 237.46\n",
            "Episode 2670, Reward: 50.06, Best: 237.46\n",
            "Episode 2680, Reward: -86.66, Best: 237.46\n",
            "Episode 2690, Reward: 39.82, Best: 237.46\n",
            "Episode 2700, Reward: -7.91, Best: 237.46\n",
            "Episode 2710, Reward: 44.50, Best: 237.46\n",
            "Episode 2720, Reward: 86.70, Best: 237.46\n",
            "Episode 2730, Reward: 108.47, Best: 237.46\n",
            "Episode 2740, Reward: -10.94, Best: 237.46\n",
            "Episode 2750, Reward: 54.94, Best: 237.46\n",
            "Episode 2760, Reward: -32.45, Best: 237.46\n",
            "Episode 2770, Reward: -69.14, Best: 237.46\n",
            "Episode 2780, Reward: 106.39, Best: 237.46\n",
            "Episode 2790, Reward: 91.61, Best: 237.46\n",
            "Episode 2800, Reward: 95.36, Best: 237.46\n",
            "Episode 2810, Reward: 45.45, Best: 237.46\n",
            "Episode 2820, Reward: 90.16, Best: 237.46\n",
            "Episode 2830, Reward: -76.92, Best: 237.46\n",
            "Episode 2840, Reward: 43.89, Best: 237.46\n",
            "Episode 2850, Reward: 79.31, Best: 237.46\n",
            "Episode 2860, Reward: -76.29, Best: 237.46\n",
            "Episode 2870, Reward: -32.48, Best: 237.46\n",
            "Episode 2880, Reward: 99.20, Best: 237.46\n",
            "Episode 2890, Reward: -57.55, Best: 237.46\n",
            "Episode 2900, Reward: 92.35, Best: 237.46\n",
            "Episode 2910, Reward: 52.31, Best: 237.46\n",
            "Episode 2920, Reward: -45.76, Best: 237.46\n",
            "Episode 2930, Reward: 56.22, Best: 237.46\n",
            "Episode 2940, Reward: 42.61, Best: 237.46\n",
            "Episode 2950, Reward: 88.65, Best: 237.46\n",
            "Episode 2960, Reward: 72.07, Best: 237.46\n",
            "Episode 2970, Reward: 57.88, Best: 237.46\n",
            "Episode 2980, Reward: -92.11, Best: 237.46\n",
            "Episode 2990, Reward: 36.97, Best: 237.46\n",
            "Episode 3000, Reward: 45.84, Best: 237.46\n",
            "Episode 3010, Reward: -101.76, Best: 237.46\n",
            "Episode 3020, Reward: -33.75, Best: 237.46\n",
            "Episode 3030, Reward: 75.45, Best: 237.46\n",
            "Episode 3040, Reward: 84.08, Best: 237.46\n",
            "Episode 3050, Reward: 34.30, Best: 237.46\n",
            "Episode 3060, Reward: 39.86, Best: 237.46\n",
            "Episode 3070, Reward: 55.36, Best: 237.46\n",
            "Episode 3080, Reward: 38.63, Best: 237.46\n",
            "Episode 3090, Reward: 80.30, Best: 237.46\n",
            "Episode 3100, Reward: 79.12, Best: 237.46\n",
            "Episode 3110, Reward: 86.90, Best: 237.46\n",
            "Episode 3120, Reward: 57.25, Best: 237.46\n",
            "Episode 3130, Reward: 54.44, Best: 237.46\n",
            "Episode 3140, Reward: 63.16, Best: 237.46\n",
            "Episode 3150, Reward: 70.43, Best: 237.46\n",
            "Episode 3160, Reward: -52.42, Best: 237.46\n",
            "Episode 3170, Reward: 65.38, Best: 237.46\n",
            "Episode 3180, Reward: 87.06, Best: 237.46\n",
            "Episode 3190, Reward: -58.40, Best: 237.46\n",
            "Episode 3200, Reward: -26.81, Best: 237.46\n",
            "Episode 3210, Reward: 97.42, Best: 237.46\n",
            "Episode 3220, Reward: 54.49, Best: 237.46\n",
            "Episode 3230, Reward: 92.58, Best: 237.46\n",
            "Episode 3240, Reward: 87.54, Best: 237.46\n",
            "Episode 3250, Reward: 73.76, Best: 237.46\n",
            "Episode 3260, Reward: 121.26, Best: 237.46\n",
            "Episode 3270, Reward: 69.70, Best: 237.46\n",
            "Episode 3280, Reward: 89.17, Best: 237.46\n",
            "Episode 3290, Reward: 70.49, Best: 237.46\n",
            "Episode 3300, Reward: 92.73, Best: 237.46\n",
            "Episode 3310, Reward: 77.16, Best: 237.46\n",
            "Episode 3320, Reward: 92.89, Best: 237.46\n",
            "Episode 3330, Reward: 45.89, Best: 237.46\n",
            "Episode 3340, Reward: -25.10, Best: 237.46\n",
            "Episode 3350, Reward: 67.35, Best: 237.46\n",
            "Episode 3360, Reward: 75.39, Best: 237.46\n",
            "Episode 3370, Reward: 44.07, Best: 237.46\n",
            "Episode 3380, Reward: 76.70, Best: 237.46\n",
            "Episode 3390, Reward: 85.21, Best: 237.46\n",
            "Episode 3400, Reward: 66.32, Best: 237.46\n",
            "Episode 3410, Reward: 75.49, Best: 237.46\n",
            "Episode 3420, Reward: 62.49, Best: 237.46\n",
            "Episode 3430, Reward: 62.95, Best: 237.46\n",
            "Episode 3440, Reward: 90.99, Best: 237.46\n",
            "Episode 3450, Reward: -74.24, Best: 237.46\n",
            "Episode 3460, Reward: 34.69, Best: 237.46\n",
            "Episode 3470, Reward: -73.99, Best: 237.46\n",
            "Episode 3480, Reward: -64.64, Best: 237.46\n",
            "Episode 3490, Reward: 36.96, Best: 237.46\n",
            "Episode 3500, Reward: 75.02, Best: 237.46\n",
            "Episode 3510, Reward: 48.39, Best: 237.46\n",
            "Episode 3520, Reward: 71.37, Best: 237.46\n",
            "Episode 3530, Reward: 50.98, Best: 237.46\n",
            "Episode 3540, Reward: -56.42, Best: 237.46\n",
            "Episode 3550, Reward: 55.92, Best: 237.46\n",
            "Episode 3560, Reward: -89.68, Best: 237.46\n",
            "Episode 3570, Reward: 25.23, Best: 237.46\n",
            "Episode 3580, Reward: 38.96, Best: 237.46\n",
            "Episode 3590, Reward: 48.29, Best: 237.46\n",
            "Episode 3600, Reward: 49.68, Best: 237.46\n",
            "Episode 3610, Reward: 35.81, Best: 237.46\n",
            "Episode 3620, Reward: 49.34, Best: 237.46\n",
            "Episode 3630, Reward: 39.14, Best: 237.46\n",
            "Episode 3640, Reward: 31.15, Best: 237.46\n",
            "Episode 3650, Reward: 39.52, Best: 237.46\n",
            "Episode 3660, Reward: 32.18, Best: 237.46\n",
            "Episode 3670, Reward: -71.71, Best: 237.46\n",
            "Episode 3680, Reward: 34.06, Best: 237.46\n",
            "Episode 3690, Reward: 58.48, Best: 237.46\n",
            "Episode 3700, Reward: 50.09, Best: 237.46\n",
            "Episode 3710, Reward: 37.54, Best: 237.46\n",
            "Episode 3720, Reward: 38.95, Best: 237.46\n",
            "Episode 3730, Reward: -81.10, Best: 237.46\n",
            "Episode 3740, Reward: 45.66, Best: 237.46\n",
            "Episode 3750, Reward: 34.49, Best: 237.46\n",
            "Episode 3760, Reward: 38.92, Best: 237.46\n",
            "Episode 3770, Reward: 65.02, Best: 237.46\n",
            "Episode 3780, Reward: 68.31, Best: 237.46\n",
            "Episode 3790, Reward: 42.39, Best: 237.46\n",
            "Episode 3800, Reward: 68.21, Best: 237.46\n",
            "Episode 3810, Reward: -73.89, Best: 237.46\n",
            "Episode 3820, Reward: -30.91, Best: 237.46\n",
            "Episode 3830, Reward: -61.60, Best: 237.46\n",
            "Episode 3840, Reward: -42.53, Best: 237.46\n",
            "Episode 3850, Reward: -21.72, Best: 237.46\n",
            "Episode 3860, Reward: 51.93, Best: 237.46\n",
            "Episode 3870, Reward: -70.77, Best: 237.46\n",
            "Episode 3880, Reward: 53.65, Best: 237.46\n",
            "Episode 3890, Reward: 40.49, Best: 237.46\n",
            "Episode 3900, Reward: 59.61, Best: 237.46\n",
            "Episode 3910, Reward: 82.45, Best: 237.46\n",
            "Episode 3920, Reward: 74.56, Best: 237.46\n",
            "Episode 3930, Reward: -69.85, Best: 237.46\n",
            "Episode 3940, Reward: -78.89, Best: 237.46\n",
            "Episode 3950, Reward: 106.01, Best: 237.46\n",
            "Episode 3960, Reward: 71.60, Best: 237.46\n",
            "Episode 3970, Reward: 62.29, Best: 237.46\n",
            "Episode 3980, Reward: 71.90, Best: 237.46\n",
            "Episode 3990, Reward: 76.74, Best: 237.46\n",
            "Episode 4000, Reward: 84.86, Best: 237.46\n",
            "Episode 4010, Reward: 55.14, Best: 237.46\n",
            "Episode 4020, Reward: 56.68, Best: 237.46\n",
            "Episode 4030, Reward: -82.03, Best: 237.46\n",
            "Episode 4040, Reward: 56.97, Best: 237.46\n",
            "Episode 4050, Reward: 47.91, Best: 237.46\n",
            "Episode 4060, Reward: 70.71, Best: 237.46\n",
            "Episode 4070, Reward: 53.13, Best: 237.46\n",
            "Episode 4080, Reward: -82.46, Best: 237.46\n",
            "Episode 4090, Reward: 48.34, Best: 237.46\n",
            "Episode 4100, Reward: -95.47, Best: 237.46\n",
            "Episode 4110, Reward: 97.88, Best: 237.46\n",
            "Episode 4120, Reward: 73.82, Best: 237.46\n",
            "Episode 4130, Reward: 41.08, Best: 237.46\n",
            "Episode 4140, Reward: 43.49, Best: 237.46\n",
            "Episode 4150, Reward: 60.77, Best: 237.46\n",
            "Episode 4160, Reward: 68.73, Best: 237.46\n",
            "Episode 4170, Reward: -15.55, Best: 237.46\n",
            "Episode 4180, Reward: 51.07, Best: 237.46\n",
            "Episode 4190, Reward: 83.54, Best: 237.46\n",
            "Episode 4200, Reward: 74.63, Best: 237.46\n",
            "Episode 4210, Reward: -70.30, Best: 237.46\n",
            "Episode 4220, Reward: -21.17, Best: 237.46\n",
            "Episode 4230, Reward: 75.09, Best: 237.46\n",
            "Episode 4240, Reward: 51.84, Best: 237.46\n",
            "Episode 4250, Reward: 45.72, Best: 237.46\n",
            "Episode 4260, Reward: 72.64, Best: 237.46\n",
            "Episode 4270, Reward: 35.25, Best: 237.46\n",
            "Episode 4280, Reward: 67.37, Best: 237.46\n",
            "Episode 4290, Reward: 72.65, Best: 237.46\n",
            "Episode 4300, Reward: 66.80, Best: 237.46\n",
            "Episode 4310, Reward: 95.28, Best: 237.46\n",
            "Episode 4320, Reward: 71.60, Best: 237.46\n",
            "Episode 4330, Reward: 94.29, Best: 237.46\n",
            "Episode 4340, Reward: 92.85, Best: 237.46\n",
            "Episode 4350, Reward: 47.43, Best: 237.46\n",
            "Episode 4360, Reward: -47.68, Best: 237.46\n",
            "Episode 4370, Reward: 96.07, Best: 237.46\n",
            "Episode 4380, Reward: -63.48, Best: 237.46\n",
            "Episode 4390, Reward: 37.40, Best: 237.46\n",
            "Episode 4400, Reward: 76.74, Best: 237.46\n",
            "Episode 4410, Reward: 98.25, Best: 237.46\n",
            "Episode 4420, Reward: 99.84, Best: 237.46\n",
            "Episode 4430, Reward: 57.50, Best: 237.46\n",
            "Episode 4440, Reward: 83.50, Best: 237.46\n",
            "Episode 4450, Reward: 12.72, Best: 237.46\n",
            "Episode 4460, Reward: 105.99, Best: 237.46\n",
            "Episode 4470, Reward: 84.38, Best: 237.46\n",
            "Episode 4480, Reward: 5.37, Best: 237.46\n",
            "Episode 4490, Reward: 93.00, Best: 237.46\n",
            "Episode 4500, Reward: 85.32, Best: 237.46\n",
            "Episode 4510, Reward: 63.58, Best: 237.46\n",
            "Episode 4520, Reward: 89.51, Best: 237.46\n",
            "Episode 4530, Reward: 59.83, Best: 237.46\n",
            "Episode 4540, Reward: 75.02, Best: 237.46\n",
            "Episode 4550, Reward: 82.94, Best: 237.46\n",
            "Episode 4560, Reward: 78.10, Best: 237.46\n",
            "Episode 4570, Reward: 79.30, Best: 237.46\n",
            "Episode 4580, Reward: 73.56, Best: 237.46\n",
            "Episode 4590, Reward: 79.49, Best: 237.46\n",
            "Episode 4600, Reward: 62.37, Best: 237.46\n",
            "Episode 4610, Reward: 109.83, Best: 237.46\n",
            "Episode 4620, Reward: 89.80, Best: 237.46\n",
            "Episode 4630, Reward: 78.84, Best: 237.46\n",
            "Episode 4640, Reward: 82.17, Best: 237.46\n",
            "Episode 4650, Reward: 111.81, Best: 237.46\n",
            "Episode 4660, Reward: 48.43, Best: 237.46\n",
            "Episode 4670, Reward: 98.66, Best: 237.46\n",
            "Episode 4680, Reward: -46.74, Best: 237.46\n",
            "Episode 4690, Reward: 99.30, Best: 237.46\n",
            "Episode 4700, Reward: 77.19, Best: 237.46\n",
            "Episode 4710, Reward: 120.15, Best: 237.46\n",
            "Episode 4720, Reward: -57.75, Best: 237.46\n",
            "Episode 4730, Reward: -36.34, Best: 237.46\n",
            "Episode 4740, Reward: -8.91, Best: 237.46\n",
            "Episode 4750, Reward: -106.97, Best: 237.46\n",
            "Episode 4760, Reward: -16.48, Best: 237.46\n",
            "Episode 4770, Reward: 107.40, Best: 237.46\n",
            "Episode 4780, Reward: 97.53, Best: 237.46\n",
            "Episode 4790, Reward: 84.53, Best: 237.46\n",
            "Episode 4800, Reward: 85.00, Best: 237.46\n",
            "Episode 4810, Reward: 74.54, Best: 237.46\n",
            "Episode 4820, Reward: 92.58, Best: 237.46\n",
            "Episode 4830, Reward: 106.60, Best: 237.46\n",
            "Episode 4840, Reward: 90.99, Best: 237.46\n",
            "Episode 4850, Reward: 112.18, Best: 237.46\n",
            "Episode 4860, Reward: 67.24, Best: 237.46\n",
            "Episode 4870, Reward: 73.55, Best: 237.46\n",
            "Episode 4880, Reward: 86.13, Best: 237.46\n",
            "Episode 4890, Reward: 63.39, Best: 237.46\n",
            "Episode 4900, Reward: 75.40, Best: 237.46\n",
            "Episode 4910, Reward: 85.54, Best: 237.46\n",
            "Episode 4920, Reward: 71.85, Best: 237.46\n",
            "Episode 4930, Reward: -70.36, Best: 237.46\n",
            "Episode 4940, Reward: -87.66, Best: 237.46\n",
            "Episode 4950, Reward: 75.69, Best: 237.46\n",
            "Episode 4960, Reward: -10.43, Best: 237.46\n",
            "Episode 4970, Reward: 87.54, Best: 237.46\n",
            "Episode 4980, Reward: 7.42, Best: 237.46\n",
            "Episode 4990, Reward: -5.13, Best: 237.46\n",
            "Episode 5000, Reward: 113.79, Best: 237.46\n",
            "Episode 5010, Reward: 99.83, Best: 237.46\n",
            "Episode 5020, Reward: 106.55, Best: 237.46\n",
            "Episode 5030, Reward: -31.24, Best: 237.46\n",
            "Episode 5040, Reward: -36.64, Best: 237.46\n",
            "Episode 5050, Reward: 90.77, Best: 237.46\n",
            "Episode 5060, Reward: 102.64, Best: 237.46\n",
            "Episode 5070, Reward: -62.72, Best: 237.46\n",
            "Episode 5080, Reward: -61.53, Best: 237.46\n",
            "Episode 5090, Reward: -10.61, Best: 237.46\n",
            "Episode 5100, Reward: 97.10, Best: 237.46\n",
            "Episode 5110, Reward: -44.88, Best: 237.46\n",
            "Episode 5120, Reward: 86.12, Best: 237.46\n",
            "Episode 5130, Reward: 97.34, Best: 237.46\n",
            "Episode 5140, Reward: -1.18, Best: 237.46\n",
            "Episode 5150, Reward: -2.60, Best: 237.46\n",
            "Episode 5160, Reward: 90.93, Best: 237.46\n",
            "Episode 5170, Reward: 88.85, Best: 237.46\n",
            "Episode 5180, Reward: 105.18, Best: 237.46\n",
            "Episode 5190, Reward: 86.76, Best: 237.46\n",
            "Episode 5200, Reward: 79.17, Best: 237.46\n",
            "Episode 5210, Reward: 93.89, Best: 237.46\n",
            "Episode 5220, Reward: 123.97, Best: 237.46\n",
            "Episode 5230, Reward: 122.52, Best: 237.46\n",
            "Episode 5240, Reward: 80.70, Best: 237.46\n",
            "Episode 5250, Reward: 95.35, Best: 237.46\n",
            "Episode 5260, Reward: 115.75, Best: 237.46\n",
            "Episode 5270, Reward: 89.56, Best: 237.46\n",
            "Episode 5280, Reward: 86.88, Best: 237.46\n",
            "Episode 5290, Reward: 87.35, Best: 237.46\n",
            "Episode 5300, Reward: 91.32, Best: 237.46\n",
            "Episode 5310, Reward: -45.85, Best: 237.46\n",
            "Episode 5320, Reward: 3.25, Best: 237.46\n",
            "Episode 5330, Reward: -18.03, Best: 237.46\n",
            "Episode 5340, Reward: 4.42, Best: 237.46\n",
            "Episode 5350, Reward: -11.01, Best: 237.46\n",
            "Episode 5360, Reward: -29.91, Best: 237.46\n",
            "Episode 5370, Reward: -10.24, Best: 237.46\n",
            "Episode 5380, Reward: -66.88, Best: 237.46\n",
            "Episode 5390, Reward: -70.33, Best: 237.46\n",
            "Episode 5400, Reward: -53.61, Best: 237.46\n",
            "Episode 5410, Reward: 70.32, Best: 237.46\n",
            "Episode 5420, Reward: 80.58, Best: 237.46\n",
            "Episode 5430, Reward: -28.00, Best: 237.46\n",
            "Episode 5440, Reward: 94.51, Best: 237.46\n",
            "Episode 5450, Reward: 121.03, Best: 237.46\n",
            "Episode 5460, Reward: 70.76, Best: 237.46\n",
            "Episode 5470, Reward: 109.09, Best: 237.46\n",
            "Episode 5480, Reward: 113.07, Best: 237.46\n",
            "Episode 5490, Reward: -53.83, Best: 237.46\n",
            "Episode 5500, Reward: 66.00, Best: 237.46\n",
            "Episode 5510, Reward: 83.02, Best: 237.46\n",
            "Episode 5520, Reward: 95.61, Best: 237.46\n",
            "Episode 5530, Reward: 109.93, Best: 237.46\n",
            "Episode 5540, Reward: 102.07, Best: 237.46\n",
            "Episode 5550, Reward: 108.81, Best: 237.46\n",
            "Episode 5560, Reward: 108.48, Best: 237.46\n",
            "Episode 5570, Reward: 85.54, Best: 237.46\n",
            "Episode 5580, Reward: 74.09, Best: 237.46\n",
            "Episode 5590, Reward: 82.02, Best: 237.46\n",
            "Episode 5600, Reward: 115.63, Best: 237.46\n",
            "Episode 5610, Reward: 114.25, Best: 237.46\n",
            "Episode 5620, Reward: 95.43, Best: 237.46\n",
            "Episode 5630, Reward: 95.88, Best: 237.46\n",
            "Episode 5640, Reward: 108.19, Best: 237.46\n",
            "Episode 5650, Reward: 105.82, Best: 237.46\n",
            "Episode 5660, Reward: 95.51, Best: 237.46\n",
            "Episode 5670, Reward: 75.62, Best: 237.46\n",
            "Episode 5680, Reward: 98.33, Best: 237.46\n",
            "Episode 5690, Reward: -9.10, Best: 237.46\n",
            "Episode 5700, Reward: 76.50, Best: 237.46\n",
            "Episode 5710, Reward: 130.50, Best: 237.46\n",
            "Episode 5720, Reward: 98.00, Best: 237.46\n",
            "Episode 5730, Reward: 99.48, Best: 237.46\n",
            "Episode 5740, Reward: 117.78, Best: 237.46\n",
            "Episode 5750, Reward: 96.74, Best: 237.46\n",
            "Episode 5760, Reward: 80.00, Best: 237.46\n",
            "Episode 5770, Reward: 92.64, Best: 237.46\n",
            "Episode 5780, Reward: 104.60, Best: 237.46\n",
            "Episode 5790, Reward: 83.76, Best: 237.46\n",
            "Episode 5800, Reward: -20.17, Best: 237.46\n",
            "Episode 5810, Reward: 107.22, Best: 237.46\n",
            "Episode 5820, Reward: 82.98, Best: 237.46\n",
            "Episode 5830, Reward: -86.58, Best: 237.46\n",
            "Episode 5840, Reward: -116.83, Best: 237.46\n",
            "Episode 5850, Reward: 88.69, Best: 237.46\n",
            "Episode 5860, Reward: -51.63, Best: 237.46\n",
            "Episode 5870, Reward: -52.68, Best: 237.46\n",
            "Episode 5880, Reward: 128.77, Best: 237.46\n",
            "Episode 5890, Reward: 93.19, Best: 237.46\n",
            "Episode 5900, Reward: -24.30, Best: 237.46\n",
            "Episode 5910, Reward: 99.40, Best: 237.46\n",
            "Episode 5920, Reward: 107.11, Best: 237.46\n",
            "Episode 5930, Reward: 87.33, Best: 237.46\n",
            "Episode 5940, Reward: 104.85, Best: 237.46\n",
            "Episode 5950, Reward: 94.25, Best: 237.46\n",
            "Episode 5960, Reward: 47.87, Best: 237.46\n",
            "Episode 5970, Reward: -25.47, Best: 237.46\n",
            "Episode 5980, Reward: 100.46, Best: 237.46\n",
            "Episode 5990, Reward: 105.16, Best: 237.46\n",
            "Episode 6000, Reward: 102.61, Best: 237.46\n",
            "Episode 6010, Reward: 100.25, Best: 237.46\n",
            "Episode 6020, Reward: 72.25, Best: 237.46\n",
            "Episode 6030, Reward: 117.02, Best: 237.46\n",
            "Episode 6040, Reward: 100.12, Best: 237.46\n",
            "Episode 6050, Reward: 87.16, Best: 237.46\n",
            "Episode 6060, Reward: 86.97, Best: 237.46\n",
            "Episode 6070, Reward: 118.53, Best: 237.46\n",
            "Episode 6080, Reward: 110.06, Best: 237.46\n",
            "Episode 6090, Reward: 80.82, Best: 237.46\n",
            "Episode 6100, Reward: 122.40, Best: 237.46\n",
            "Episode 6110, Reward: 101.19, Best: 237.46\n",
            "Episode 6120, Reward: 111.65, Best: 237.46\n",
            "Episode 6130, Reward: -15.70, Best: 237.46\n",
            "Episode 6140, Reward: -39.44, Best: 237.46\n",
            "Episode 6150, Reward: 101.81, Best: 237.46\n",
            "Episode 6160, Reward: 96.31, Best: 237.46\n",
            "Episode 6170, Reward: -40.62, Best: 237.46\n",
            "Episode 6180, Reward: 108.57, Best: 237.46\n",
            "Episode 6190, Reward: 17.41, Best: 237.46\n",
            "Episode 6200, Reward: 104.86, Best: 237.46\n",
            "Episode 6210, Reward: 4.28, Best: 237.46\n",
            "Episode 6220, Reward: 164.88, Best: 237.46\n",
            "Episode 6230, Reward: 92.55, Best: 237.46\n",
            "Episode 6240, Reward: 129.31, Best: 237.46\n",
            "Episode 6250, Reward: 87.61, Best: 237.46\n",
            "Episode 6260, Reward: 89.75, Best: 237.46\n",
            "Episode 6270, Reward: 79.33, Best: 237.46\n",
            "Episode 6280, Reward: 117.97, Best: 237.46\n",
            "Episode 6290, Reward: 80.49, Best: 237.46\n",
            "Episode 6300, Reward: 87.54, Best: 237.46\n",
            "Episode 6310, Reward: 89.76, Best: 237.46\n",
            "Episode 6320, Reward: 105.37, Best: 237.46\n",
            "Episode 6330, Reward: 106.55, Best: 237.46\n",
            "Episode 6340, Reward: 95.54, Best: 237.46\n",
            "Episode 6350, Reward: 102.43, Best: 237.46\n",
            "Episode 6360, Reward: 110.15, Best: 237.46\n",
            "Episode 6370, Reward: 97.55, Best: 237.46\n",
            "Episode 6380, Reward: 100.27, Best: 237.46\n",
            "Episode 6390, Reward: 92.13, Best: 237.46\n",
            "Episode 6400, Reward: 99.63, Best: 237.46\n",
            "Episode 6410, Reward: 98.24, Best: 243.32\n",
            "Episode 6420, Reward: 122.24, Best: 243.32\n",
            "Episode 6430, Reward: -61.89, Best: 243.32\n",
            "Episode 6440, Reward: 117.43, Best: 243.32\n",
            "Episode 6450, Reward: -2.67, Best: 243.32\n",
            "Episode 6460, Reward: 3.11, Best: 243.32\n",
            "Episode 6470, Reward: 8.54, Best: 243.32\n",
            "Episode 6480, Reward: 16.85, Best: 243.32\n",
            "Episode 6490, Reward: 90.66, Best: 243.32\n",
            "Episode 6500, Reward: -19.11, Best: 243.32\n",
            "Episode 6510, Reward: 79.00, Best: 243.32\n",
            "Episode 6520, Reward: 86.38, Best: 243.32\n",
            "Episode 6530, Reward: 100.88, Best: 243.32\n",
            "Episode 6540, Reward: 71.27, Best: 243.32\n",
            "Episode 6550, Reward: 66.88, Best: 243.32\n",
            "Episode 6560, Reward: 57.79, Best: 243.32\n",
            "Episode 6570, Reward: 122.10, Best: 243.32\n",
            "Episode 6580, Reward: 101.04, Best: 243.32\n",
            "Episode 6590, Reward: 123.86, Best: 243.32\n",
            "Episode 6600, Reward: 133.89, Best: 243.32\n",
            "Episode 6610, Reward: 105.71, Best: 243.32\n",
            "Episode 6620, Reward: 98.57, Best: 243.32\n",
            "Episode 6630, Reward: 98.29, Best: 243.32\n",
            "Episode 6640, Reward: 112.66, Best: 243.32\n",
            "Episode 6650, Reward: 94.16, Best: 243.32\n",
            "Episode 6660, Reward: 102.90, Best: 243.32\n",
            "Episode 6670, Reward: 101.37, Best: 243.32\n",
            "Episode 6680, Reward: 117.29, Best: 243.32\n",
            "Episode 6690, Reward: 90.25, Best: 243.32\n",
            "Episode 6700, Reward: 95.06, Best: 243.32\n",
            "Episode 6710, Reward: 85.84, Best: 243.32\n",
            "Episode 6720, Reward: 124.69, Best: 243.32\n",
            "Episode 6730, Reward: 136.15, Best: 243.32\n",
            "Episode 6740, Reward: 112.49, Best: 243.32\n",
            "Episode 6750, Reward: 101.21, Best: 243.32\n",
            "Episode 6760, Reward: 114.53, Best: 243.32\n",
            "Episode 6770, Reward: 116.99, Best: 243.32\n",
            "Episode 6780, Reward: 112.45, Best: 243.32\n",
            "Episode 6790, Reward: 64.15, Best: 243.32\n",
            "Episode 6800, Reward: 116.12, Best: 243.32\n",
            "Episode 6810, Reward: 104.17, Best: 243.32\n",
            "Episode 6820, Reward: 116.06, Best: 243.32\n",
            "Episode 6830, Reward: 113.01, Best: 243.32\n",
            "Episode 6840, Reward: 26.09, Best: 243.32\n",
            "Episode 6850, Reward: -0.31, Best: 243.32\n",
            "Episode 6860, Reward: 121.00, Best: 243.32\n",
            "Episode 6870, Reward: 97.64, Best: 243.32\n",
            "Episode 6880, Reward: 150.72, Best: 243.32\n",
            "Episode 6890, Reward: -10.55, Best: 243.32\n",
            "Episode 6900, Reward: -5.37, Best: 243.32\n",
            "Episode 6910, Reward: -65.88, Best: 243.32\n",
            "Episode 6920, Reward: 108.88, Best: 243.32\n",
            "Episode 6930, Reward: 114.35, Best: 243.32\n",
            "Episode 6940, Reward: 143.89, Best: 243.32\n",
            "Episode 6950, Reward: 96.32, Best: 243.32\n",
            "Episode 6960, Reward: 104.05, Best: 243.32\n",
            "Episode 6970, Reward: 105.93, Best: 243.32\n",
            "Episode 6980, Reward: 124.10, Best: 243.32\n",
            "Episode 6990, Reward: 99.14, Best: 243.32\n",
            "Episode 7000, Reward: 102.64, Best: 243.32\n",
            "Episode 7010, Reward: 105.01, Best: 243.32\n",
            "Episode 7020, Reward: 148.08, Best: 243.32\n",
            "Episode 7030, Reward: 114.19, Best: 243.32\n",
            "Episode 7040, Reward: 141.55, Best: 243.32\n",
            "Episode 7050, Reward: 120.01, Best: 243.32\n",
            "Episode 7060, Reward: 126.86, Best: 243.32\n",
            "Episode 7070, Reward: 90.55, Best: 243.32\n",
            "Episode 7080, Reward: 112.94, Best: 243.32\n",
            "Episode 7090, Reward: -17.77, Best: 243.32\n",
            "Episode 7100, Reward: -89.57, Best: 243.32\n",
            "Episode 7110, Reward: -99.87, Best: 243.32\n",
            "Episode 7120, Reward: -10.38, Best: 243.32\n",
            "Episode 7130, Reward: 106.84, Best: 243.32\n",
            "Episode 7140, Reward: 74.84, Best: 243.32\n",
            "Episode 7150, Reward: 118.49, Best: 243.32\n",
            "Episode 7160, Reward: 71.09, Best: 243.32\n",
            "Episode 7170, Reward: 100.85, Best: 243.32\n",
            "Episode 7180, Reward: 113.88, Best: 243.32\n",
            "Episode 7190, Reward: 88.80, Best: 243.32\n",
            "Episode 7200, Reward: 93.21, Best: 279.81\n",
            "Episode 7210, Reward: 92.99, Best: 279.81\n",
            "Episode 7220, Reward: 83.28, Best: 279.81\n",
            "Episode 7230, Reward: 103.26, Best: 279.81\n",
            "Episode 7240, Reward: 115.82, Best: 279.81\n",
            "Episode 7250, Reward: 109.31, Best: 279.81\n",
            "Episode 7260, Reward: 88.51, Best: 279.81\n",
            "Episode 7270, Reward: 86.76, Best: 279.81\n",
            "Episode 7280, Reward: -20.63, Best: 279.81\n",
            "Episode 7290, Reward: 126.25, Best: 279.81\n",
            "Episode 7300, Reward: 82.34, Best: 279.81\n",
            "Episode 7310, Reward: 86.79, Best: 279.81\n",
            "Episode 7320, Reward: 121.22, Best: 279.81\n",
            "Episode 7330, Reward: 41.27, Best: 279.81\n",
            "Episode 7340, Reward: -36.47, Best: 279.81\n",
            "Episode 7350, Reward: -63.62, Best: 279.81\n",
            "Episode 7360, Reward: -79.38, Best: 279.81\n",
            "Episode 7370, Reward: -62.62, Best: 279.81\n",
            "Episode 7380, Reward: -78.07, Best: 279.81\n",
            "Episode 7390, Reward: 112.87, Best: 279.81\n",
            "Episode 7400, Reward: 89.57, Best: 279.81\n",
            "Episode 7410, Reward: 88.29, Best: 279.81\n",
            "Episode 7420, Reward: 120.32, Best: 279.81\n",
            "Episode 7430, Reward: 133.36, Best: 279.81\n",
            "Episode 7440, Reward: 104.12, Best: 279.81\n",
            "Episode 7450, Reward: 52.24, Best: 279.81\n",
            "Episode 7460, Reward: 116.50, Best: 279.81\n",
            "Episode 7470, Reward: 55.65, Best: 279.81\n",
            "Episode 7480, Reward: 132.63, Best: 279.81\n",
            "Episode 7490, Reward: -0.94, Best: 279.81\n",
            "Episode 7500, Reward: 45.68, Best: 279.81\n",
            "Episode 7510, Reward: 109.89, Best: 279.81\n",
            "Episode 7520, Reward: 213.34, Best: 279.81\n",
            "Episode 7530, Reward: 119.40, Best: 279.81\n",
            "Episode 7540, Reward: 235.65, Best: 279.81\n",
            "Episode 7550, Reward: 129.34, Best: 279.81\n",
            "Episode 7560, Reward: 124.77, Best: 279.81\n",
            "Episode 7570, Reward: 105.59, Best: 279.81\n",
            "Episode 7580, Reward: -5.64, Best: 279.81\n",
            "Episode 7590, Reward: 117.92, Best: 279.81\n",
            "Episode 7600, Reward: 136.15, Best: 279.81\n",
            "Episode 7610, Reward: 130.41, Best: 279.81\n",
            "Episode 7620, Reward: -30.68, Best: 279.81\n",
            "Episode 7630, Reward: -1.91, Best: 279.81\n",
            "Episode 7640, Reward: 134.57, Best: 279.81\n",
            "Episode 7650, Reward: 99.64, Best: 279.81\n",
            "Episode 7660, Reward: -21.96, Best: 279.81\n",
            "Episode 7670, Reward: 13.44, Best: 279.81\n",
            "Episode 7680, Reward: 222.88, Best: 279.81\n",
            "Episode 7690, Reward: 5.14, Best: 279.81\n",
            "Episode 7700, Reward: -0.94, Best: 279.81\n",
            "Episode 7710, Reward: 27.76, Best: 279.81\n",
            "Episode 7720, Reward: 135.65, Best: 279.81\n",
            "Episode 7730, Reward: 90.81, Best: 279.81\n",
            "Episode 7740, Reward: -94.28, Best: 279.81\n",
            "Episode 7750, Reward: 134.76, Best: 279.81\n",
            "Episode 7760, Reward: 229.36, Best: 279.81\n",
            "Episode 7770, Reward: 127.49, Best: 279.81\n",
            "Episode 7780, Reward: -5.88, Best: 279.81\n",
            "Episode 7790, Reward: -19.10, Best: 279.81\n",
            "Episode 7800, Reward: 137.97, Best: 279.81\n",
            "Episode 7810, Reward: 97.16, Best: 279.81\n",
            "Episode 7820, Reward: 89.80, Best: 279.81\n",
            "Episode 7830, Reward: 100.98, Best: 279.81\n",
            "Episode 7840, Reward: -24.96, Best: 279.81\n",
            "Episode 7850, Reward: -107.50, Best: 279.81\n",
            "Episode 7860, Reward: 1.36, Best: 279.81\n",
            "Episode 7870, Reward: 9.61, Best: 279.81\n",
            "Episode 7880, Reward: -12.39, Best: 279.81\n",
            "Episode 7890, Reward: 116.24, Best: 279.81\n",
            "Episode 7900, Reward: -24.04, Best: 279.81\n",
            "Episode 7910, Reward: -54.64, Best: 279.81\n",
            "Episode 7920, Reward: 28.43, Best: 279.81\n",
            "Episode 7930, Reward: 106.16, Best: 279.81\n",
            "Episode 7940, Reward: 81.17, Best: 279.81\n",
            "Episode 7950, Reward: 114.78, Best: 279.81\n",
            "Episode 7960, Reward: 56.62, Best: 279.81\n",
            "Episode 7970, Reward: 128.84, Best: 279.81\n",
            "Episode 7980, Reward: 86.56, Best: 279.81\n",
            "Episode 7990, Reward: 76.54, Best: 279.81\n",
            "Episode 8000, Reward: 119.31, Best: 279.81\n",
            "Episode 8010, Reward: 29.53, Best: 279.81\n",
            "Episode 8020, Reward: 13.04, Best: 279.81\n",
            "Episode 8030, Reward: -6.91, Best: 279.81\n",
            "Episode 8040, Reward: 23.50, Best: 279.81\n",
            "Episode 8050, Reward: 112.85, Best: 279.81\n",
            "Episode 8060, Reward: 85.76, Best: 279.81\n",
            "Episode 8070, Reward: 90.20, Best: 279.81\n",
            "Episode 8080, Reward: 117.54, Best: 279.81\n",
            "Episode 8090, Reward: 82.44, Best: 279.81\n",
            "Episode 8100, Reward: 93.56, Best: 279.81\n",
            "Episode 8110, Reward: -86.22, Best: 279.81\n",
            "Episode 8120, Reward: 106.65, Best: 279.81\n",
            "Episode 8130, Reward: 68.27, Best: 279.81\n",
            "Episode 8140, Reward: 76.17, Best: 279.81\n",
            "Episode 8150, Reward: 99.60, Best: 279.81\n",
            "Episode 8160, Reward: 91.42, Best: 279.81\n",
            "Episode 8170, Reward: 100.97, Best: 279.81\n",
            "Episode 8180, Reward: 102.42, Best: 279.81\n",
            "Episode 8190, Reward: 97.15, Best: 279.81\n",
            "Episode 8200, Reward: 111.14, Best: 279.81\n",
            "Episode 8210, Reward: 105.95, Best: 279.81\n",
            "Episode 8220, Reward: -15.60, Best: 279.81\n",
            "Episode 8230, Reward: -33.30, Best: 279.81\n",
            "Episode 8240, Reward: -63.79, Best: 279.81\n",
            "Episode 8250, Reward: 19.97, Best: 279.81\n",
            "Episode 8260, Reward: 40.96, Best: 279.81\n",
            "Episode 8270, Reward: 85.42, Best: 279.81\n",
            "Episode 8280, Reward: 92.77, Best: 279.81\n",
            "Episode 8290, Reward: 93.95, Best: 279.81\n",
            "Episode 8300, Reward: 255.34, Best: 279.81\n",
            "Episode 8310, Reward: 66.67, Best: 279.81\n",
            "Episode 8320, Reward: 116.88, Best: 279.81\n",
            "Episode 8330, Reward: 111.16, Best: 279.81\n",
            "Episode 8340, Reward: 66.24, Best: 279.81\n",
            "Episode 8350, Reward: 106.47, Best: 279.81\n",
            "Episode 8360, Reward: 97.06, Best: 279.81\n",
            "Episode 8370, Reward: 149.81, Best: 279.81\n",
            "Episode 8380, Reward: -2.50, Best: 279.81\n",
            "Episode 8390, Reward: 82.16, Best: 279.81\n",
            "Episode 8400, Reward: 127.77, Best: 279.81\n",
            "Episode 8410, Reward: -48.04, Best: 279.81\n",
            "Episode 8420, Reward: 119.88, Best: 279.81\n",
            "Episode 8430, Reward: 105.11, Best: 279.81\n",
            "Episode 8440, Reward: 147.29, Best: 279.81\n",
            "Episode 8450, Reward: 85.80, Best: 279.81\n",
            "Episode 8460, Reward: 99.66, Best: 279.81\n",
            "Episode 8470, Reward: 109.60, Best: 279.81\n",
            "Episode 8480, Reward: 72.45, Best: 279.81\n",
            "Episode 8490, Reward: 110.96, Best: 279.81\n",
            "Episode 8500, Reward: 31.70, Best: 279.81\n",
            "Episode 8510, Reward: 26.82, Best: 279.81\n",
            "Episode 8520, Reward: -1.46, Best: 279.81\n",
            "Episode 8530, Reward: 6.70, Best: 279.81\n",
            "Episode 8540, Reward: 77.85, Best: 279.81\n",
            "Episode 8550, Reward: 36.80, Best: 279.81\n",
            "Episode 8560, Reward: 104.76, Best: 279.81\n",
            "Episode 8570, Reward: 78.63, Best: 279.81\n",
            "Episode 8580, Reward: 88.65, Best: 279.81\n",
            "Episode 8590, Reward: 75.00, Best: 279.81\n",
            "Episode 8600, Reward: 110.74, Best: 279.81\n",
            "Episode 8610, Reward: 104.34, Best: 279.81\n",
            "Episode 8620, Reward: 105.02, Best: 279.81\n",
            "Episode 8630, Reward: 262.03, Best: 279.81\n",
            "Episode 8640, Reward: 115.28, Best: 279.81\n",
            "Episode 8650, Reward: 21.84, Best: 279.81\n",
            "Episode 8660, Reward: 105.85, Best: 279.81\n",
            "Episode 8670, Reward: 105.41, Best: 279.81\n",
            "Episode 8680, Reward: 102.04, Best: 279.81\n",
            "Episode 8690, Reward: 78.65, Best: 279.81\n",
            "Episode 8700, Reward: 116.12, Best: 279.81\n",
            "Episode 8710, Reward: 81.59, Best: 279.81\n",
            "Episode 8720, Reward: 1.98, Best: 279.81\n",
            "Episode 8730, Reward: 3.18, Best: 279.81\n",
            "Episode 8740, Reward: -52.42, Best: 279.81\n",
            "Episode 8750, Reward: -10.57, Best: 279.81\n",
            "Episode 8760, Reward: -35.06, Best: 279.81\n",
            "Episode 8770, Reward: -6.03, Best: 279.81\n",
            "Episode 8780, Reward: 59.57, Best: 279.81\n",
            "Episode 8790, Reward: 68.29, Best: 279.81\n",
            "Episode 8800, Reward: 82.00, Best: 279.81\n",
            "Episode 8810, Reward: 103.65, Best: 279.81\n",
            "Episode 8820, Reward: 117.01, Best: 279.81\n",
            "Episode 8830, Reward: 110.72, Best: 279.81\n",
            "Episode 8840, Reward: 103.09, Best: 279.81\n",
            "Episode 8850, Reward: 46.65, Best: 279.81\n",
            "Episode 8860, Reward: 128.26, Best: 279.81\n",
            "Episode 8870, Reward: 91.92, Best: 279.81\n",
            "Episode 8880, Reward: 111.26, Best: 279.81\n",
            "Episode 8890, Reward: 111.32, Best: 279.81\n",
            "Episode 8900, Reward: 110.03, Best: 279.81\n",
            "Episode 8910, Reward: 72.83, Best: 279.81\n",
            "Episode 8920, Reward: 95.15, Best: 279.81\n",
            "Episode 8930, Reward: 107.36, Best: 279.81\n",
            "Episode 8940, Reward: 116.35, Best: 279.81\n",
            "Episode 8950, Reward: 114.90, Best: 279.81\n",
            "Episode 8960, Reward: -13.26, Best: 279.81\n",
            "Episode 8970, Reward: 120.06, Best: 279.81\n",
            "Episode 8980, Reward: 25.46, Best: 279.81\n",
            "Episode 8990, Reward: 124.27, Best: 279.81\n",
            "Episode 9000, Reward: 116.23, Best: 279.81\n",
            "Episode 9010, Reward: 78.23, Best: 279.81\n",
            "Episode 9020, Reward: 27.95, Best: 279.81\n",
            "Episode 9030, Reward: 41.46, Best: 279.81\n",
            "Episode 9040, Reward: 98.63, Best: 279.81\n",
            "Episode 9050, Reward: 88.77, Best: 279.81\n",
            "Episode 9060, Reward: 109.21, Best: 279.81\n",
            "Episode 9070, Reward: 32.34, Best: 279.81\n",
            "Episode 9080, Reward: 113.12, Best: 279.81\n",
            "Episode 9090, Reward: 108.86, Best: 279.81\n",
            "Episode 9100, Reward: 115.32, Best: 279.81\n",
            "Episode 9110, Reward: 104.34, Best: 279.81\n",
            "Episode 9120, Reward: 64.50, Best: 279.81\n",
            "Episode 9130, Reward: 89.04, Best: 279.81\n",
            "Episode 9140, Reward: 81.95, Best: 279.81\n",
            "Episode 9150, Reward: 100.58, Best: 279.81\n",
            "Episode 9160, Reward: 105.96, Best: 279.81\n",
            "Episode 9170, Reward: 85.19, Best: 279.81\n",
            "Episode 9180, Reward: -59.70, Best: 279.81\n",
            "Episode 9190, Reward: -37.96, Best: 279.81\n",
            "Episode 9200, Reward: 93.17, Best: 279.81\n",
            "Episode 9210, Reward: 49.81, Best: 279.81\n",
            "Episode 9220, Reward: 94.57, Best: 279.81\n",
            "Episode 9230, Reward: 100.91, Best: 279.81\n",
            "Episode 9240, Reward: 115.04, Best: 279.81\n",
            "Episode 9250, Reward: 87.69, Best: 279.81\n",
            "Episode 9260, Reward: 248.68, Best: 279.81\n",
            "Episode 9270, Reward: 115.85, Best: 279.81\n",
            "Episode 9280, Reward: 96.12, Best: 279.81\n",
            "Episode 9290, Reward: 100.96, Best: 279.81\n",
            "Episode 9300, Reward: 112.02, Best: 279.81\n",
            "Episode 9310, Reward: -32.84, Best: 279.81\n",
            "Episode 9320, Reward: -33.12, Best: 279.81\n",
            "Episode 9330, Reward: -12.89, Best: 279.81\n",
            "Episode 9340, Reward: 114.91, Best: 279.81\n",
            "Episode 9350, Reward: 99.63, Best: 279.81\n",
            "Episode 9360, Reward: 126.75, Best: 279.81\n",
            "Episode 9370, Reward: 55.26, Best: 279.81\n",
            "Episode 9380, Reward: 98.95, Best: 279.81\n",
            "Episode 9390, Reward: 81.70, Best: 279.81\n",
            "Episode 9400, Reward: -28.68, Best: 279.81\n",
            "Episode 9410, Reward: 92.19, Best: 279.81\n",
            "Episode 9420, Reward: 106.48, Best: 279.81\n",
            "Episode 9430, Reward: 109.38, Best: 279.81\n",
            "Episode 9440, Reward: 167.70, Best: 279.81\n",
            "Episode 9450, Reward: 82.80, Best: 279.81\n",
            "Episode 9460, Reward: 98.27, Best: 279.81\n",
            "Episode 9470, Reward: 89.01, Best: 279.81\n",
            "Episode 9480, Reward: 89.93, Best: 279.81\n",
            "Episode 9490, Reward: 103.16, Best: 279.81\n",
            "Episode 9500, Reward: 90.12, Best: 279.81\n",
            "Episode 9510, Reward: 85.54, Best: 279.81\n",
            "Episode 9520, Reward: 88.69, Best: 279.81\n",
            "Episode 9530, Reward: 33.87, Best: 279.81\n",
            "Episode 9540, Reward: 31.87, Best: 279.81\n",
            "Episode 9550, Reward: 23.64, Best: 279.81\n",
            "Episode 9560, Reward: 43.07, Best: 279.81\n",
            "Episode 9570, Reward: 109.69, Best: 279.81\n",
            "Episode 9580, Reward: 39.80, Best: 279.81\n",
            "Episode 9590, Reward: 128.15, Best: 279.81\n",
            "Episode 9600, Reward: 92.25, Best: 279.81\n",
            "Episode 9610, Reward: 22.83, Best: 279.81\n",
            "Episode 9620, Reward: 107.75, Best: 279.81\n",
            "Episode 9630, Reward: 6.68, Best: 279.81\n",
            "Episode 9640, Reward: 79.42, Best: 279.81\n",
            "Episode 9650, Reward: -43.92, Best: 279.81\n",
            "Episode 9660, Reward: 55.24, Best: 279.81\n",
            "Episode 9670, Reward: 67.55, Best: 279.81\n",
            "Episode 9680, Reward: 107.75, Best: 279.81\n",
            "Episode 9690, Reward: 84.14, Best: 279.81\n",
            "Episode 9700, Reward: 113.62, Best: 279.81\n",
            "Episode 9710, Reward: 109.16, Best: 279.81\n",
            "Episode 9720, Reward: -59.29, Best: 279.81\n",
            "Episode 9730, Reward: 30.31, Best: 279.81\n",
            "Episode 9740, Reward: 131.50, Best: 279.81\n",
            "Episode 9750, Reward: -87.95, Best: 279.81\n",
            "Episode 9760, Reward: 114.41, Best: 279.81\n",
            "Episode 9770, Reward: 230.22, Best: 279.81\n",
            "Episode 9780, Reward: 95.97, Best: 279.81\n",
            "Episode 9790, Reward: 99.18, Best: 279.81\n",
            "Episode 9800, Reward: 104.25, Best: 279.81\n",
            "Episode 9810, Reward: 108.99, Best: 279.81\n",
            "Episode 9820, Reward: 113.94, Best: 284.25\n",
            "Episode 9830, Reward: 78.86, Best: 284.25\n",
            "Episode 9840, Reward: 94.67, Best: 284.25\n",
            "Episode 9850, Reward: 107.08, Best: 284.25\n",
            "Episode 9860, Reward: 104.69, Best: 284.25\n",
            "Episode 9870, Reward: 117.74, Best: 284.25\n",
            "Episode 9880, Reward: 113.68, Best: 284.25\n",
            "Episode 9890, Reward: 99.58, Best: 284.25\n",
            "Episode 9900, Reward: 76.52, Best: 284.25\n",
            "Episode 9910, Reward: 120.30, Best: 284.25\n",
            "Episode 9920, Reward: 96.60, Best: 284.25\n",
            "Episode 9930, Reward: 78.39, Best: 284.25\n",
            "Episode 9940, Reward: 237.52, Best: 284.25\n",
            "Episode 9950, Reward: 69.59, Best: 284.25\n",
            "Episode 9960, Reward: 126.38, Best: 284.25\n",
            "Episode 9970, Reward: 102.66, Best: 284.25\n",
            "Episode 9980, Reward: 113.93, Best: 284.25\n",
            "Episode 9990, Reward: -46.10, Best: 284.25\n",
            "Episode 10000, Reward: -47.85, Best: 284.25\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>▂▂▂▁▄▃▂▃▂▅▂▇▂▃▃▄▆▄▃▄▃▃▂▃▃▇▂▆▂▄▄█▂▅▁▃▂▇▄▂</td></tr><tr><td>critic_loss</td><td>▄▃▅▃▅▇▇▇▇█▅▆▄▅▄▂▁▇▅▂▁▆▆▂▂▁▅▁█▅▃▁▃▆▁▁▄▄▇▄</td></tr><tr><td>entropy</td><td>▆█▇▆▇▄▄▇▆▇▅▅▅▇▇▆▇▅▇▄▆▄▃▃▄▅▃▃▅▃▇▁▂▅▂▄▃▂▄▂</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>reward</td><td>▅▆▅▅▅▅▆█▇▁▆▅▆▆▃▆▆▆▇█▃▇▇▇█▇█▇█▇▃▆▇▇▇▇▇▇▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>-0.03034</td></tr><tr><td>critic_loss</td><td>0.36869</td></tr><tr><td>entropy</td><td>1.00773</td></tr><tr><td>episode</td><td>9999</td></tr><tr><td>reward</td><td>-47.8511</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zany-water-3</strong> at: <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander/runs/yqycu6qy' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander/runs/yqycu6qy</a><br> View project at: <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLander</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250216_111230-yqycu6qy/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training completed! Training statistics:\n",
            "Mean reward: -29.41\n",
            "Max reward: 284.25\n",
            "Mean episode length: 103.88\n",
            "\n",
            "Starting model testing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-40e75fc282cc>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'lunar_lander_ppo.pth'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3cf13e4d0323>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting model testing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtest_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTesting completed! Test statistics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-40e75fc282cc>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model_path, num_episodes)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0maction_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStorageBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-40e75fc282cc>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lunar_lander_ppo.pth'"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # this is for training and testing the model\n",
        "    # note that this environment is bit complex for that it will take a lot more training if you have gpu access that can speed up the training\n",
        "    num_episodes = 10000  # You can change this number\n",
        "    print(f\"Starting training for {num_episodes} episodes...\")\n",
        "    agent, train_storage = train_ppo(num_episodes=num_episodes)\n",
        "\n",
        "    print(\"\\nTraining completed! Training statistics:\")\n",
        "\n",
        "    train_stats = train_storage.get_statistics()\n",
        "    print(f\"Mean reward: {train_stats['mean_reward']:.2f}\")\n",
        "    print(f\"Max reward: {train_stats['max_reward']:.2f}\")\n",
        "    print(f\"Mean episode length: {train_stats['mean_length']:.2f}\")\n",
        "\n",
        "    model_path = \"ppo_lunar_lander.pth\"\n",
        "\n",
        "    print(\"\\nStarting model testing...\")\n",
        "    test_storage = test_model(model_path, num_episodes=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtuyD7q2slKn",
        "outputId": "60bba164-6248-4b62-cf5d-ed15ebd52ccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting model testing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3880\\1801936496.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Episode 1, Total Reward: 199.12\n",
            "Test Episode 2, Total Reward: -15.60\n",
            "Test Episode 3, Total Reward: -75.68\n",
            "Test Episode 4, Total Reward: 190.00\n",
            "Test Episode 5, Total Reward: 194.63\n"
          ]
        }
      ],
      "source": [
        "# testing the model with the saved params\n",
        "\n",
        "model_path = \"ppo_lunar_lander.pth\"\n",
        "\n",
        "print(\"\\nStarting model testing...\")\n",
        "test_storage = test_model(model_path, num_episodes=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFF5PQjjX-uZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
