{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JDy7dXK6KdH",
        "outputId": "93dce95f-7731-43bf-c394-ad3b831241c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.11/dist-packages (2.3.10)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.11/dist-packages (2.3.5)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d box2d-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H78N2vBd6Lo8",
        "outputId": "f3c38923-f8fd-4790-f7bb-acf092efbc00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351226 sha256=1bfc5b5ce94a0104732ba94c74ced1f747a916c57d975efd1d150c4af923fd61\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "  Attempting uninstall: box2d-py\n",
            "    Found existing installation: box2d-py 2.3.8\n",
            "    Uninstalling box2d-py-2.3.8:\n",
            "      Successfully uninstalled box2d-py-2.3.8\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K63eots4Y9EA",
        "outputId": "a1f20f7e-39e3-435a-97a9-898cb83cd3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Heu3h2k46aHo"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# this class define the storage buffer of the environment\n",
        "\n",
        "class StorageBuffer:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.episode_rewards = []  # Store rewards for each episode\n",
        "        self.episode_lengths = []  # Store lengths for each episode\n",
        "\n",
        "    def reset(self):\n",
        "        # Current episode storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.next_state = None\n",
        "        self.current_reward = 0\n",
        "\n",
        "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "        self.current_reward += reward\n",
        "\n",
        "        if done:\n",
        "            self.next_state = next_state\n",
        "            self.episode_rewards.append(self.current_reward)\n",
        "            self.episode_lengths.append(len(self.rewards))\n",
        "\n",
        "    def get_episode_data(self):\n",
        "        return {\n",
        "            'states': torch.FloatTensor(np.array(self.states)),\n",
        "            'actions': torch.tensor(self.actions),\n",
        "            'rewards': torch.tensor(self.rewards),\n",
        "            'log_probs': torch.tensor(self.log_probs),\n",
        "            'dones': torch.tensor(self.dones),\n",
        "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
        "        }\n",
        "\n",
        "    def get_statistics(self):\n",
        "\n",
        "        if not self.episode_rewards:\n",
        "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
        "\n",
        "        return {\n",
        "            \"mean_reward\": np.mean(self.episode_rewards),\n",
        "            \"max_reward\": np.max(self.episode_rewards),\n",
        "            \"min_reward\": np.min(self.episode_rewards),\n",
        "            \"mean_length\": np.mean(self.episode_lengths),\n",
        "            \"current_reward\": self.current_reward,\n",
        "            \"current_length\": len(self.rewards)\n",
        "        }\n",
        "\n",
        "#the policy and value networks\n",
        "# i used small model since it simple game but you can make it bigger if you want\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean =   self.fc(x)\n",
        "        std = self.log_std.exp()\n",
        "        return mean,std\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "YY39iEFCZz3M",
        "outputId": "8d0f3b8d-965f-4b92-c9bd-c2a5894ded83"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamedrxo4\u001b[0m (\u001b[33mmohamedrxo4-netflix\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "# wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "NRV8o62Fhr_B",
        "outputId": "fd611bdb-85e6-4978-b636-f4cf16f350b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>▅▆▇█▇▄▅▄▆▇▇▆▆▆▅▅▃▃▃▅▅▅▆▁▄▄▂█▅▆▇▅▇▅▇▅▅▃▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>-47.98077</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">tough-cloud-2</strong> at: <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous/runs/d067sg9t' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous/runs/d067sg9t</a><br> View project at: <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250222_181358-d067sg9t/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "yx2Eu8oRZcHl",
        "outputId": "c395ac50-486b-42e8-da3a-7d776f310baa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250222_183059-vvc3x9wo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous/runs/vvc3x9wo' target=\"_blank\">solar-spaceship-3</a></strong> to <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous/runs/vvc3x9wo' target=\"_blank\">https://wandb.ai/mohamedrxo4-netflix/PPO-LunarLande_continous/runs/vvc3x9wo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    project=\"PPO-LunarLande_continous\",\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zHTcUCwj6zkL"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Normal\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=10):\n",
        "\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.old_policy = PolicyNetwork(state_dim, action_dim)\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "        self.value_net = ValueNetwork(state_dim)\n",
        "\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.k_epochs = k_epochs\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            mean, std = self.old_policy(state)\n",
        "\n",
        "        dist = Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        action_log_prob = dist.log_prob(action).sum(dim=-1)  # Sum for multi-dimensional actions\n",
        "        return action.numpy()[0], action_log_prob.numpy()[0]\n",
        "\n",
        "    def compute_advantages(self, rewards, values, next_value, dones, lambda_gae=0.95):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
        "            gae = delta + self.gamma * lambda_gae * (1 - float(dones[t])) * gae\n",
        "            advantages.insert(0, gae)\n",
        "            next_value = values[t]\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float32)\n",
        "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # def save_model(self, filepath):\n",
        "    #     torch.save({\n",
        "    #         'policy_state_dict': self.policy.state_dict(),\n",
        "    #         'value_state_dict': self.value_net.state_dict()\n",
        "    #     }, filepath)\n",
        "\n",
        "    # def load_model(self, filepath):\n",
        "    #     checkpoint = torch.load(filepath)\n",
        "    #     self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "    #     self.value_net.load_state_dict(checkpoint['value_state_dict'])\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'old_policy_state_dict': self.old_policy.state_dict(),\n",
        "            'value_state_dict': self.value_net.state_dict(),\n",
        "            'policy_optimizer_state_dict': self.policy_optimizer.state_dict(),\n",
        "            'value_optimizer_state_dict': self.value_optimizer.state_dict()\n",
        "        }, filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.old_policy.load_state_dict(checkpoint['old_policy_state_dict'])\n",
        "        self.value_net.load_state_dict(checkpoint['value_state_dict'])\n",
        "        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
        "        self.value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])\n",
        "\n",
        "        # Ensure all models are in eval mode for testing\n",
        "        self.policy.eval()\n",
        "        self.old_policy.eval()\n",
        "        self.value_net.eval()\n",
        "\n",
        "    def update(self, buffer, batch_size=64):\n",
        "        data = buffer.get_episode_data()\n",
        "        states = torch.tensor(data['states'], dtype=torch.float32)\n",
        "        actions = torch.tensor(data['actions'], dtype=torch.float32)\n",
        "        log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n",
        "        rewards = data['rewards']\n",
        "        dones = data['dones']\n",
        "\n",
        "        values = self.value_net(states).squeeze().detach()\n",
        "        next_value = self.value_net(torch.tensor(data['next_state'], dtype=torch.float32)).item() if data['next_state'] else 0.0\n",
        "        advantages = self.compute_advantages(rewards, values, next_value, dones)\n",
        "        targets = advantages + values\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for _ in range(self.k_epochs):\n",
        "            for batch_states, batch_actions, batch_log_probs_old, batch_advantages, batch_targets in dataloader:\n",
        "\n",
        "                mean, std = self.policy(batch_states)\n",
        "                values_pred  =  self.value_net(batch_states)\n",
        "                dist = Normal(mean, std)\n",
        "                log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
        "\n",
        "                ratios = torch.exp(log_probs - batch_log_probs_old)\n",
        "                surr1 = ratios * batch_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
        "                loss_actor = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                loss_critic = self.mse_loss(values_pred.squeeze(), batch_targets)\n",
        "\n",
        "                self.policy_optimizer.zero_grad()\n",
        "                loss_actor.backward(retain_graph=True)\n",
        "                self.policy_optimizer.step()\n",
        "\n",
        "                self.value_optimizer.zero_grad()\n",
        "                loss_critic.backward()\n",
        "                self.value_optimizer.step()\n",
        "\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xTkbnhH073ZF"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "def train_ppo(env_name=\"LunarLanderContinuous-v2\", num_episodes=1000, max_timesteps=2000):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    buffer = StorageBuffer()\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    mean_raward = []\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        buffer.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(max_timesteps):\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "\n",
        "            buffer.add_step(state, action, reward, log_prob, done, next_state if not done else None)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        mean_raward.append(episode_reward)\n",
        "        wandb.log({\"episode_reward\":episode_reward})\n",
        "\n",
        "        agent.update(buffer)\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            agent.save_model(\"lunar_lander_continuous2.pth\")\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}, mean raward: {statistics.mean(mean_raward)}, Best: {best_reward:.2f}\")\n",
        "            mean_raward = []\n",
        "\n",
        "    env.close()\n",
        "    return agent\n",
        "\n",
        "\n",
        "def test_model(model_path, env_name=\"LunarLanderContinuous-v2\", num_episodes=10):\n",
        "    env = gym.make(env_name, render_mode=\"human\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    agent.load_model(model_path)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Test Episode {episode + 1}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOwk5RyM8FJb",
        "outputId": "3baf4b84-7d65-43dc-ba93-22054bf9abb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-40-2ae0e21e481a>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(data['states'], dtype=torch.float32)\n",
            "<ipython-input-40-2ae0e21e481a>:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actions = torch.tensor(data['actions'], dtype=torch.float32)\n",
            "<ipython-input-40-2ae0e21e481a>:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10, mean raward: -316.90505711824824, Best: -142.09\n",
            "Episode 20, mean raward: -202.02425145012722, Best: -72.09\n",
            "Episode 30, mean raward: -222.1707567527065, Best: -66.79\n",
            "Episode 40, mean raward: -243.20076068485295, Best: -48.63\n",
            "Episode 50, mean raward: -173.2686520546552, Best: -13.43\n",
            "Episode 60, mean raward: -212.9633876995138, Best: -13.43\n",
            "Episode 70, mean raward: -157.76324666839693, Best: -13.43\n",
            "Episode 80, mean raward: -91.78177700931286, Best: 27.85\n",
            "Episode 90, mean raward: -302.2741856110952, Best: 27.85\n",
            "Episode 100, mean raward: -358.05486627674827, Best: 27.85\n",
            "Episode 110, mean raward: -400.5408956523221, Best: 27.85\n",
            "Episode 120, mean raward: -133.89960364438488, Best: 28.66\n",
            "Episode 130, mean raward: -233.49998975943487, Best: 28.66\n",
            "Episode 140, mean raward: -147.71557081395295, Best: 30.93\n",
            "Episode 150, mean raward: -66.35643860595002, Best: 30.93\n",
            "Episode 160, mean raward: -124.29937165323702, Best: 30.93\n",
            "Episode 170, mean raward: -133.16533601493427, Best: 30.93\n",
            "Episode 180, mean raward: -122.31668508880456, Best: 30.93\n",
            "Episode 190, mean raward: -88.36273009840215, Best: 30.93\n",
            "Episode 200, mean raward: -78.8202249043378, Best: 30.93\n",
            "Episode 210, mean raward: -92.650365024739, Best: 30.93\n",
            "Episode 220, mean raward: -94.76114563231319, Best: 30.93\n",
            "Episode 230, mean raward: -49.548794129249195, Best: 50.10\n",
            "Episode 240, mean raward: -74.77039872659306, Best: 50.10\n",
            "Episode 250, mean raward: -79.69334804381437, Best: 50.10\n",
            "Episode 260, mean raward: -67.97994005323179, Best: 50.10\n",
            "Episode 270, mean raward: -60.71228177724965, Best: 50.10\n",
            "Episode 280, mean raward: -45.6827647143883, Best: 50.10\n",
            "Episode 290, mean raward: -25.932594340246137, Best: 50.10\n",
            "Episode 300, mean raward: -86.8001977865108, Best: 50.10\n",
            "Episode 310, mean raward: -63.3840895256392, Best: 50.10\n",
            "Episode 320, mean raward: -72.87166726996044, Best: 50.10\n",
            "Episode 330, mean raward: -96.89779488801892, Best: 50.10\n",
            "Episode 340, mean raward: -66.65969447031061, Best: 50.10\n",
            "Episode 350, mean raward: -80.76627442917211, Best: 50.10\n",
            "Episode 360, mean raward: -62.0657624434735, Best: 50.10\n",
            "Episode 370, mean raward: -81.19296155560352, Best: 50.10\n",
            "Episode 380, mean raward: -43.17532455306746, Best: 91.25\n",
            "Episode 390, mean raward: -64.15004035114703, Best: 91.25\n",
            "Episode 400, mean raward: -42.765208439139215, Best: 91.25\n",
            "Episode 410, mean raward: -31.303777919622295, Best: 91.25\n",
            "Episode 420, mean raward: 2.21698640020517, Best: 167.59\n",
            "Episode 430, mean raward: 7.61744981771733, Best: 167.59\n",
            "Episode 440, mean raward: -49.95657961960894, Best: 167.59\n",
            "Episode 450, mean raward: -55.38590368076346, Best: 167.59\n",
            "Episode 460, mean raward: -26.655255188310125, Best: 167.59\n",
            "Episode 470, mean raward: -46.34844548150778, Best: 167.59\n",
            "Episode 480, mean raward: -75.58213842830513, Best: 167.59\n",
            "Episode 490, mean raward: -48.67012842291741, Best: 167.59\n",
            "Episode 500, mean raward: -109.7272520180895, Best: 167.59\n",
            "Episode 510, mean raward: -111.62426601192522, Best: 167.59\n",
            "Episode 520, mean raward: -95.01948044692232, Best: 167.59\n",
            "Episode 530, mean raward: -73.60043143349861, Best: 167.59\n",
            "Episode 540, mean raward: -65.22473173867063, Best: 167.59\n",
            "Episode 550, mean raward: -72.72743299578072, Best: 167.59\n",
            "Episode 560, mean raward: -77.24336999617394, Best: 167.59\n",
            "Episode 570, mean raward: -73.15377094209762, Best: 167.59\n",
            "Episode 580, mean raward: -62.89248992484207, Best: 167.59\n",
            "Episode 590, mean raward: -64.4828660398564, Best: 167.59\n",
            "Episode 600, mean raward: -60.82034750479923, Best: 167.59\n",
            "Episode 610, mean raward: -65.18882573056909, Best: 167.59\n",
            "Episode 620, mean raward: -71.00449686687398, Best: 167.59\n",
            "Episode 630, mean raward: -32.52108181963789, Best: 167.59\n",
            "Episode 640, mean raward: -24.38541071394976, Best: 167.59\n",
            "Episode 650, mean raward: -35.59269850855398, Best: 167.59\n",
            "Episode 660, mean raward: -33.75224016747978, Best: 167.59\n",
            "Episode 670, mean raward: -89.76894716849127, Best: 167.59\n",
            "Episode 680, mean raward: -33.700694043561676, Best: 167.59\n",
            "Episode 690, mean raward: -68.55260208200512, Best: 167.59\n",
            "Episode 700, mean raward: -37.80275567787543, Best: 167.59\n",
            "Episode 710, mean raward: -76.68809697979341, Best: 167.59\n",
            "Episode 720, mean raward: -50.80781685942202, Best: 167.59\n",
            "Episode 730, mean raward: -71.59184705923958, Best: 167.59\n",
            "Episode 740, mean raward: -58.848767504404776, Best: 167.59\n",
            "Episode 750, mean raward: -33.56917590438365, Best: 167.59\n",
            "Episode 760, mean raward: -20.723755918278492, Best: 182.59\n",
            "Episode 770, mean raward: -26.823065074860274, Best: 182.59\n",
            "Episode 780, mean raward: -66.69200252641475, Best: 182.59\n",
            "Episode 790, mean raward: -62.93746292792477, Best: 182.59\n",
            "Episode 800, mean raward: -65.13180659630791, Best: 182.59\n",
            "Episode 810, mean raward: -95.31490516197366, Best: 182.59\n",
            "Episode 820, mean raward: -60.94440389911653, Best: 182.59\n",
            "Episode 830, mean raward: -39.078769826104896, Best: 182.59\n",
            "Episode 840, mean raward: 21.58588492177503, Best: 182.59\n",
            "Episode 850, mean raward: -9.895809531132771, Best: 182.59\n",
            "Episode 860, mean raward: -97.86290691253572, Best: 182.59\n",
            "Episode 870, mean raward: -41.565448948145466, Best: 182.59\n",
            "Episode 880, mean raward: -84.36154243395566, Best: 182.59\n",
            "Episode 890, mean raward: -60.448306910206085, Best: 182.59\n",
            "Episode 900, mean raward: -58.56157115759801, Best: 182.59\n",
            "Episode 910, mean raward: -75.91994280653728, Best: 182.59\n",
            "Episode 920, mean raward: -105.37410060503419, Best: 182.59\n",
            "Episode 930, mean raward: -73.60614323547061, Best: 182.59\n",
            "Episode 940, mean raward: -89.07709954031756, Best: 182.59\n",
            "Episode 950, mean raward: -93.71739495538421, Best: 182.59\n",
            "Episode 960, mean raward: -104.80418515734455, Best: 182.59\n",
            "Episode 970, mean raward: -64.58033249975716, Best: 182.59\n",
            "Episode 980, mean raward: -98.13001855808265, Best: 182.59\n",
            "Episode 990, mean raward: -48.9730157256627, Best: 182.59\n",
            "Episode 1000, mean raward: -82.0730094923212, Best: 182.59\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.PPOAgent at 0x7f1348b75790>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ppo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0PwtdbL8v0o",
        "outputId": "35022902-40d2-4264-ffd5-f879b6f863a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6428\\839437471.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Episode 1, Total Reward: 77.36\n",
            "Test Episode 2, Total Reward: -68.43\n",
            "Test Episode 3, Total Reward: -96.00\n",
            "Test Episode 4, Total Reward: -88.84\n",
            "Test Episode 5, Total Reward: -92.62\n",
            "Test Episode 6, Total Reward: -203.78\n",
            "Test Episode 7, Total Reward: -67.98\n",
            "Test Episode 8, Total Reward: 72.53\n",
            "Test Episode 9, Total Reward: -97.86\n",
            "Test Episode 10, Total Reward: -91.42\n"
          ]
        }
      ],
      "source": [
        "model_path  = \"lunar_lander_continuous2.pth\"\n",
        "test_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H_JuquMWwHd",
        "outputId": "68e1d4fb-2791-4f4c-87c1-9086693ff85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
