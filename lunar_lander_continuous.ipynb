{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HInEl6DTyyyo",
        "outputId": "930707f3-6430-47b9-b0f5-6cd5cd5a6506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,407 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2351172 sha256=64deffa9166402136732da785a59c9c861e9f300fcbbe33ae4825a7e4f8efc4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-py-2.3.8\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig\n",
        "!pip install box2d box2d-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6kx-1Nxy0h6",
        "outputId": "a25cf3b8-9bcb-40e2-f83d-e65e61554568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351229 sha256=37450f9bef3ae7173773be85d9b2d0397b5aa2420c3cf59f8d348c7cc834e9cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "  Attempting uninstall: box2d-py\n",
            "    Found existing installation: box2d-py 2.3.8\n",
            "    Uninstalling box2d-py-2.3.8:\n",
            "      Successfully uninstalled box2d-py-2.3.8\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u_Ch7Qj1y2gi"
      },
      "outputs": [],
      "source": [
        "#importing dependencies\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# this class define the storage buffer of the environment\n",
        "\n",
        "class StorageBuffer:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.episode_rewards = []  # Store rewards for each episode\n",
        "        self.episode_lengths = []  # Store lengths for each episode\n",
        "\n",
        "    def reset(self):\n",
        "        # Current episode storage\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.log_probs = []\n",
        "        self.dones = []\n",
        "        self.next_state = None\n",
        "        self.current_reward = 0\n",
        "\n",
        "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.dones.append(done)\n",
        "        self.current_reward += reward\n",
        "\n",
        "        if done:\n",
        "            self.next_state = next_state\n",
        "            self.episode_rewards.append(self.current_reward)\n",
        "            self.episode_lengths.append(len(self.rewards))\n",
        "\n",
        "    def get_episode_data(self):\n",
        "        return {\n",
        "            'states': torch.FloatTensor(np.array(self.states)),\n",
        "            'actions': torch.tensor(self.actions),\n",
        "            'rewards': torch.tensor(self.rewards),\n",
        "            'log_probs': torch.tensor(self.log_probs),\n",
        "            'dones': torch.tensor(self.dones),\n",
        "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
        "        }\n",
        "\n",
        "    def get_statistics(self):\n",
        "\n",
        "        if not self.episode_rewards:\n",
        "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
        "\n",
        "        return {\n",
        "            \"mean_reward\": np.mean(self.episode_rewards),\n",
        "            \"max_reward\": np.max(self.episode_rewards),\n",
        "            \"min_reward\": np.min(self.episode_rewards),\n",
        "            \"mean_length\": np.mean(self.episode_lengths),\n",
        "            \"current_reward\": self.current_reward,\n",
        "            \"current_length\": len(self.rewards)\n",
        "        }\n",
        "\n",
        "#the policy and value networks\n",
        "# i used small model since it simple game but you can make it bigger if you want\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VWktv73cy7hq"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # Actor (Mean and Std Dev for continuous actions)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "             nn.Tanh(),\n",
        "            nn.Linear(64, action_dim)  # Outputs mean of action\n",
        "        )\n",
        "\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Learnable log standard deviation\n",
        "\n",
        "        # Critic (State Value)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # Single value output\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        mean = self.actor(state)\n",
        "        std = self.log_std.exp()  # Convert log_std to std\n",
        "        value = self.critic(state)\n",
        "        return mean, std, value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t6emBEYPy9ew"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Normal\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=10):\n",
        "        self.policy = ActorCritic(state_dim, action_dim)\n",
        "        self.old_policy = ActorCritic(state_dim, action_dim)\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.k_epochs = k_epochs\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            mean, std, _ = self.old_policy(state)\n",
        "\n",
        "        dist = Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        action_log_prob = dist.log_prob(action).sum(dim=-1)  # Sum for multi-dimensional actions\n",
        "        return action.numpy()[0], action_log_prob.numpy()[0]\n",
        "\n",
        "    def compute_advantages(self, rewards, values, next_value, dones, lambda_gae=0.95):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
        "            gae = delta + self.gamma * lambda_gae * (1 - float(dones[t])) * gae\n",
        "            advantages.insert(0, gae)\n",
        "            next_value = values[t]\n",
        "        advantages = torch.tensor(advantages, dtype=torch.float32)\n",
        "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        torch.save({\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict()\n",
        "        }, filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "\n",
        "    def update(self, buffer, batch_size=64):\n",
        "        data = buffer.get_episode_data()\n",
        "        states = torch.tensor(data['states'], dtype=torch.float32)\n",
        "        actions = torch.tensor(data['actions'], dtype=torch.float32)\n",
        "        log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n",
        "        rewards = data['rewards']\n",
        "        dones = data['dones']\n",
        "\n",
        "        values = self.policy.critic(states).squeeze().detach()\n",
        "        next_value = self.policy.critic(torch.tensor(data['next_state'], dtype=torch.float32)).item() if data['next_state'] else 0.0\n",
        "        advantages = self.compute_advantages(rewards, values, next_value, dones)\n",
        "        targets = advantages + values\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(states, actions, log_probs_old, advantages, targets)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for _ in range(self.k_epochs):\n",
        "            for batch_states, batch_actions, batch_log_probs_old, batch_advantages, batch_targets in dataloader:\n",
        "\n",
        "                mean, std, values_pred = self.policy(batch_states)\n",
        "                dist = Normal(mean, std)\n",
        "                log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
        "\n",
        "                ratios = torch.exp(log_probs - batch_log_probs_old)\n",
        "                surr1 = ratios * batch_advantages\n",
        "                surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
        "                loss_actor = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                loss_critic = self.mse_loss(values_pred.squeeze(), batch_targets)\n",
        "\n",
        "                loss = loss_actor + 0.5 * loss_critic\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sF0aDJ-3y9w-"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "def train_ppo(env_name=\"LunarLanderContinuous-v2\", num_episodes=1000, max_timesteps=2000):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    buffer = StorageBuffer()\n",
        "    best_reward = -float('inf')\n",
        "\n",
        "    mean_raward = []\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        buffer.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(max_timesteps):\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "\n",
        "            buffer.add_step(state, action, reward, log_prob, done, next_state if not done else None)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "        mean_raward.append(episode_reward)\n",
        "\n",
        "        agent.update(buffer)\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            agent.save_model(\"lunar_lander_continuous.pth\")\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}, mean raward: {statistics.mean(mean_raward)}, Best: {best_reward:.2f}\")\n",
        "            mean_raward = []\n",
        "\n",
        "    env.close()\n",
        "    return agent\n",
        "\n",
        "\n",
        "def test_model(model_path, env_name=\"LunarLanderContinuous-v2\", num_episodes=10):\n",
        "    env = gym.make(env_name, render_mode=\"human\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    agent = PPOAgent(state_dim, action_dim)\n",
        "    agent.load_model(model_path)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Test Episode {episode + 1}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n_8wyp2z5qO",
        "outputId": "7cbab2d8-ea32-48ac-ee4e-1f0f3fb228c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-19536a4bc822>:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(data['states'], dtype=torch.float32)\n",
            "<ipython-input-23-19536a4bc822>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  actions = torch.tensor(data['actions'], dtype=torch.float32)\n",
            "<ipython-input-23-19536a4bc822>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  log_probs_old = torch.tensor(data['log_probs'], dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10, mean raward: -183.58017089277988, Best: -58.78\n",
            "Episode 20, mean raward: -351.9370443369463, Best: -58.78\n",
            "Episode 30, mean raward: -177.95139493568928, Best: -56.87\n",
            "Episode 40, mean raward: -176.55515385063444, Best: -23.65\n",
            "Episode 50, mean raward: -135.39923742339832, Best: 43.18\n",
            "Episode 60, mean raward: -406.1119892217437, Best: 43.18\n",
            "Episode 70, mean raward: -525.9344330712152, Best: 43.18\n",
            "Episode 80, mean raward: -597.6216824474468, Best: 43.18\n",
            "Episode 90, mean raward: -83.53261962996963, Best: 43.18\n",
            "Episode 100, mean raward: -67.01166763324015, Best: 43.18\n",
            "Episode 110, mean raward: -9.84739522050436, Best: 46.97\n",
            "Episode 120, mean raward: -18.632398736876596, Best: 84.00\n",
            "Episode 130, mean raward: 70.75937320406683, Best: 167.41\n",
            "Episode 140, mean raward: 94.64526832527706, Best: 175.06\n",
            "Episode 150, mean raward: 142.01454435530368, Best: 196.29\n",
            "Episode 160, mean raward: 7.780285618314126, Best: 196.29\n",
            "Episode 170, mean raward: 39.39317996904693, Best: 196.29\n",
            "Episode 180, mean raward: 53.8630350974497, Best: 196.29\n",
            "Episode 190, mean raward: 47.878025033924295, Best: 196.29\n",
            "Episode 200, mean raward: 23.38405698852891, Best: 196.29\n",
            "Episode 210, mean raward: 20.46542625151067, Best: 196.29\n",
            "Episode 220, mean raward: 45.07290333716302, Best: 196.29\n",
            "Episode 230, mean raward: 110.82592192664035, Best: 196.29\n",
            "Episode 240, mean raward: 89.40710602322473, Best: 214.72\n",
            "Episode 250, mean raward: 108.03816512806084, Best: 214.72\n",
            "Episode 260, mean raward: 35.353156001934416, Best: 214.72\n",
            "Episode 270, mean raward: 113.70320298216718, Best: 214.72\n",
            "Episode 280, mean raward: 45.76125749862176, Best: 214.72\n",
            "Episode 290, mean raward: 80.57719359218848, Best: 214.72\n",
            "Episode 300, mean raward: 112.01991616376964, Best: 214.72\n",
            "Episode 310, mean raward: 59.48944547245187, Best: 214.72\n",
            "Episode 320, mean raward: 93.65007816894908, Best: 214.72\n",
            "Episode 330, mean raward: -16.335812826365913, Best: 214.72\n",
            "Episode 340, mean raward: 3.712997059540366, Best: 229.31\n",
            "Episode 350, mean raward: 66.97595971541398, Best: 229.31\n",
            "Episode 360, mean raward: 59.941843744587, Best: 229.31\n",
            "Episode 370, mean raward: 89.69145677240078, Best: 240.73\n",
            "Episode 380, mean raward: 83.83199284754858, Best: 240.73\n",
            "Episode 390, mean raward: 83.14650428288978, Best: 240.73\n",
            "Episode 400, mean raward: 32.66510444232354, Best: 240.73\n",
            "Episode 410, mean raward: 119.85600974559559, Best: 240.73\n",
            "Episode 420, mean raward: 118.46521956656825, Best: 240.73\n",
            "Episode 430, mean raward: 127.66580190197922, Best: 240.73\n",
            "Episode 440, mean raward: 75.3343036972797, Best: 240.73\n",
            "Episode 450, mean raward: 122.97014237351948, Best: 278.31\n",
            "Episode 460, mean raward: 107.2209076569162, Best: 278.31\n",
            "Episode 470, mean raward: 67.48412538188461, Best: 278.31\n",
            "Episode 480, mean raward: 83.57691360770546, Best: 278.31\n",
            "Episode 490, mean raward: 93.76415154737309, Best: 278.31\n",
            "Episode 500, mean raward: 115.49905250623132, Best: 278.31\n",
            "Episode 510, mean raward: 89.80370771511852, Best: 278.31\n",
            "Episode 520, mean raward: 85.70100723775053, Best: 278.31\n",
            "Episode 530, mean raward: 100.49955343695086, Best: 278.31\n",
            "Episode 540, mean raward: 123.37730955118535, Best: 278.31\n",
            "Episode 550, mean raward: 144.10962446197573, Best: 278.31\n",
            "Episode 560, mean raward: 11.002002053074346, Best: 278.31\n",
            "Episode 570, mean raward: 19.130181335815404, Best: 278.31\n",
            "Episode 580, mean raward: 52.047528356362754, Best: 278.31\n",
            "Episode 590, mean raward: 60.660642815897305, Best: 278.31\n",
            "Episode 600, mean raward: -49.89952042668146, Best: 278.31\n",
            "Episode 610, mean raward: -13.363174922923928, Best: 278.31\n",
            "Episode 620, mean raward: 26.226114481131184, Best: 278.31\n",
            "Episode 630, mean raward: 48.46194895845749, Best: 278.31\n",
            "Episode 640, mean raward: 112.54849639646807, Best: 278.31\n",
            "Episode 650, mean raward: 37.03810394222807, Best: 278.31\n",
            "Episode 660, mean raward: -84.37701695036509, Best: 278.31\n",
            "Episode 670, mean raward: -19.864301130788593, Best: 278.31\n",
            "Episode 680, mean raward: -73.96605472518708, Best: 278.31\n",
            "Episode 690, mean raward: 3.347341792325744, Best: 278.31\n",
            "Episode 700, mean raward: -15.655789705128633, Best: 278.31\n",
            "Episode 710, mean raward: -68.03551984592566, Best: 278.31\n",
            "Episode 720, mean raward: 13.207417126320987, Best: 278.31\n",
            "Episode 730, mean raward: -7.731499521889227, Best: 278.31\n",
            "Episode 740, mean raward: 14.38862604595379, Best: 278.31\n",
            "Episode 750, mean raward: -72.71358504412949, Best: 278.31\n",
            "Episode 760, mean raward: -35.98950684828809, Best: 278.31\n",
            "Episode 770, mean raward: 1.8598957918630656, Best: 278.31\n",
            "Episode 780, mean raward: -20.859624466835193, Best: 278.31\n",
            "Episode 790, mean raward: -44.209053517762456, Best: 278.31\n",
            "Episode 800, mean raward: 35.695601634762816, Best: 278.31\n",
            "Episode 810, mean raward: 82.43880084717811, Best: 278.31\n",
            "Episode 820, mean raward: 132.29769886454383, Best: 278.31\n",
            "Episode 830, mean raward: 51.28512081679714, Best: 278.31\n",
            "Episode 840, mean raward: 48.59129192248705, Best: 278.31\n",
            "Episode 850, mean raward: 16.05693868455225, Best: 278.31\n",
            "Episode 860, mean raward: 83.61205535769577, Best: 278.31\n",
            "Episode 870, mean raward: -1.261912443023228, Best: 278.31\n",
            "Episode 880, mean raward: 92.36701725931344, Best: 278.31\n",
            "Episode 890, mean raward: 30.351510886719005, Best: 278.31\n",
            "Episode 900, mean raward: 32.98320951493849, Best: 278.31\n",
            "Episode 910, mean raward: 95.78982112100633, Best: 278.31\n",
            "Episode 920, mean raward: 42.67794599331488, Best: 278.31\n",
            "Episode 930, mean raward: 64.55757189757568, Best: 278.31\n",
            "Episode 940, mean raward: 15.393383168946444, Best: 278.31\n",
            "Episode 950, mean raward: 10.543310994474018, Best: 278.31\n",
            "Episode 960, mean raward: 64.3892289089726, Best: 278.31\n",
            "Episode 970, mean raward: 28.691146025419748, Best: 278.31\n",
            "Episode 980, mean raward: -38.276071895944604, Best: 278.31\n",
            "Episode 990, mean raward: 33.22161526213599, Best: 278.31\n",
            "Episode 1000, mean raward: 25.396140263332015, Best: 278.31\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.PPOAgent at 0x7b87c4a971d0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ppo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gS1amHhy_uo",
        "outputId": "7f98f2b0-7cba-4bf0-b849-279fe201a1ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9804\\1286707640.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Episode 1, Total Reward: 54.34\n",
            "Test Episode 2, Total Reward: -0.86\n",
            "Test Episode 3, Total Reward: 26.17\n",
            "Test Episode 4, Total Reward: 38.09\n",
            "Test Episode 5, Total Reward: 8.93\n",
            "Test Episode 6, Total Reward: 107.00\n",
            "Test Episode 7, Total Reward: 246.87\n",
            "Test Episode 8, Total Reward: 121.78\n",
            "Test Episode 9, Total Reward: 88.87\n",
            "Test Episode 10, Total Reward: 232.88\n"
          ]
        }
      ],
      "source": [
        "model_path  = \"lunar_lander_continuous.pth\"\n",
        "test_model(model_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
