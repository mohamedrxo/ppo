{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# this class define the storage buffer of the environment  \n",
    "\n",
    "class StorageBuffer:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.episode_rewards = []  # Store rewards for each episode\n",
    "        self.episode_lengths = []  # Store lengths for each episode\n",
    "        \n",
    "    def reset(self):\n",
    "        # Current episode storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.next_state = None\n",
    "        self.current_reward = 0\n",
    "        \n",
    "    def add_step(self, state, action, reward, log_prob, done, next_state=None):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "        self.current_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            self.next_state = next_state\n",
    "            self.episode_rewards.append(self.current_reward)\n",
    "            self.episode_lengths.append(len(self.rewards))\n",
    "            \n",
    "    def get_episode_data(self):\n",
    "        return {\n",
    "            'states': torch.FloatTensor(np.array(self.states)),\n",
    "            'actions': torch.tensor(self.actions),\n",
    "            'rewards': torch.tensor(self.rewards),\n",
    "            'log_probs': torch.tensor(self.log_probs),\n",
    "            'dones': torch.tensor(self.dones),\n",
    "            'next_state': torch.FloatTensor(self.next_state).unsqueeze(0) if self.next_state is not None else None\n",
    "        }\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        if not self.episode_rewards:\n",
    "            return {\"mean_reward\": 0, \"max_reward\": 0, \"min_reward\": 0, \"mean_length\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"mean_reward\": np.mean(self.episode_rewards),\n",
    "            \"max_reward\": np.max(self.episode_rewards),\n",
    "            \"min_reward\": np.min(self.episode_rewards),\n",
    "            \"mean_length\": np.mean(self.episode_lengths),\n",
    "            \"current_reward\": self.current_reward,\n",
    "            \"current_length\": len(self.rewards)\n",
    "        }\n",
    "\n",
    "#the policy and value networks\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space observation_space 4\n",
      "action space 2\n"
     ]
    }
   ],
   "source": [
    "# this is our environment that will train our ppo agent on \n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()\n",
    "print('observation space observation_space',env.observation_space.shape[0])\n",
    "print('action space',env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ppo agent class uses the value and policy network and adamW optimizer and MSELoss\n",
    "\n",
    "class PPOAgent():\n",
    "    def __init__(self,state_dim,action_dim,lr=3e-4,gamma = 0.99,eps_clip =  0.2 , k_epochs=4):\n",
    "        self.policy  = PolicyNetwork(state_dim,action_dim)\n",
    "        self.old_policy  = PolicyNetwork(state_dim,action_dim)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        self.value_function = ValueNetwork(state_dim)\n",
    "        self.policy_optimizer =  optim.AdamW(self.policy.parameters(),lr=lr)\n",
    "        self.value_optimizer =  optim.AdamW(self.value_function.parameters(),lr)\n",
    "        self.gamma = gamma\n",
    "        self.k_epochs = k_epochs \n",
    "        self.eps_clip  = eps_clip\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    def select_action(self,state):\n",
    "        # this function used to select action by our old policy and returning the log_prob of it \n",
    "        state = torch.tensor(state,dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_prob  =  self.old_policy(state)\n",
    "        disc =  Categorical(action_prob)\n",
    "        action  = disc.sample()\n",
    "        return action.item(),disc.log_prob(action)\n",
    "    def compute_advantages(self, rewards, values, next_value, dones):\n",
    "        # this function calculate the advantage  function\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * (1 - float(dones[t])) * next_value - values[t]\n",
    "            gae = delta + self.gamma * (1 - float(dones[t])) * gae\n",
    "            advantages.insert(0, gae)\n",
    "            next_value = values[t]\n",
    "        return torch.tensor(advantages, dtype=torch.float32)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "\n",
    "        torch.save({\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'value_state_dict': self.value_function.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.old_policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.value_function.load_state_dict(checkpoint['value_state_dict'])\n",
    "    \n",
    "    def update(self,buffer):\n",
    "        # update is the main function to train the model using ppo algorithm\n",
    "        data = buffer.get_episode_data()\n",
    "        states = data['states']\n",
    "        actions = data['actions']\n",
    "        rewards = data['rewards']\n",
    "        dones = data['dones']\n",
    "        log_probs_old = data['log_probs']\n",
    "\n",
    "        values = self.value_function(states).squeeze().detach()\n",
    "\n",
    "        if data['next_state'] is not None:\n",
    "            next_value = self.value_function(data['next_state']).item()\n",
    "        else:\n",
    "            next_value = 0.0\n",
    "        \n",
    "        advantages = self.compute_advantages(rewards, values, next_value, dones)\n",
    "        targets = advantages + values\n",
    "\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            action_probs = self.policy(states)\n",
    "            dist = Categorical(action_probs)\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            \n",
    "            \n",
    "            ratios = torch.exp(log_probs - log_probs_old)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            \n",
    "            \n",
    "            loss_actor = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            \n",
    "            values_pred = self.value_function(states).squeeze()\n",
    "            loss_critic = self.mse_loss(values_pred, targets)\n",
    "\n",
    "            policy_losses.append(loss_actor.item())\n",
    "            value_losses.append(loss_critic.item())\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            loss_actor.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            self.value_optimizer.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': np.mean(policy_losses),\n",
    "            'value_loss': np.mean(value_losses),\n",
    "            'average_value': values.mean().item()\n",
    "        }\n",
    "\n",
    "\n",
    "def train_ppo(num_episodes=100,max_time_steps=200):\n",
    "        \n",
    "        # this function uses the interact with the environment and passes data to the update function to apply ppo on the model\n",
    "\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        state_dim  =  env.observation_space.shape[0]\n",
    "        action_dim =   env.action_space.n\n",
    "        best_reward = 0\n",
    "        agent  =  PPOAgent(state_dim,action_dim)\n",
    "        storage = StorageBuffer()\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            state = np.array(state)\n",
    "            storage.reset()  \n",
    "            for t in range(max_time_steps):\n",
    "                action ,old_log_prob  = agent.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                storage.add_step(state, action, reward, old_log_prob, done, next_state if done else None)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            agent.update(storage)\n",
    "            stats = storage.get_statistics()\n",
    "            if stats['current_reward'] >= best_reward:\n",
    "                best_reward = stats['current_reward']\n",
    "                agent.save_model('cartpole_ppo.pth')\n",
    "\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {stats['current_reward']}, Best Reward: {best_reward}\")\n",
    "\n",
    "\n",
    "        env.close()\n",
    "        return agent, storage\n",
    "    \n",
    "def test_model(model_path, num_episodes = 10):\n",
    "    # this function allow us to view the model performance on the environment\n",
    "    env = gym.make(\"CartPole-v1\",render_mode=\"human\")\n",
    "    state_dim  =  env.observation_space.shape[0]\n",
    "    action_dim =   env.action_space.n\n",
    "    best_reward = 0\n",
    "    agent  =  PPOAgent(state_dim,action_dim)\n",
    "    agent.load_model(model_path)\n",
    "\n",
    "    storage = StorageBuffer()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state)\n",
    "        storage.reset()  # Reset storage for new episode\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state = np.array(next_state)\n",
    "            \n",
    "            storage.add_step(state, action, reward, log_prob, done, next_state if done else None)\n",
    "            state = next_state\n",
    "        \n",
    "        stats = storage.get_statistics()\n",
    "        print(f\"Test Episode {episode + 1}, Total Reward: {stats['current_reward']}\")\n",
    "    \n",
    "    env.close()\n",
    "    return storage\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 300 episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 34.0, Best Reward: 34.0\n",
      "Episode 2, Total Reward: 33.0, Best Reward: 34.0\n",
      "Episode 3, Total Reward: 34.0, Best Reward: 34.0\n",
      "Episode 4, Total Reward: 21.0, Best Reward: 34.0\n",
      "Episode 5, Total Reward: 19.0, Best Reward: 34.0\n",
      "Episode 6, Total Reward: 48.0, Best Reward: 48.0\n",
      "Episode 7, Total Reward: 15.0, Best Reward: 48.0\n",
      "Episode 8, Total Reward: 29.0, Best Reward: 48.0\n",
      "Episode 9, Total Reward: 34.0, Best Reward: 48.0\n",
      "Episode 10, Total Reward: 20.0, Best Reward: 48.0\n",
      "Episode 11, Total Reward: 36.0, Best Reward: 48.0\n",
      "Episode 12, Total Reward: 13.0, Best Reward: 48.0\n",
      "Episode 13, Total Reward: 15.0, Best Reward: 48.0\n",
      "Episode 14, Total Reward: 12.0, Best Reward: 48.0\n",
      "Episode 15, Total Reward: 13.0, Best Reward: 48.0\n",
      "Episode 16, Total Reward: 21.0, Best Reward: 48.0\n",
      "Episode 17, Total Reward: 58.0, Best Reward: 58.0\n",
      "Episode 18, Total Reward: 21.0, Best Reward: 58.0\n",
      "Episode 19, Total Reward: 16.0, Best Reward: 58.0\n",
      "Episode 20, Total Reward: 16.0, Best Reward: 58.0\n",
      "Episode 21, Total Reward: 23.0, Best Reward: 58.0\n",
      "Episode 22, Total Reward: 13.0, Best Reward: 58.0\n",
      "Episode 23, Total Reward: 19.0, Best Reward: 58.0\n",
      "Episode 24, Total Reward: 17.0, Best Reward: 58.0\n",
      "Episode 25, Total Reward: 33.0, Best Reward: 58.0\n",
      "Episode 26, Total Reward: 29.0, Best Reward: 58.0\n",
      "Episode 27, Total Reward: 39.0, Best Reward: 58.0\n",
      "Episode 28, Total Reward: 16.0, Best Reward: 58.0\n",
      "Episode 29, Total Reward: 24.0, Best Reward: 58.0\n",
      "Episode 30, Total Reward: 14.0, Best Reward: 58.0\n",
      "Episode 31, Total Reward: 28.0, Best Reward: 58.0\n",
      "Episode 32, Total Reward: 25.0, Best Reward: 58.0\n",
      "Episode 33, Total Reward: 18.0, Best Reward: 58.0\n",
      "Episode 34, Total Reward: 59.0, Best Reward: 59.0\n",
      "Episode 35, Total Reward: 23.0, Best Reward: 59.0\n",
      "Episode 36, Total Reward: 28.0, Best Reward: 59.0\n",
      "Episode 37, Total Reward: 23.0, Best Reward: 59.0\n",
      "Episode 38, Total Reward: 17.0, Best Reward: 59.0\n",
      "Episode 39, Total Reward: 11.0, Best Reward: 59.0\n",
      "Episode 40, Total Reward: 36.0, Best Reward: 59.0\n",
      "Episode 41, Total Reward: 13.0, Best Reward: 59.0\n",
      "Episode 42, Total Reward: 14.0, Best Reward: 59.0\n",
      "Episode 43, Total Reward: 28.0, Best Reward: 59.0\n",
      "Episode 44, Total Reward: 21.0, Best Reward: 59.0\n",
      "Episode 45, Total Reward: 28.0, Best Reward: 59.0\n",
      "Episode 46, Total Reward: 34.0, Best Reward: 59.0\n",
      "Episode 47, Total Reward: 40.0, Best Reward: 59.0\n",
      "Episode 48, Total Reward: 22.0, Best Reward: 59.0\n",
      "Episode 49, Total Reward: 60.0, Best Reward: 60.0\n",
      "Episode 50, Total Reward: 98.0, Best Reward: 98.0\n",
      "Episode 51, Total Reward: 46.0, Best Reward: 98.0\n",
      "Episode 52, Total Reward: 38.0, Best Reward: 98.0\n",
      "Episode 53, Total Reward: 23.0, Best Reward: 98.0\n",
      "Episode 54, Total Reward: 36.0, Best Reward: 98.0\n",
      "Episode 55, Total Reward: 16.0, Best Reward: 98.0\n",
      "Episode 56, Total Reward: 17.0, Best Reward: 98.0\n",
      "Episode 57, Total Reward: 19.0, Best Reward: 98.0\n",
      "Episode 58, Total Reward: 36.0, Best Reward: 98.0\n",
      "Episode 59, Total Reward: 24.0, Best Reward: 98.0\n",
      "Episode 60, Total Reward: 19.0, Best Reward: 98.0\n",
      "Episode 61, Total Reward: 71.0, Best Reward: 98.0\n",
      "Episode 62, Total Reward: 42.0, Best Reward: 98.0\n",
      "Episode 63, Total Reward: 59.0, Best Reward: 98.0\n",
      "Episode 64, Total Reward: 20.0, Best Reward: 98.0\n",
      "Episode 65, Total Reward: 35.0, Best Reward: 98.0\n",
      "Episode 66, Total Reward: 16.0, Best Reward: 98.0\n",
      "Episode 67, Total Reward: 41.0, Best Reward: 98.0\n",
      "Episode 68, Total Reward: 22.0, Best Reward: 98.0\n",
      "Episode 69, Total Reward: 60.0, Best Reward: 98.0\n",
      "Episode 70, Total Reward: 27.0, Best Reward: 98.0\n",
      "Episode 71, Total Reward: 66.0, Best Reward: 98.0\n",
      "Episode 72, Total Reward: 29.0, Best Reward: 98.0\n",
      "Episode 73, Total Reward: 43.0, Best Reward: 98.0\n",
      "Episode 74, Total Reward: 29.0, Best Reward: 98.0\n",
      "Episode 75, Total Reward: 28.0, Best Reward: 98.0\n",
      "Episode 76, Total Reward: 59.0, Best Reward: 98.0\n",
      "Episode 77, Total Reward: 19.0, Best Reward: 98.0\n",
      "Episode 78, Total Reward: 44.0, Best Reward: 98.0\n",
      "Episode 79, Total Reward: 43.0, Best Reward: 98.0\n",
      "Episode 80, Total Reward: 57.0, Best Reward: 98.0\n",
      "Episode 81, Total Reward: 24.0, Best Reward: 98.0\n",
      "Episode 82, Total Reward: 34.0, Best Reward: 98.0\n",
      "Episode 83, Total Reward: 28.0, Best Reward: 98.0\n",
      "Episode 84, Total Reward: 19.0, Best Reward: 98.0\n",
      "Episode 85, Total Reward: 102.0, Best Reward: 102.0\n",
      "Episode 86, Total Reward: 30.0, Best Reward: 102.0\n",
      "Episode 87, Total Reward: 87.0, Best Reward: 102.0\n",
      "Episode 88, Total Reward: 26.0, Best Reward: 102.0\n",
      "Episode 89, Total Reward: 12.0, Best Reward: 102.0\n",
      "Episode 90, Total Reward: 42.0, Best Reward: 102.0\n",
      "Episode 91, Total Reward: 43.0, Best Reward: 102.0\n",
      "Episode 92, Total Reward: 97.0, Best Reward: 102.0\n",
      "Episode 93, Total Reward: 45.0, Best Reward: 102.0\n",
      "Episode 94, Total Reward: 84.0, Best Reward: 102.0\n",
      "Episode 95, Total Reward: 52.0, Best Reward: 102.0\n",
      "Episode 96, Total Reward: 45.0, Best Reward: 102.0\n",
      "Episode 97, Total Reward: 61.0, Best Reward: 102.0\n",
      "Episode 98, Total Reward: 88.0, Best Reward: 102.0\n",
      "Episode 99, Total Reward: 87.0, Best Reward: 102.0\n",
      "Episode 100, Total Reward: 73.0, Best Reward: 102.0\n",
      "Episode 101, Total Reward: 56.0, Best Reward: 102.0\n",
      "Episode 102, Total Reward: 52.0, Best Reward: 102.0\n",
      "Episode 103, Total Reward: 39.0, Best Reward: 102.0\n",
      "Episode 104, Total Reward: 95.0, Best Reward: 102.0\n",
      "Episode 105, Total Reward: 46.0, Best Reward: 102.0\n",
      "Episode 106, Total Reward: 122.0, Best Reward: 122.0\n",
      "Episode 107, Total Reward: 49.0, Best Reward: 122.0\n",
      "Episode 108, Total Reward: 53.0, Best Reward: 122.0\n",
      "Episode 109, Total Reward: 120.0, Best Reward: 122.0\n",
      "Episode 110, Total Reward: 31.0, Best Reward: 122.0\n",
      "Episode 111, Total Reward: 35.0, Best Reward: 122.0\n",
      "Episode 112, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 113, Total Reward: 24.0, Best Reward: 200.0\n",
      "Episode 114, Total Reward: 30.0, Best Reward: 200.0\n",
      "Episode 115, Total Reward: 105.0, Best Reward: 200.0\n",
      "Episode 116, Total Reward: 111.0, Best Reward: 200.0\n",
      "Episode 117, Total Reward: 64.0, Best Reward: 200.0\n",
      "Episode 118, Total Reward: 130.0, Best Reward: 200.0\n",
      "Episode 119, Total Reward: 117.0, Best Reward: 200.0\n",
      "Episode 120, Total Reward: 57.0, Best Reward: 200.0\n",
      "Episode 121, Total Reward: 44.0, Best Reward: 200.0\n",
      "Episode 122, Total Reward: 160.0, Best Reward: 200.0\n",
      "Episode 123, Total Reward: 31.0, Best Reward: 200.0\n",
      "Episode 124, Total Reward: 90.0, Best Reward: 200.0\n",
      "Episode 125, Total Reward: 19.0, Best Reward: 200.0\n",
      "Episode 126, Total Reward: 155.0, Best Reward: 200.0\n",
      "Episode 127, Total Reward: 176.0, Best Reward: 200.0\n",
      "Episode 128, Total Reward: 126.0, Best Reward: 200.0\n",
      "Episode 129, Total Reward: 114.0, Best Reward: 200.0\n",
      "Episode 130, Total Reward: 78.0, Best Reward: 200.0\n",
      "Episode 131, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 132, Total Reward: 55.0, Best Reward: 200.0\n",
      "Episode 133, Total Reward: 156.0, Best Reward: 200.0\n",
      "Episode 134, Total Reward: 52.0, Best Reward: 200.0\n",
      "Episode 135, Total Reward: 70.0, Best Reward: 200.0\n",
      "Episode 136, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 137, Total Reward: 137.0, Best Reward: 200.0\n",
      "Episode 138, Total Reward: 91.0, Best Reward: 200.0\n",
      "Episode 139, Total Reward: 169.0, Best Reward: 200.0\n",
      "Episode 140, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 141, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 142, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 143, Total Reward: 163.0, Best Reward: 200.0\n",
      "Episode 144, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 145, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 146, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 147, Total Reward: 106.0, Best Reward: 200.0\n",
      "Episode 148, Total Reward: 163.0, Best Reward: 200.0\n",
      "Episode 149, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 150, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 151, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 152, Total Reward: 142.0, Best Reward: 200.0\n",
      "Episode 153, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 154, Total Reward: 173.0, Best Reward: 200.0\n",
      "Episode 155, Total Reward: 158.0, Best Reward: 200.0\n",
      "Episode 156, Total Reward: 163.0, Best Reward: 200.0\n",
      "Episode 157, Total Reward: 172.0, Best Reward: 200.0\n",
      "Episode 158, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 159, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 160, Total Reward: 153.0, Best Reward: 200.0\n",
      "Episode 161, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 162, Total Reward: 176.0, Best Reward: 200.0\n",
      "Episode 163, Total Reward: 127.0, Best Reward: 200.0\n",
      "Episode 164, Total Reward: 148.0, Best Reward: 200.0\n",
      "Episode 165, Total Reward: 47.0, Best Reward: 200.0\n",
      "Episode 166, Total Reward: 148.0, Best Reward: 200.0\n",
      "Episode 167, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 168, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 169, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 170, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 171, Total Reward: 185.0, Best Reward: 200.0\n",
      "Episode 172, Total Reward: 167.0, Best Reward: 200.0\n",
      "Episode 173, Total Reward: 72.0, Best Reward: 200.0\n",
      "Episode 174, Total Reward: 122.0, Best Reward: 200.0\n",
      "Episode 175, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 176, Total Reward: 193.0, Best Reward: 200.0\n",
      "Episode 177, Total Reward: 14.0, Best Reward: 200.0\n",
      "Episode 178, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 179, Total Reward: 118.0, Best Reward: 200.0\n",
      "Episode 180, Total Reward: 181.0, Best Reward: 200.0\n",
      "Episode 181, Total Reward: 146.0, Best Reward: 200.0\n",
      "Episode 182, Total Reward: 139.0, Best Reward: 200.0\n",
      "Episode 183, Total Reward: 62.0, Best Reward: 200.0\n",
      "Episode 184, Total Reward: 84.0, Best Reward: 200.0\n",
      "Episode 185, Total Reward: 192.0, Best Reward: 200.0\n",
      "Episode 186, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 187, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 188, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 189, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 190, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 191, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 192, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 193, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 194, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 195, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 196, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 197, Total Reward: 167.0, Best Reward: 200.0\n",
      "Episode 198, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 199, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 200, Total Reward: 197.0, Best Reward: 200.0\n",
      "Episode 201, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 202, Total Reward: 181.0, Best Reward: 200.0\n",
      "Episode 203, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 204, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 205, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 206, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 207, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 208, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 209, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 210, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 211, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 212, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 213, Total Reward: 158.0, Best Reward: 200.0\n",
      "Episode 214, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 215, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 216, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 217, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 218, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 219, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 220, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 221, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 222, Total Reward: 185.0, Best Reward: 200.0\n",
      "Episode 223, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 224, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 225, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 226, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 227, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 228, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 229, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 230, Total Reward: 184.0, Best Reward: 200.0\n",
      "Episode 231, Total Reward: 182.0, Best Reward: 200.0\n",
      "Episode 232, Total Reward: 142.0, Best Reward: 200.0\n",
      "Episode 233, Total Reward: 121.0, Best Reward: 200.0\n",
      "Episode 234, Total Reward: 169.0, Best Reward: 200.0\n",
      "Episode 235, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 236, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 237, Total Reward: 192.0, Best Reward: 200.0\n",
      "Episode 238, Total Reward: 163.0, Best Reward: 200.0\n",
      "Episode 239, Total Reward: 154.0, Best Reward: 200.0\n",
      "Episode 240, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 241, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 242, Total Reward: 189.0, Best Reward: 200.0\n",
      "Episode 243, Total Reward: 195.0, Best Reward: 200.0\n",
      "Episode 244, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 245, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 246, Total Reward: 196.0, Best Reward: 200.0\n",
      "Episode 247, Total Reward: 188.0, Best Reward: 200.0\n",
      "Episode 248, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 249, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 250, Total Reward: 199.0, Best Reward: 200.0\n",
      "Episode 251, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 252, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 253, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 254, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 255, Total Reward: 114.0, Best Reward: 200.0\n",
      "Episode 256, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 257, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 258, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 259, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 260, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 261, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 262, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 263, Total Reward: 182.0, Best Reward: 200.0\n",
      "Episode 264, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 265, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 266, Total Reward: 160.0, Best Reward: 200.0\n",
      "Episode 267, Total Reward: 197.0, Best Reward: 200.0\n",
      "Episode 268, Total Reward: 184.0, Best Reward: 200.0\n",
      "Episode 269, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 270, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 271, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 272, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 273, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 274, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 275, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 276, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 277, Total Reward: 150.0, Best Reward: 200.0\n",
      "Episode 278, Total Reward: 197.0, Best Reward: 200.0\n",
      "Episode 279, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 280, Total Reward: 171.0, Best Reward: 200.0\n",
      "Episode 281, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 282, Total Reward: 178.0, Best Reward: 200.0\n",
      "Episode 283, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 284, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 285, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 286, Total Reward: 172.0, Best Reward: 200.0\n",
      "Episode 287, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 288, Total Reward: 146.0, Best Reward: 200.0\n",
      "Episode 289, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 290, Total Reward: 154.0, Best Reward: 200.0\n",
      "Episode 291, Total Reward: 191.0, Best Reward: 200.0\n",
      "Episode 292, Total Reward: 193.0, Best Reward: 200.0\n",
      "Episode 293, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 294, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 295, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 296, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 297, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 298, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 299, Total Reward: 200.0, Best Reward: 200.0\n",
      "Episode 300, Total Reward: 200.0, Best Reward: 200.0\n",
      "\n",
      "Training completed! Training statistics:\n",
      "Mean reward: 81.88\n",
      "Max reward: 200.00\n",
      "Mean episode length: 81.88\n",
      "\n",
      "Starting model testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13444\\3046547010.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1, Total Reward: 500.0\n",
      "Test Episode 2, Total Reward: 500.0\n",
      "Test Episode 3, Total Reward: 262.0\n",
      "Test Episode 4, Total Reward: 500.0\n",
      "Test Episode 5, Total Reward: 418.0\n",
      "\n",
      "Testing completed! Test statistics:\n",
      "Mean test reward: 436.00\n",
      "Max test reward: 500.00\n",
      "Min test reward: 262.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # this is for training and testing the model\n",
    "    num_episodes = 300  # You can change this number\n",
    "    print(f\"Starting training for {num_episodes} episodes...\")\n",
    "    agent, train_storage = train_ppo(num_episodes=num_episodes)\n",
    "    \n",
    "    print(\"\\nTraining completed! Training statistics:\")\n",
    "    train_stats = train_storage.get_statistics()\n",
    "    print(f\"Mean reward: {train_stats['mean_reward']:.2f}\")\n",
    "    print(f\"Max reward: {train_stats['max_reward']:.2f}\")\n",
    "    print(f\"Mean episode length: {train_stats['mean_length']:.2f}\")\n",
    "    model_path = \"cartpole_ppo.pth\"\n",
    "    \n",
    "    print(\"\\nStarting model testing...\")\n",
    "    test_storage = test_model(model_path, num_episodes=5)\n",
    "    \n",
    "    print(\"\\nTesting completed! Test statistics:\")\n",
    "    test_stats = test_storage.get_statistics()\n",
    "    print(f\"Mean test reward: {test_stats['mean_reward']:.2f}\")\n",
    "    print(f\"Max test reward: {test_stats['max_reward']:.2f}\")\n",
    "    print(f\"Min test reward: {test_stats['min_reward']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13444\\3046547010.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1, Total Reward: 340.0\n",
      "Test Episode 2, Total Reward: 450.0\n",
      "Test Episode 3, Total Reward: 471.0\n",
      "Test Episode 4, Total Reward: 450.0\n",
      "Test Episode 5, Total Reward: 455.0\n",
      "\n",
      "Testing completed! Test statistics:\n",
      "Mean test reward: 433.20\n",
      "Max test reward: 471.00\n",
      "Min test reward: 340.00\n"
     ]
    }
   ],
   "source": [
    "# testing the model with the saved params\n",
    "\n",
    "model_path = \"cartpole_ppo.pth\"\n",
    "    \n",
    "print(\"\\nStarting model testing...\")\n",
    "test_storage = test_model(model_path, num_episodes=5)\n",
    "\n",
    "print(\"\\nTesting completed! Test statistics:\")\n",
    "test_stats = test_storage.get_statistics()\n",
    "print(f\"Mean test reward: {test_stats['mean_reward']:.2f}\")\n",
    "print(f\"Max test reward: {test_stats['max_reward']:.2f}\")\n",
    "print(f\"Min test reward: {test_stats['min_reward']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
